# Comparing Models {#sec-model_comparison}

> "A map is not the territory it represents, but, if correct, it has a similar structure to the territory." -Alfred Korzybski

Models should be designed as approximations to help us understand a particular problem,
or a class of related problems. Models are not designed to be verbatim copies of the real
world. Thus, all models are wrong in the same sense that maps are not the territory. Even
when a priori, we consider every model to be wrong, not every model is equally wrong;
some models will be better than others at describing a given problem. In the foregoing
chapters, we focused our attention on the inference problem, that is, how to learn values of
parameters from the data. In this chapter, we are going to focus on a complementary
problem: how to compare two or more models that are used to explain the same data. As
we will learn, this is not a simple problem to solve and at the same time is a central problem
in data analysis.

In this chapter, we will explore the following topics:

* Overfitting and underfitting
* Cross-validation
* Bayes factors
* Regularizing priors


## Posterior predictive tests 

We have previously introduced and discussed posterior predictive tests as a way to assess how well models explain the same data used to fit a model. The purpose of this type of testing is not to determine that a model is incorrect; We already know this! The goal of the exercise is to understand how well we are capturing the data. We often capture different aspects of data in different ways. By performing posterior predictive tests, we aim to better understand the limitations of a model. Once we undestand the limitations we can simply acknoledgment them or try to remove them by improve the model. It is expected that a model will not be able to reproduce all aspects of a problem and this is usually not a problem as models are built with a purpose in mind. Posterior predictive tests are ne way to evaluate that purpose, so if we have more than one model, we can compare them using posteriori predictive tests.


Let's look at a simple example. We have a data set with two variables `x` and `y`. We are going to fit these data with a linear model:

$$
y = \alpha + \beta x
$$

And a quadratic model, that is a model with one more term than the linear model. For this extra term we just take `x` to the power of 2 and add a $\beta$ coefficient, that is:
$$
y = \alpha + \beta_0 x + \beta_1 x^2
$$

We can write in PyMC this models as usual, see the next block of code. The only difference is that we pass the argument `idata_kwargs={"log_likelihood": True}` to `pm.sample`. This will store the log likelihood in the `InferenceData` object, we will use this later.

```python
with pm.Model() as model_l:
    α = pm.Normal('α', mu=0, sigma=1)
    β = pm.Normal('β', mu=0, sigma=10)
    σ = pm.HalfNormal('σ', 5)

    μ = α + β * x_c[0]
    
    y_pred = pm.Normal('y_pred', mu=μ, sigma=σ, observed=y_c)

    idata_l = pm.sample(2000, idata_kwargs={"log_likelihood": True})
    idata_l.extend(pm.sample_posterior_predictive(idata_l))

with pm.Model() as model_q:
    α = pm.Normal('α', mu=0, sigma=1)
    β = pm.Normal('β', mu=0, sigma=10, shape=order)
    σ = pm.HalfNormal('σ', 5)

    μ = α + pm.math.dot(β, x_c)
    
    y_pred = pm.Normal('y_pred', mu=μ, sigma=σ, observed=y_c)

    idata_q = pm.sample(2000, idata_kwargs={"log_likelihood": True})
    idata_q.extend(pm.sample_posterior_predictive(idata_q))
```

Figure @fig-lin-pol-fit shows the mean fit from both models. 
    
![Mean fit for `model_l` (linear) and `model_q`](/fig/lin-pol-fit.png){#fig-lin-pol-fit}
    
Visually, both models seems to provide a resonable fit to the data. To gain further insight we can do a posterior predictive check, Figure @fig-lin-pol-ppc shows KDEs for the observed and predicted data. Here it becomes easier to see that `model_q`, the quadratic model, provides a better fit to the data. We can also see there is a lot of uncertainty, in particular at the tails of the distributions, this is because we have a small number of data points.

![Posterior predictive checks for `model_l` and `model_q` created with `az.plot_ppc` function](/fig/lin-pol-ppc.png){#fig-lin-pol-ppc}

Posterior predictive checks are a very verstail idea. We can compare observed and predicted data in so many ways. Instead of comparing the densities of the distributions we can compare summary statistics. 


For instance on the left panel of Figure @fig-lin-pol-bpv we have the distributions of means for both models. The dot over the x-axis indicates the observed value. We can see that both model captures the mean very well, with the quadratic model having less variance. That both models capture the mean very well is not surprising as we are explicitelly modeling the mean with both models. On the right panel we have the distributions of interquartile range, this comparisons favor the linear model instead.

![Posterior predictive checks for `model_l` and `model_q` created with `az.plot_bpv` function](/fig/lin-pol-bpv.png){#fig-lin-pol-bpv}

In general, a statistic that is _orthogonal_ to what the model is explicitelly modeling will be more informative for evaluating the model. When in doubt, it may be convenient to evaluate more than one statistic. A useful question is to ask yourself what aspects of the data you are interested in capturing.

To generate Figure @fig-lin-pol-bpv we used the ArviZ function `az.plot_bpv`. An extract of the full code to generate that figure is the following:

```python
idatas = [idata_l, idata_q]

def iqr(x, a=-1):
    """interquartile range"""
    return np.subtract(*np.percentile(x, [75, 25], axis=a))

for idata in idatas:
    az.plot_bpv(idata, kind="t_stat", t_stat="mean", ax=axes[0])
    

for idata in idatas:
    az.plot_bpv(idata, kind="t_stat", t_stat=iqr, ax=axes[1])
```

Notice the argument `kind="t_stat"` to indicate we are going to use a summary statistics and the use of `t_stat="mean"`, i.e. a string to indicate that we want to use the mean as the summary statistic. Or we can use a user defined function, as in `t_stat=iqr` to indicate that we want to use the interquartile range as the summary statistic.

You may have noticed that Figure @fig-lin-pol-bpv also includes some values called `bpv`, for Bayesian p-value. The bpv is a numerical way of summarizing a comparison between simulated data and actual data. To obtain them, a summary statistic $T$ is chosen, such as the mean, median, standard deviation, whatever you may think worth of comparison. Then $T$ is calculated for the observed data $T_{\text{obs}}$ and for the simulated data $T_{\text{sim}}$. Finally we ask ourselves the question:

What is the probability that $T_{\text{sim}}$ is less than or equal to $T_{\text{obs}}$? If the observed values agree with the predicted ones, the expected value will be 0.5. In other words, half of the predictions will be below the observations and half above. This quantity is known as the _Bayesian p-value_.

$$\text{Bayesian p-value} \triangleq p(T_{\text{sim}} \le T_{\text{obs}} \mid \tilde Y)$$

For those who are familiar with _p-values_ and their use in frequentist statistics, there are a couple of clarifications. What is **Bayesian** about these _p-values_ is that we are NOT using a sampling distribution but _the posterior predictive distribution_. And an important difference with the frequentist version is that in this case we are not doing a null hypothesis test, nor trying to declare that a difference is "significant". We are simply trying to quantify how well the model explains the data.

There is yet another way to compute a Bayesian p-value. Instead of using a summary statistic, we can use the entire distribution. In this case, we can ask ourselves the question: What is the probability of predicting a lower or equal value for **each observed value**. If the model is well calibrated these probability should be the same for all observed values, the model is capturing all observations equally well. Thus we should expect a uniform distribution. Again ArviZ can help us with this with the function `az.plot_bpv` with the argument `kind="p_value"` (which is the default). Figure @fig-lin-pol-bpv2 shows the results of this calculation. The white line indicates the expected uniform distribution and the gray band shows the expected deviation given the finite size of the sample. It can be seen that both models are very similar.

![Posterior predictive checks for `model_l` and `model_q` created with `az.plot_bpv` function](/fig/lin-pol-bpv2.png){#fig-lin-pol-bpv2}


Predictive _post_testing_ provides a very flexible framework for evaluating and comparing models, either using graphs or numerical summaries such as _Bayesian p-values_, or even a combination of both. The concept is general enough to allow an analyst to use her imagination to find different ways to explore the predictive distribution _posteriori_ and use the best fit for the purposes of interpreting the data and models.

In the following sections we will explore other methods for comparing models. These methods are not mutually exclusive, they can be used in combination with posterior predictive tests. 

## The balance between simplicity and accuracy

When choosing between alternative explanations, there is a principle known as Occam's razor. In very general lines, this principle establishes that given two or more equivalent explanations for the same phenomenon, the simplest is the preferred explanation. A common criterion of simplicity is the number of parameters in a model.

There are many justifications for this heuristic, we are not going to discuss any of them, we are just going to accept it as a reasonable guide.

Another factor that we generally have to take into account when comparing models is their accuracy, that is, how good a model is at fitting the data. According to this criterion, if we have two (or more) models and one of them explains the data better than the other, then that is the preferred model.

Intuitively, it seems that when comparing models, we tend to prefer those that best fit the data and those that are simpler. But what to do if these two principles conflict? Or more generally, is there a quantitative way to consider both contributions? The short answer is yes, in fact there is more than one way to do it. But first let's see an example in order to generate more intuition.

### Many parameters (can) lead to overfitting

Figure @fig-over_under_fit shows 3 models with increazing number of parameters. The first one (order 0) is just a constant value, whatwer the value of `x`, the model always predicts the same values for `y`, the second model (order 1) is a linear model as we saw in the previous chapter and the last one (order 5) is a polynomial model of order 5, that is something of the form $\alpha + \beta_0 x + \beta_0 x^2 + \beta_0 x^3 + \beta_0 x^4 + \beta_0 x^5$. 


![3 simple models and a simple dataset](/fig/over_under_fit.png){#fig-over_under_fit}

From the Figure @fig-over_under_fit we can see that the increase in the complexity of the model (number of parameters) is accompanied by a greater accuracy reflected in the coefficient of determination R², this is a way to measure the fit of a model (for more information please read <https://en.wikipedia.org/wiki/Coefficient_of_determination>). In fact, we can see that the polynomial of order 5 fits the data perfectly, obtaining R²=1.

Why the polynomial of order 5 can capture the data without losing a single one of them? The reason is that we have the same number of parameters as data, that is, 6. Therefore, the model is simply acting as an alternative way of expressing the data. The model is not learning something about the data, it is memorizing the data! This can be problematic, the easier way to notice this is thinking what will happend to a model that memory data, when preseted with new, unobserved data. The performance is expected to be bad, as someone that just memorizes the questions for an exam only to find the questions are different in the actual exam! This is represented in Figure @fig-over_under_fit2, here we have added two new datapoints, maybe we got the money to perform a new experiment or our boss just send us new data. We can see that the model of order 5, that was able to exacltly fit the data, now has actually a worst performance (as measured by R²) than the linear model. From this simple example, we can see that a model with the best fit is not always the ideal one.


![3 simple models and a simple dataset, plus a couple of new points](/fig/over_under_fit2.png){#fig-over_under_fit2}

Looselly speacking, when a model fits the data set used to learn the parameters of that model very well, but new data sets very poorly, we say we have overfitting. This is a very common problem when analyzing data. A very useful way to think about overfitting is to consider a data set as having two components; the signal and the noise. The signal is what we want to capture (or learn) from the data. If we use a data set it is because we believe there is a signal there, otherwise it will be an exercise in futility. Noise, on the other hand, is not useful and is the product of measurement errors, limitations in the way the data was generated or captured, the presence of corrupted data, etc. A model overfits when it is so flexible (for a data set) that it is capable of _learning_ noise. This has the consequence that the signal is hidden.

This is a practical justification for Occam's razor. And he warns us that at least in principle, it is always possible to create a model so complex that it explains all the details, even the most irrelevant ones. As in the Empire described by Borges, where the cartographers reached such a level of sophistication that they created a map of the Empire whose size was the size of the Empire itself, and which coincided point for point with it.

### Too few parameters lead to underfitting

Continuing with the same example but at the other extreme of complexity, we have the model of order 0. This model is simply a Gaussian disguised as a linear model. This model is only capable of capturing the value of the mean of $Y$, and is therefore totally indifferent to the values of $x$. We say that this model has underfitted the data.

## Measures of predictive accuracy

_Everything should be made as simple as possible, but not simpler_ is a quote often attributed to Einstein. As in a healthy diet, when modeling we have to maintain a balance. Ideally, we would like to have a model that neither underfits nor overfits the data. Somehow you have to balance simplicity and goodness of fit.

In the previous example, it is relatively easy to see that the model of order 0 is _too_ simple while the model of order 5 is _too_ complex. But what can we say about the other two models? How could we establish a numerical ranking of these models? In order to do this we need to formalize our intuition about this balance between simplicity and accuracy.

Let's look at a couple of terms that will be useful to us.

* **Accuracy within the sample** (within-sample accuracy). The accuracy measured with the same data used to fit the model.
* **Accuracy out of sample** (out-of-sample accuracy). The accuracy measured with data not used to fit the model.

The within-sample accuracy will, on average, be less than the out-of-sample accuracy. That is why using the within-sample accuracy to evaluate a model in general will lead us to think that we have a better model than it really is. Using out-of-sample accuracy is therefore a better idea to avoid fooling ourselves. However, this approximation requires leaving data out of the fit, which is a luxury we generally cannot afford. Since this is a central problem in data analysis, there are several proposals to address it. Two very popular approaches are:


* Cross validation: This is an empirical strategy based on dividing the available data into separate subsets that are used to fit and alternatively evaluate

* Information criteria: This is a general term used to refer to various expressions that approximate out-of-sample accuracy as in-sample accuracy plus a term that penalizes model complexity.

### Cross validation

Cross validation is a simple and, in most cases, effective solution for comparing models. We take our data and divide it into K slices. We try to keep the slices more or less the same (in size and sometimes also in other characteristics, such as an equal number of classes). We then use K-1 portions to train the model and the rest to test it. This process is systematically repeated leaving, for each iteration, a different slice out of the training set and using that slice as the evaluation set. This is repeated until we have completed K rounds of adjustment-evaluation. The accuracy of the model will be the average over the K rounds. This is known as K-fold cross validation. Finally, once we have done the cross-validation, we use all the data to last fit our model and this is the model that is used to make predictions or for any other purpose.


![](fig/cv.png){#fig-cv width="80%"}

When K equals the number of data points, we get what is known as _leave-one-out cross-validation_ (LOOCV).

Cross validation is a routine practice in _machine learning_. And we have barely described the most essential aspects of this practice. For more information you can read [The Hundred-Page Machine Learning Book](http://themlbook.com/) or [Python Machine Learning](https://www.amazon.com/Python-Machine-Learning-scikit-learn-TensorFlow-ebook/dp/B0742K7HYF/ref=dp_ob_title_def), by Sebastian Rasch ka, or [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/) by Jake Vanderplas.

Cross validation is a very simple and useful idea, but for some models or for large amounts of data, the computational cost of cross validation may be beyond our means. Many people have tried to find simpler quantities to calculate that approximate the results obtained with cross-validation or work in scenarios where cross-validation may not be as easy to perform. And that is the topic of the next section.

### Information criteria


Information criteria are a collection of closely related tools used to compare models in terms of goodness-of-fit and model complexity. In other words, the information criteria formalize the intuition that we developed at the beginning of the chapter.
The exact way in which these quantities are derived has to do with a field known as [Information Theory](http://www.inference.org.uk/mackay/itila/book.html).


An intuitive way to measure how well a model fits the data is to calculate the root mean square error between the data and the predictions made by the model:


$$\frac{1}{n} \sum_{i=1}^{n} (y_i - \operatorname{E} (y_i \mid \theta))^2$$


$\operatorname{E} (y_i \mid \theta)$ is the predicted value given the estimated parameters. It is important to note that this is essentially the average of the difference between the observed and predicted data.
Taking the square of the errors ensures that the differences do not cancel out and emphasizes large errors compared to other alternatives such as calculating absolute value.

The root mean square error may be familiar to us as it is very popular. But if we stop to reflect on this quantity, we will see that in principle there is nothing special about it and we could well devise other similar expressions. When we adopt a probabilistic approach we see that a more general (and *natural*) expression is the following:

$$ \sum_{i=1}^{n} \log p(y_i \mid \theta)$$

That is, the sum (over $n$ data) of the _likelihoods_ (in logarithmic scale). This is _natural_ because when choosing a likelihood in a model we are implicitly choosing a metric to evaluate the fit of the model. When $p(y_i \mid \theta)$ is a Gaussian then the sum of log-likelihood will be proportional to the root mean square error.

#### Akaike Information Criterion

This is a well-known and widely used information criterion outside the Bayesian universe and is defined as:

$$AIC = -2 \sum_{i=1}^{n} \log p(y_i \mid \hat{\theta}_{mle}) + 2 k $$

Where, k is the number of model parameters and $\hat{\theta}_{mle}$ is the maximum likelihood estimate for $\theta$.


Maximum likelihood estimation is common practice for non-Bayesians and is, in general, equivalent to Bayesian maximum a posteriori (MAP) estimation when *flat* priors are used. It is important to note that $\hat{\theta}_{mle}$ is a point estimate and not a distribution.

The factor $-2$ is just a constant, and we could omit it but usually don't. What is important, from a practical point of view, is that the first term takes into account how well the model fits the data, while the second term penalizes the complexity of the model. Therefore if two models fit the data equally well. AIC says that we should choose the model with the least number of parameters.


AIC works fine in non-Bayesian approaches, but is problematic otherwise. One reason is that it does not use the posterior distribution of $\theta$ and therefore discards information about estimation uncertainty. Also, AIC, from a Bayesian perspective, assumes that priors are *flat* and therefore AIC is incompatible with informative and slightly informative priors like those used in this book. Also, the number of parameters in a model is not a good measure of the model's complexity when using informative priors or structures like hierarchical structures, as these are ways of reducing the _effective number of parameters_, also known as *regularization*. We will return to this idea of regularization later.

#### Widely applicable information criteria

WAIC is something like the Bayesian version of AIC, just like the latter WAIC is made up of two terms, one that measures the adjustment and the other that penalizes. The following expression assumes that the posterior distribution is represented as a sample of size S (as obtained from an MCMC method).

$$WAIC = -2 \sum_i^n \log \left(\frac{1}{S} \sum_{s=1}^S p(y_i \mid \theta^s) \right) + 2 \sum_i^n \left( V_{s=1}^S \log p(y_i \mid \theta^s) \right)$$


The first term is similar to the Akaike criterion, only that it is evaluated for all the observations and all the samples of the later one. The second term is a bit more difficult to justify without getting into technicalities. But it is also a way of penalizing the complexity of the model. What is important from a practical point of view is that WAIC uses the entire posterior (and not a point estimate) for the calculation of both terms, so WAIC can be applied to virtually any Bayesian model.


#### Approximating Cross validation

The key problem with leaving one out cross validation is that it is very expensive as we have to refit the model as many times as we have data. Luckily, it is possible to approximate it with a single fit to the data! The method for doing this is called "importance sampling using Pareto smoothing." The name is so ugly that in practice we call it LOO. Conceptually what we are trying to calculate is:


$$
\text{ELPD}_\text{LOO-CV} = \sum_{i=1}^{n} \log
     \int \ p(y_i \mid \theta) \; p(\theta \mid y_{-i}) d\theta
$$

$$
\sum_{i}^{n} \log
     \left( \frac{1}{s}\sum_j^s \mathbin{\color{#E9692C}{p(y_i \mid \theta_{-i}^j)}} \right)
$$

where $_{-i}$ means that we leave the observation $i$ out. It is possible to approximate $\color{#E9692C}{p(y_i \mid \theta_{-i}^j})$ using importance sampling, which is a way of approximating a distribution by re-weighting values obtained from another distribution. In our case, the known distribution, once a model has been fitted, is the log-likelihood for all the observations. And we want to approximate the log-likelihood if we had dropped an observation. For this we need to estimate the "importance" (or weight) that each observation has in determining the posterior distribution. An observation will be more "important" (or heavy) the more the subsequent changes when removing that observation. Intuitively, a relatively unlikely observation is more important (or carries more weight) than an expected one. Luckily these weights can be estimated without the need to readjust the model, in fact the weight of the observation $i$ for the sample of the subsequent $s$ is:

$$
w_s = \frac{1}{p(y_i \mid \theta_s)}
$$


The problem is that under certain conditions these weights may not be reliable. The main problem is that a few $w_s$ could be so large that they dominate the calculation, and this is where Pareto smoothing comes in, which basically consists of replacing some of these weights with weights obtained from fitting a Pareto distribution. Why a Pareto distribution? Because the theory indicates that the weights should follow this distribution. So for each observation $y_i$ , the larger weights are used to estimate a Pareto distribution and that distribution is used to replace those weights with "smoothed" weights. This procedure gives robustness to the estimation of the ELPD and also provides a diagnosis since values of $k$ (one of the parameters of the Pareto distribution) greater than 0.7 indicate that we possibly have "very influential" observations.


#### Other information criteria

Another widely used information criterion is DIC, if we use the *bayesometer™*, DIC is more Bayesian than AIC but less than WAIC. Although still popular, WAIC and mainly LOO have been shown to be more useful both theoretically and empirically than DIC. Therefore we do NOT recommend its use.

Another widely used criterion is BIC (Bayesian Information Criteria), like logistic regression and my mother's *dry soup*, this name can be misleading. BIC was proposed as a way to correct some of the problems with AIC and the author proposed a Bayesian justification for it. But BIC is not really Bayesian in the sense that like AIC it assumes *flat* priors and uses maximum likelihood estimation.

But more importantly, BIC differs from AIC and WAIC in its objective. AIC and WAIC try to reflect which model generalizes better to other data (predictive accuracy) while BIC tries to identify which is the _correct_ model and therefore is more related to Bayes factors than WAIC. Later we will discuss Bayes Factors and see how it differs from criteria like WAIC and LOO.


## Predictive accuracy calculation using ArviZ

Fortunately, calculating information criteria with ArviZ is very simple. Let's see how to compute LOO

```python
az.loo(idata_l)
```

```bash
Computed from 8000 posterior samples and 33 observations log-likelihood matrix.

         Estimate       SE
elpd_loo   -14.31     2.67
p_loo        2.40        -
------

Pareto k diagnostic values:
                         Count   Pct.
(-Inf, 0.5]   (good)       33  100.0%
 (0.5, 0.7]   (ok)          0    0.0%
   (0.7, 1]   (bad)         0    0.0%
   (1, Inf)   (very bad)    0    0.0%
```

The output of `az.loo` has two sections on the first section we get a table with two rows, the first one is the ELPD (`elpd_loo`) and the second one is the effective number of parameters (`p_loo`). On the second seccion we have the Pareto k diagnostic. This is a measure of the reliability of the approximation of the LOO. Values of k greater than 0.7 indicate that we possibly have "very influential" observations. In this case we have 33 observations and all of them are good, so we can trust the approximation. 

To compute WAIC you can use `az.waic`, the otuput will be similar, except that we will not get the Pareto k diagnostic, or any similar diagnostics, this is a downside of WAIC, we do not get any information about the reliability of the approximation.

If we compute the LOO for the quadratic model we will get a similar output, but the ELPD will be higher (around -4), indicating that the quadratic model is better at predicting the data.

Values of LOO (or WAIC) are not that usefull by themselves, and must be interpreted relatively. That is why ArviZ offers two helper functions to facilitate this comparison let's look at `az.compare` first.

```python
cmp_df = az.compare({'modelo_l':idata_l, 'modelo_p':idata_p})
```

|          |   rank |   elpd_loo | p_loo |   elpd_diff |      weight |   se |  dse | warning   | scale   |
|:---------|-------:|-----------:|------:|------------:|------------:|-----:|-----:|:----------|:--------|
| modelo_p |      0 |   -4.6     | 2.68  |     0       | 1           | 2.36 | 0    | False     | log     |
| modelo_l |      1 |  -14.3     | 2.42  |     9.73625 | 3.01e-14    | 2.67 | 2.65 | False     | log     |


In the rows we have the compared models and in the columns we have

* rank : the order of the models (from best to worst)
* elpd : the point estimate of the elpd using
* p : the effective parameters
* elpd_diff : the difference between the ELPD of the best model and the other models
* weight : the relative weight of each model. If we wanted to make predictions by combining the different models, instead of choosing just one, this would be the weight that we should assign to each model. In this case we see that the polynomial model takes all the weight.
* se : the standard error of the ELPD
* dse : the standard error of the differences
* warning : a warning about high k values
* scale : the scale on which the ELPD is calculated

The other helper function offered by ArviZ is `az.compareplot`, this function provides similar information to `az.compare`, but graphically. Figure @fig-compareplot shows the output of this function.

![Output of `az.compareplot(cmp_df)`](/fig/compareplot.png){#fig-compareplot width="80%"}


* The empty circles represent the ELPD values and the black lines the standard error.
* The highest value of the ELPD is indicated with a vertical dashed gray line to facilitate comparison with other values.
* For all models except *the best*, we also get a triangle indicating the value of the ELPD difference between each model and the *best* model. The gray error bar indicating the standard error of the differences between the point estimates.

The easiest way to use the information criteria is to choose a single model. Just choose the model with the highest ELPD value. If we follow this rule we will have to accept that the quadratic model is the best. Even if we take into account the standard errors we can see that they do not overlap. This gives us some certainty that indeed the models are *different* from each other. If instead the standard errors did overlap, we should provide a more nuanced answer.

## Average modeling