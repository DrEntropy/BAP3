# Hierarchical Models {#sec-hierarchical}

> "Something, something" - some one

In @sec-programming_probabilistically we saw the tips example where we had multiple groups in our data, one for each of four days, Thursday, Friday, Saturday, and Sunday. We decided to model each group separately. That's sometimes fine, but we should be aware of our assumptions. By modeling each group independently we assumed the groups as unrelated. In other words, we are assuming that knowing the tip for one day does not give us any information about the tip for another day. That could be a too strong assumption. Would it be possible to build a model that allows us to share information between groups? That's not only possible, that's the main topic of this chapter, Lucky you! In this chapter, we will cover the following topics:

* Hierarchical models
* Partial pooling
* Shrinkage


## Sharing information, sharing priors

Hierarchical models are also known as multilevel models, mixed-effects models, random-effects models, or nested models. They are particularly useful when dealing with data that can be described as grouped or having different levels, like data nested within geographic regions (like cities belonging to a province and provinces belonging to a country), or with a hierarchical structure (e.g., students nested within schools, or patients nested within hospitals) or repeated measurements on the same individuals.

Hierarchical models are a natural way to share information between groups. In a hierarchical model, the parameters of the prior distributions are themselves given a prior distribution. These higher-level priors are often called hyper-priors, hyper means over in Greek. Having hyper-priors allows the model to share information between groups, while still allowing differences between groups. In other words, we can think of the parameters of the prior distributions as belonging to a common population of parameters. @fig-hierarchical_model shows a diagram with the high-level differences between a pooled model (a single group), an unpooled model (all separated groups) and a hierarchical model, also known as a partial-pooled model. 


![Diagram showing the differences between a pooled model, an unpooled model, and a hierarchical model.](fig/hierarchical_model.pdf){#fig-hierarchical_model width=80%}

## Hierarchical shifts

Proteins are molecules formed by 20 units, called amino acids, each amino acid can appear in a protein 0 or more times. Just as a melody is defined by a sequence of musical notes, a protein is defined by a sequence of amino acids. Some musical note variations can result in small variations of the melody and other variations in completely different melodies. Something similar happens with proteins. One way to study proteins is by using nuclear magnetic resonance (the same technique used for medical imaging). This technique allows us to measure various quantities, one of which is called a chemical shift. You may remember we already saw an example using chemical shifts in @sec-programming_probabilistically. 

Suppose we want to compare a theoretical method to compute chemical shifts against the experimental observations, among other reasons to evaluate the ability of the theoretical method to reproduce the experimental values. Luckily for us, someone already performed the experiments and the theoretical calculations and we just need to compare them. The following data set contains chemical shift values for a set of proteins. If you inspect the DataFrame `cs_data` you will see that it has 4 columns:

1. The first is a code that identifies the protein (you can get a lot of information about that protein by entering that code at https://www.rcsb.org/.).
2. The second column has the name of the amino acid (can you verify that there are only 20 unique names).
3. The third contains theoretical values of chemical shifts (calculated using quantum methods).
4. The fourth has experimental values.

Now that we have the data, how we should proceed? One option is to take the empirical differences and fit a Gaussian or maybe Student's T model. Because amino acids are a family of chemical compounds it would make sense to assume they are all the same and estimate a single Gaussian for all the differences. But you may object there are 20 different kinds of amino acids, each one with different chemical properties and hence a better choice is to fit 20 separated Gaussians. What we should do?

Let's take a moment to think about which option is the best. If we combine all the data our estimates are going to be more accurate, but we will not be able to get information from individual groups (amino acids). On the contrary, if we treat them as separate groups we will get a much more detailed analysis but with less accuracy. What we should do?

When in doubt, everything![^all], well kind of. We can build a hierarchical model, in that way we allow estimates at the group level, but with the restriction that they all belong to a larger group or population. To better understand this, let's build a hierarchical model for the chemical shift data.

[^all]: Not sure this is good general advice for your life, but I like the song  <https://www.youtube.com/watch?v=1di09XZUlIw>

To see the difference between a non-hierarchical (un-pooled) model and a hierarchical one, we are going to build two models. The first one is essentially the same as the `comparing_groups` model from @sec-programming_probabilistically.


```python
with pm.Model(coords=coords) as cs_nh:         
    μ = pm.Normal('μ', mu=0, sigma=10, dims="aa") 
    σ = pm.HalfNormal('σ', sigma=10, dims="aa") 
 
    y = pm.Normal('y', mu=μ[idx], sigma=σ[idx], observed=diff) 
     
    idata_cs_nh = pm.sample()
```

Now, we will build the hierarchical version of the model. We are adding two hyper-priors, one for the mean of $\mu$ and one for the standard deviation of $\mu$. We are leaving $\sigma$ without hyper-priors, in other words, we are assuming the variance between observed and theoretical values should be the same for all groups This is a modeling choice, you may face a problem where this seems unacceptable and you may consider it necessary to add a hyper-prior for $\sigma$; feel free to do that.

```python
with pm.Model(coords=coords) as cs_h:
    # hyper_priors
    μ_mu = pm.Normal('μ_mu', mu=0, sigma=10)
    μ_sd = pm.HalfNormal('μ_sd', 10)

    # priors
    μ = pm.Normal('μ', mu=μ_mu, sigma=μ_sd, dims="aa") 
    σ = pm.HalfNormal('σ', sigma=10, dims="aa")

    # likelihood
    y = pm.Normal('y', mu=μ[idx], sigma=σ[idx], observed=diff) 

    idata_cs_h = pm.sample()
```

We are going to compare the results using ArviZ's `plot_forest` function. We can pass more than one model to this function. This is useful when we want to compare the values of parameters from different models such as with the present example. On @fig-csh_vs_csnh we have a plot for the 40 estimated means, one per amino acid (20) times the two models. We also have their 94% HDI and the inter-quantile range (the central 50% of the distribution). The vertical dashed line is the global mean according to the hierarchical model. This value is close to zero, as expected for theoretical values faithfully reproducing experimental ones. The most relevant part of this plot is that the estimates from the hierarchical model are pulled toward the partially-pooled mean, or equivalently they are shrunken in comparison to the un-pooled estimates. You will also notice that the effect is more notorious for those groups farther away from the mean (such as `PRO`) and that the uncertainty is on par or smaller than that from the non-hierarchical model. The estimates are partially pooled because we have one estimate for each group, but estimates for individual groups restrict each other through the hyper-prior. Therefore, we get an intermediate situation between having a single group, all chemical shifts together, and having 20 separated groups, one per amino acid. And that is ladies, gentlemen, and non-binary-gender-fluid people, the beauty of hierarchical models.

![Chemical shift differences for the hierarchical and non-hierarchical models.](fig/csh_vs_csnh.png){#fig-csh_vs_csnh}

## Water quality

Suppose we want to analyze the quality of water in a city, so we take samples by dividing the city into neighborhoods. We may think we have two options to analyze this data:

* Study each neighborhood as a separate entity
* Pool all the data together and estimate the water quality of the city as a single big group

Probably you already noticed the pattern here. We can justify the first option by saying we obtain a more detailed view of the problem, which otherwise could become invisible or less evident if we average the data. The second option can be justified by saying that if we pool the data, we obtain a bigger sample size and hence a more accurate estimation. But we already know we have a third option, we can do a hierarchical model!

For this example, we are going to use synthetic data. I love using synthetic data, it is a great way to understand things. If you don't understand something simulate it! There are many uses for synthetic data. Here we are going to imagine we have collected water samples from three different regions of the same city and we have measured the lead content of water; samples with concentrations of lead above recommendations from the World Health Organization (WHO) are marked with zero and samples with values below the recommendations are marked with one. This is a very simple scenario, in a more realistic example, we would have a continuous measurement of lead concentration and probably many more groups. Nevertheless, for our current purposes, this example is good enough to uncover the details of hierarchical models. We can generate the synthetic data with the following code:

```python
N_samples = [30, 30, 30]
G_samples = [18, 18, 18]
group_idx = np.repeat(np.arange(len(N_samples)), N_samples)
data = []
for i in range(0, len(N_samples)):
    data.extend(np.repeat([1, 0], [G_samples[i], N_samples[i]-G_samples[i]]))
```

We are simulating an experiment where we have measured three groups, each one consisting of a certain number of samples; we store the total number of samples per group in the `N_samples` list. Using the `G_samples` list, we keep a record of the number of good-quality samples per group. The rest of the code is there just to generate a list of the data, filled with zeros and ones.

The model for this problem is similar to the one we used for the coin problem, except for two important features:

* We have defined two hyper-priors that will influence the beta prior.
* Instead of putting hyper-priors on the parameters $\alpha$ and $\beta$, we are defining the beta distribution in terms of $\mu$, the mean, and $\nu$, the concentration (or precision) of the beta distribution. The precision is analog to the inverse of the standard deviation; the larger the value of $\nu$, the more concentrated the beta distribution will be. In statistical notation, our model is:

$$
\begin{aligned}
\mu &\sim \text{Beta}(\alpha_{\mu}, \beta_{\mu}) \\
\nu &\sim \mathcal{HN}(\sigma_{\nu}) \\
\theta_i &\sim \text{Beta}(\mu, \nu) \\
y_i &\sim \text{Bernoulli}(\theta_i)
\end{aligned}
$$


Notice we are using the subindex $i$ to indicate that the model has groups with different values for some of the parameters. That is, not all parameters are shared between the groups. Using Kruschke diagrams (see @fig-beta_binomial_hierarchical_dag), we can see that the new model has one additional level compared to the one from @fig-beta_binomial_dag. Notice also that for this model we are parametrizing the Beta prior distribution in terms of $\mu$ and $\nu$ instead of $\alpha$ and $\beta$. This is a common practice in Bayesian statistics, and it is done because $\mu$ and $\nu$ are more intuitive parameters than $\alpha$ and $\beta$. 

![Hierarchical model](/fig/beta_binomial_hierarchical_dag.pdf){#fig-beta_binomial_hierarchical_dag width=40%}

Let's write the model in PyMC:

```python
with pm.Model() as model_h:
    # hypyerpriors
    μ = pm.Beta('μ', 1, 1)
    ν = pm.HalfNormal('ν', 10)
    # prior
    θ = pm.Beta('θ', mu=μ, nu=ν, shape=len(N_samples))
    # likelihood
    y = pm.Bernoulli('y', p=θ[group_idx], observed=data)

    idata_h = pm.sample()
```

### Shrinkage

To show you one of the main consequences of hierarchical models, I will require your assistance, so please join me in a brief experiment. I will need you to print and save the summary computed with `az.summary(idata_h)`. Then, I want you to re-run the model two more
times after making small changes to the synthetic data. Remember to save the summary after each run. In total, we will have three runs:

* One run setting all the elements of G_samples to 18
* One run setting all the elements of G_samples to 3
* One last run setting one element to 18 and the other two to 3

Before continuing, please take a moment to think about the outcome of this experiment. Focus on the estimated mean value of $\theta$ in each experiment. Based on the first two runs of the model, could you predict the outcome for the third case?

If we put the result in a table, we get something more or less like this; remember that small variations could occur due to the stochastic nature of the sampling process:

| G_samples  |        Mean      |
|------------|------------------|
| 18, 18, 18 |  0.6,  0.6,  0.6 |
|  3,  3,  3 | 0.11, 0.11, 0.11 |
| 18,  3,  3 | 0.55, 0.13, 0.13 |

In the first row, we can see that for a dataset of 18 good samples out of 30 we get a mean value for $\theta$ of 0.6; remember that now the mean of $\theta$ is a vector of three elements, one per group. Then, on the second row, we have only 3 good samples out of 30 and the mean of $\theta$ is 0.11. These results should not be surprising, our estimates are practically the same as the empirical means. The interesting part comes in the third row. Instead of getting a mix of the mean estimates of $\theta$ from the other two rows, such as
0.6, 0.11, and 0.11, we get different values, namely 0.55, 0.13, and 0.13.

What on Earth happened? Did we make a mistake somewhere? Nothing like that. What we are seen is that the estimates shrunk toward the common mean. This is totally OK, indeed this is just a consequence of our model; by using hyper-priors, we are estimating the parameters of the Beta prior distribution from the data. Each group is informing the rest, and each group is informed by the estimation of the others. 

::: callout-note
In a hierarchical model groups sharing a common hyper-prior are effectively sharing information through the hyper-prior. This results in shrinkage, that is individual estimates are shrunk toward the common mean. By partially pooling the data; we are modeling the groups as some middle between the groups being independent from each other and being a single big group.
:::


## Hierarchies all the way up

<!-- ##  I want my hierarchies with many levels. SUBJECTS WITHIN CATEGORIES

If you, like me, are not a big sports fan and do not really care about the ability of
some guy to hit a ball with a stick, then translate the situation into something you do
care about. Instead of opportunities at bat, think of opportunities to cure a disease, for
treatments categorized into types. We are interested in estimating the probability of cure
for each treatment and each type of treatment and overall across treatments. Or think
of opportunities for students to graduate from high school, with schools categorized by
districts. We are interested in estimating the probability of graduation for each school
and each district and overall. Or, if that’s boring for you, keep thinking about guys
swinging sticks.

https://github.com/JWarmenhoven/DBDA-python/blob/master/Notebooks/data/BattingAverage.csv

https://github.com/JWarmenhoven/DBDA-python/blob/master/Notebooks/Chapter%209.ipynb
 -->


::: callout-note
Of course, is also possible to put priors over the hyper-priors and create as many levels as we want. But unless the problem necessitates additional structure, adding more levels than required does not enhance the quality of our model or inferences. Instead, we will get entangled in a web of hyper-priors and hyperparameters without the ability to assign any meaningful interpretation to them. The goal of building models is to make sense of data, and thus models should reflect, and take advantage of, the structure of the data.
:::



## Summary

In this chapter we have presented one of the most important concepts to learn from this book: hierarchical models. We can build hierarchical models every time we can identify subgroups in our data. In such cases, instead of treating the subgroups as separated entities or ignoring the subgroups and treating them as a single group, we can build a model to partially pool information among groups. The main effect of this partial pooling is that the estimates of each subgroup will be biased by the estimates of the rest of the subgroups. This effect is known as shrinkage, and in general, is a very useful trick that helps to improve inferences by making them more conservative (as each subgroup informs the others by pulling estimates toward it) and more informative. We get estimates at the subgroup level and the group level.

Paraphrasing the Zen of Python, we can certainly say, *hierarchical models are one honking great idea—let's do more of those!* In the following chapters, we will keep building hierarchical models and learn how to use them to build better models. We will also discuss how hierarchical models are related to the pervasive overfitting/underfitting issue in statistics and machine learning in @sec-model_comparison. In @sec-inference_engines, we will discuss some technical problems that we may find when sampling from hierarchical models and how to diagnose and fix those problems.



## Exercises


<!-- 
7. Repeat the exercise we did with model_h. This time, without a hierarchical
structure, use a flat prior such as
. Compare the results of both
models.
8. Create a hierarchical version of the tips example by partially pooling across the
days of the week. Compare the results to those obtained without the hierarchical
structure.
9. PyMC3 can create directed acyclic graphs (DAGs) from models that are very
similar to Kruschke's diagrams. You can obtain them using the
pm.model_to_graphviz() function. Generate a DAG for each model in this
chapter. -->

