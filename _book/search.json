[
  {
    "objectID": "chp_01.html#statistics-models-and-this-books-approach",
    "href": "chp_01.html#statistics-models-and-this-books-approach",
    "title": "1  Thinking Probabilistically",
    "section": "1.1 Statistics, models, and this book’s approach",
    "text": "1.1 Statistics, models, and this book’s approach\nStatistics is about collecting, organizing, analyzing, and interpreting data, and hence statistical knowledge is essential for data analysis. Two main statistical methods are used in data analysis:\n\nExploratory Data Analysis (EDA): This is about numerical summaries, such as the mean, mode, standard deviation, and interquartile ranges (this part of EDA is also known as descriptive statistics). EDA is also about visually inspecting the data, using tools you may be already familiar with, such as histograms and scatter plots. *Inferential statistics: This is about making statements beyond the current data. We may want to understand some particular phenomenon, or maybe we want to make predictions for future (as yet unobserved) data points, or we need to choose among several competing explanations for the same observations. Inferential statistics is the set of methods and tools that will help us to answer these types of questions.\n\n\n\n\n\n\n\nNote\n\n\n\nThe focus of this book is on how to perform Bayesian inferential statistics, and then how to use EDA to summarize, interpret, check, and communicate the results of Bayesian inference.\n\n\nMost introductory statistical courses, at least for non-statisticians, are taught as a collection of recipes that go like this: go to the statistical pantry, pick one tin can and open it, add data to taste, and stir until you obtain a consistent p-value, preferably under 0.05. The main goal of these courses is to teach you how to pick the proper can. I never liked this approach, mainly because the most common result is a bunch of confused people unable to grasp, even at the conceptual level, the unity of the different learned methods. We will take a different approach: we will also learn some recipes, but these will be homemade rather than canned food; we will learn how to mix fresh ingredients that will suit different gastronomic occasions and, more importantly, that will let you apply concepts far beyond the examples in this book.\nTaking this approach is possible for two reasons:\n\nOntological: Statistics is a form of modeling unified under the mathematical framework of probability theory. Using a probabilistic approach provides a unified view of what may seem like very disparate methods; statistical methods and machine learning methods look much more similar under the probabilistic lens.\nTechnical: Modern software, such as PyMC, allows practitioners, just like you and me, to define and solve models in a relatively easy way. Many of these models were unsolvable just a few years ago or required a high level of mathematical and technical sophistication."
  },
  {
    "objectID": "chp_01.html#working-with-data",
    "href": "chp_01.html#working-with-data",
    "title": "1  Thinking Probabilistically",
    "section": "1.2 Working with data",
    "text": "1.2 Working with data\nData is an essential ingredient in statistics and data science. Data comes from several sources, such as experiments, computer simulations, surveys, and field observations. If we are the ones in charge of generating or gathering the data, it is always a good idea to first think carefully about the questions we want to answer and which methods we will use, and only then proceed to get the data. There is a whole branch of statistics dealing with data collection, known as experimental design. In the era of the data deluge, we can sometimes forget that gathering data is not always cheap. For example, while it is true that the Large Hadron Collider (LHC) produces hundreds of terabytes a day, its construction took years of manual and intellectual labor.\nAs a general rule, we can think of the process generating the data as stochastic, because there is ontological, technical, and/or epistemic uncertainty, that is, the system is intrinsically stochastic, there are technical issues adding noise or restricting us from measuring with arbitrary precision, and/or there are conceptual limitations veiling details from us. For all these reasons, we always need to interpret data in the context of models, including mental and formal ones. Data does not speak but through models.\nIn this book, we will assume that we already have collected the data. Our data will also be clean and tidy, something rarely true in the real world. We will make these assumptions to focus on the subject of this book. I just want to emphasize, especially for newcomers to data analysis, that even when not covered in this book, these are important skills that you should learn and practice to successfully work with data.\nA very useful skill when analyzing data is knowing how to write code in a programming language, such as Python. Manipulating data is usually necessary given that we live in a messy world with even messier data, and coding helps to get things done. Even if you are lucky and your data is very clean and tidy, coding will still be very useful since modern Bayesian statistics is done mostly through programming languages such as Python or R. If you want to learn how to use Python for cleaning and manipulating data, you can find a good introduction in Python for Data Analysis by (2022)"
  },
  {
    "objectID": "chp_01.html#bayesian-modeling",
    "href": "chp_01.html#bayesian-modeling",
    "title": "1  Thinking Probabilistically",
    "section": "1.3 Bayesian modeling",
    "text": "1.3 Bayesian modeling\nModels are simplified descriptions of a given system or process that, for some reason, we are interested in. Those descriptions are deliberately designed to capture only the most relevant aspects of the system and not to explain every minor detail. This is one reason a more complex model is not always a better one. There are many different kinds of models; in this book, we will restrict ourselves to Bayesian models. We can summarize the Bayesian modeling process using three steps:\n\nGiven some data and some assumptions on how this data could have been generated, we design a model by combining building blocks known as probability distributions. Most of the time these models are crude approximations, but most of the time that’s all we need.\nWe use Bayes’ theorem to add data to our models and derive the logical consequences of combining the data and our assumptions. We say we are conditioning the model on our data.\nWe evaluate the model, and its predictions, under different criteria, including the data, our expertise on the subject, and sometimes by comparing it to other models.\n\nIn general, we will find ourselves performing these three steps in an iterative non-linear fashion. We will retrace our steps at any given point: maybe we made a silly coding mistake, or we found a way to change the model and improve it, or we realized that we need to add more data or collect a different kind of data.\nBayesian models are also known as probabilistic models because they are built using probabilities. Why probabilities? Because probabilities are a very useful tool to model uncertainty, we even have very good arguments to state they are the correct mathematical concept. So let’s take a walk through the garden of forking paths (borges_2018?)."
  },
  {
    "objectID": "chp_01.html#probability-theory",
    "href": "chp_01.html#probability-theory",
    "title": "1  Thinking Probabilistically",
    "section": "1.4 Probability theory",
    "text": "1.4 Probability theory\nThe title of this section may be a little bit pretentious, it is not possible to learn probability theory in just a few pages, but that is not my intention. I just want to introduce a few general and important concepts that are necessary to better understand Bayesian methods, and that should be enough for understanding the rest of the book. If necessary, we will expand or introduce new probability-related concepts as we need them. For a detailed study of probability theory, I highly recommend the book, Introduction to Probability by Joseph K Blitzstein and Jessica Hwang (blitzstein_2019?).\n\n1.4.1 Sample Spaces and Events\nLet’s say we are conducting a survey to see how people feel about the weather in their area. We ask three individuals whether they enjoy sunny weather, with possible responses being “yes” or “no”. The sample space of all possible outcomes can be denoted by \\(S\\) and consists of eight possible combinations:\n\\(S\\) = {(yes, yes, yes), (yes, yes, no), (yes, no, yes), (no, yes, yes), (yes, no, no), (no, yes, no), (no, no, yes), (no, no, no)}\nHere, each element of the sample space represents the responses of the three individuals in the order they were asked. For example, (yes, no, yes) means the first and third people answered “yes” while the second person answered “no”.\nWe can define events as subsets of the sample space, such as the event \\(A\\) being that all three individuals answered “yes”:\n\\(A\\) = {(yes, yes, yes)}\nSimilarly, we can define event \\(B\\) that at least one person answered “no”, and then we will have:\n\\(B\\) = {(yes, yes, no), (yes, no, yes), (no, yes, yes), (yes, no, no), (no, yes, no), (no, no, yes), (no, no, no)}\nWe can use probabilities as a measure of how likelly is a particular event. If the event is certain, for instance we go outside and we see that is actually raining, then the probability of that event will be 1 and if there is not rain 0. Most interesting things happend when probabilities take numbers in between. Let us see.\nGiven the sample space \\(S\\), the probability of event A, which is the event that all three individuals answered “yes”, is:\n$$P(A) = \nIn this case, there is only one outcome in A, and there are eight outcomes in \\(S\\). Therefore, the probability of \\(A\\) is:\n\\[P(A) = \\frac{1}{8} = 0.125\\]\nSimilarly, we can calculate the probability of event \\(B\\), which is the event that at least one person answered “no”. Since there are seven outcomes in \\(B\\) and eight outcomes in \\(S\\), the probability of \\(B\\) is:\n\\[P(B) = 7/8 = 0.875\\]\nNotice that for these computations we have assumed that all event in \\(S\\) are equally likely, but that’s not need to be like that. Considering all event equally likely is just a particular case.\nNow let us suppose we ask one person whether it will rain tomorrow, with possible responses “yes” and “no”. The sample space of possible responses is \\(S = {\\text{yes}, \\text{no}}\\). An the even that it will rain tomorrow is \\(A = {\\text{yes}}\\)\nThe axioms of probability apply as follows:\nNon-negativity: The probability of any event is a non-negative number. For example, the probability of event A, that it will rain tomorrow, is P(A) = 1/2, which is a non-negative number.\n\nNormalization: The probability of the entire sample space is equal to 1. In this case, the probability of the sample space S = {yes, no} is P(S) = 1, since it will either rain or not rain tomorrow.\n\nAdditivity: The probability of the union of any two disjoint events is equal to the sum of their individual probabilities. For example, let's consider two events: A, which is the event that it will rain tomorrow, and B, which is the event that it will not rain tomorrow. These events are disjoint, meaning they cannot occur simultaneously. Therefore, the probability of their union (i.e., the event that either A or B occurs) is:\nP(A U B) = P(A) + P(B)\nSince B is the complement of A, meaning that it will either rain or not rain tomorrow, we have P(B) = 1 - P(A). Therefore:\nP(A U B) = P(A) + P(B) = P(A) + (1 - P(A)) = 1\nSince P(A U B) is equal to 1, the axiom of additivity is also satisfied in this example.\nNevertheless there is more that one\nThis means that if we were to repeat the survey many times, we would expect all three individuals to answer “yes” about 12.5% of the time.\nSimilarly, we can calculate the probability of event B, which is the event that at least one person answered “no”. Since there are seven outcomes in B and eight outcomes in S, the probability of B is:\nP(B) = 7/8 = 0.875\nThis means that if we were to repeat the survey many times, we would expect at least one person to answer “no” about 87.5% of the time.\nWe can use probabilities to discribe how likelly an event is. If the event is certain, we go outside and is raining, then p(R) = 1 if there is no rain p(R) = 0. Things becomes more interesting for values between 0 and 1. For instance if we see dark clouds and hear thunder we may say I think is veru likely that it will rain in the next few minutes and to formalize this idea we can assign a number between 0 and 1 to p(R), with the number being closer to 1 the more likely we consider rain to happend. This is what whether forescast do, but usually they use %, i.e. the scale goes from 0 to 100, but the principle is the same. We can think of pobabilities as a conserved quantity. In the sense that if all that cvan happend is desctribed by \\(S\\) the p(S) = 1, i.e. something must happend and evne more if all that can happend is R and not-R then if p(R) increases p(not-R) should decrease by the same amount so that the total probability of both evnet remainS\nProbabilities are numbers in the \\([0, 1]\\) interval, that is, numbers between 0 and 1, including both extremes, and they folow certain rules.\nProbabilities follow a few rules; on such rule is the product rule:\n\\[p(A, B) = p(A \\mid B) p(B)\\]\nWe read this as follows: the probability of \\(A\\) and \\(B\\) is equal to the probability of \\(A\\) given \\(B\\), times the probability of \\(B\\)."
  },
  {
    "objectID": "chp_01.html#interpreting-probabilities",
    "href": "chp_01.html#interpreting-probabilities",
    "title": "1  Thinking Probabilistically",
    "section": "1.5 Interpreting probabilities",
    "text": "1.5 Interpreting probabilities\nWhile probability theory is a mature and well-established branch of mathematics, there is more than one interpretation of probability. From a Bayesian perspective, a probability is a measure that quantifies the uncertainty level of a statement. Under this definition of probability, it is totally valid and natural to ask about the probability of life on Mars, the probability of the mass of the electron being 9.1 x 10-31 kg, or the probability of the 9th of July of 1816 being a sunny day in Buenos Aires. Notice, for example, that life on Mars exists or does not exist; the outcome is binary, a yes-no question. But given that we are not sure about that fact, a sensible course of action is trying to find out how likely life on Mars is. Since this definition of probability is related to our epistemic state of mind, people often call it the subjective definition of probability. But notice that any scientific-minded person will not use their personal beliefs, or the information provided by an angel to answer such a question, instead they will use all the relevant geophysical data about Mars, and all the relevant biochemical knowledge about necessary conditions for life, and so on. Therefore, Bayesian probabilities, and by extension Bayesian statistics, is as subjective (or objective) as any other well-established scientific method we have. If we do not have information about a problem, it is reasonable to state that every possible event is equally likely, formally this is equivalent to assigning the same probability to every possible event. In the absence of information, our uncertainty is maximum. If we know instead that some events are more likely, then this can be formally represented by assigning higher probability to those events and less to the others. Notice than when we talk about events in stats-speak, we are not restricting ourselves to things that can happen, such as an asteroid crashing into Earth or my auntie’s 60th birthday party; an event is just any of the possible values (or subset of values) a variable can take, such as the event that you are older than 30, or the price of a Sachertorte, or the number of bikes sold last year around the world.\nThe concept of probability it is also related to the subject of logic. Under Aristotelian or classical logic, we can only have statements that take the values of true or false. Under the Bayesian definition of probability, certainty is just a special case: a true statement has a probability of 1, a false statement has probability of 0. We would assign a probability of 1 to the statement, There is Martian life, only after having conclusive data indicating something is growing, and reproducing, and doing other activities we associate with living organisms. Notice, however, that assigning a probability of 0 is harder because we can always think that there is some Martian spot that is unexplored, or that we have made mistakes with some experiment, or several other reasons that could lead us to falsely believe life is absent on Mars even when it is not. Related to this point is Cromwell’s rule, stating that we should reserve the use of the prior probabilities of 0 or 1 to logically true or false statements. Interestingly enough, Richard Cox mathematically proved that if we want to extend logic to include uncertainty, we must use probabilities and probability theory. Bayes’ theorem is just a logical consequence of the rules of probability, as we will see soon. Hence, another way of thinking about Bayesian statistics is as an extension of logic when dealing with uncertainty, something that clearly has nothing to do with subjective reasoning in the pejorative sense—people often used the term subjective. To summarize, using probability to model uncertainty is not necessarily related to the debate about whether nature is deterministic or random at is most fundamental level, nor is related to subjective personal beliefs. Instead, it is a purely methodological approach to model uncertainty. We recognize most phenomena are difficult to grasp because we generally have to deal with incomplete and/or noisy data, we are intrinsically limited by our evolution-sculpted primate brain, or any other sound reason you could add. As a consequence, we use a modeling approach that explicitly takes uncertainty into account.\n\n\n\n\n\n\nNote\n\n\n\nFrom a practical point of view, the most relevant piece of information from this section is that Bayesian’s use probabilities as a tool to quantify uncertainty.\n\n\n\n\n\n\nMcKinney, Wes. 2022. Python for Data Analysis: Data Wrangling with Pandas, NumPy, and Jupyter. Beijing Boston Farnham Sebastopol Tokyo."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "McKinney, Wes. 2022. Python for Data\nAnalysis: Data Wrangling with\nPandas, NumPy, and Jupyter. Beijing\nBoston Farnham Sebastopol Tokyo."
  }
]