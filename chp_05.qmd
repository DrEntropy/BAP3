# Advance Modeling with Lines {#sec-advance_linear}

> Something, Something

In the last chapter we describe the basic ingredients of linear regression models and how to generalize them to better fit our needs. In this chapter we are going to keep learning about linear models, but this time we are going to work with Bambi, a high-level Bayesian model-building interface written on top of PyMC. Bambi is designed to make it extremely easy to fit linear models, including hierarchical ones. We will see we that Bambi's domain is actually more extense that just linear models. In this chapter we are going to learn about:


* How to use Bambi to build and fit models
* How to analyze results with Bambi
* Polynomial regression and splines
* Distributional models
* Variable selection with Kulprit

## One syntax to rule them all

PyMC has a very simple and expresive syntax, that allows to buil arbitrary models. That's usually a bless, but it can be a burden too. Bambi instead focus on models for regression, this allows it to have a more restricted, but focused syntax. Additionally Bambi has an internal representation of the models that it is different from the one used by PyMC, this makes some analysis tasks easier with Bambi as we will see.

Bambi uses a formula-based syntax similar to the one in R packages like lme4 or brms. Let's assume `data` is a Panda's DataFrame like this:

|    |         y |          x |         z | g       |
|---:|----------:|-----------:|----------:|:--------|
|  0 | -0.633494 | -0.196436  | -0.355148 | Group A |
|  1 |  2.32684  |  0.0163941 | -1.22847  | Group B |
|  2 |  0.999604 |  0.107602  | -0.391528 | Group C |
|  3 | -0.119111 |  0.804268  |  0.967253 | Group A |
|  4 |  2.07504  |  0.991417  |  0.590832 | Group B |
|  5 | -0.412135 |  0.691132  | -2.13044  | Group C |

and we want to build a linear model that predicts `y` from `x`. Using PyMC we would do something like:

```python
with pm.Model() as lm:
    Intercept = pm.Normal("Intercept", 0, 1)
    x = pm.Normal("x", 0, 1)
    y_sigma = pm.HalfNormal("sigma", 1)
    y_mean = Intercept + x * data["x"]
    y = pm.Normal("y", y_mean, y_sigma, observed=data["y"])
```

The formula syntax used by Bambi allows us to define an equivalent model in much more compact way:

```python 
a_model = bmb.Model("y ~ x", data)
```

On the left side of the `~` we have the dependent variable, and on the right side the independent variable(s). With this syntax we are just specifying the mean ($\mu$ in the PyMC model `lm`). By default Bambi assumes the likelihood is Gaussian (you can change this with the `family` argument) and that we want to model both an intercept and a slope. The formula syntax does not specify, priors just how dependent and independent variables are related. Bambi will automatically define (very) weackly informative priors for us. We can get more information by printing the model, you should get something like this:

```
    Formula: y ~ x
    Family: gaussian
        Link: mu = identity
Observations: 117
    Priors: 
target = mu
    Common-level effects
        Intercept ~ Normal(mu: 0.02, sigma: 2.8414)
        x ~ Normal(mu: 0.0, sigma: 3.1104)
    
    Auxiliary parameters
        sigma ~ HalfStudentT(nu: 4.0, sigma: 1.1348)
```

The first line shows the formula used to define the model, the second one the likelihood family, and the third one the link function. The next line shows the number of observations used to fit the model, and the next is telling us we are linearly modeling the parameter `mu` of the Gaussian. The last part of the output shows the model structure. The first part shows the common-level effects, in this case the intercept (`Intercept`) and the slope (`x`). The second part shows the auxiliary parameters, in this case the standard deviation of the Gaussian.

If you want to ommit the intercept you can do it like this:

```python
no_intercept_model = bmb.Model("y ~ 0 + x", data)
```

or like this:

```python
no_intercept_model = bmb.Model("y ~ -1 + x", data)
```

Print the model `no_intercept_model`, and you will see that the intercept is not there anymore. What if we want to include more variables? We can do it like this:

```python
model_2 = bmb.Model("y ~ x + z", data)
```

And we can also include group-level effects (hierarchies), for example, if we want to use the varible `g` to partially pool the estimates of `x` we can do it like this:

```python
model_h = bmb.Model("y ~ x + z + (x | g)", data)
```
We can see a visual representation of this model in Figure @fig-bambi_dag. Notice the variables `1|g_offset` and `x|g_offset`, by defaul Bambi fits a noncentered hierarchical model, you can change this with the argument `noncentered`. 

![A visual representation of `model_h`](/fig/bambi_dag.png){#fig-bambi_dag}

The formula syntax is very simple, but it is also very powerful. We have just scrath the surface of what we can do with it. Instead of describing the syntax all at once, we are going to show it by examples. So let's start by fitting the bikes model from the previous chapter.


## The bikes model Bambi's version

Let's start by loading the data:

```python
bikes = pd.read_csv("data/bikes.csv")
```

Now we can build and fit the model:

```python
model_t = bmb.Model("rented ~ temperature", bikes, family="negativebinomial")
idata_t = model.fit()
```

Figure @fig-bambi_linear_bikes_dag shows a visual representation of the model. If you want to visually inspect the priors you can do `model.plot_priors()`.

![A visual representation of the bikes model, computed with the command `model.graph()`](/fig/bambi_linear_bikes_dag.png){#fig-bambi_linear_bikes_dag width=80%}

Let's know plot the posterior mean and the posterior predictive distribution (predictions). Omiting some details needed to make the plots look nice, the code to do this is:

```python
_, axes = plt.subplots(1, 2, sharey=True, figsize=(12, 4)) 
plot_cap(model_t, idata_t, "temperature", ax=axes[0])
plot_cap(model_t, idata_t, "temperature", pps=True, ax=axes[1])
```

Where `plot_cap` is a function from Bambi's submodel `plots`. The name comes from conditional adjusted predictions. The objective of plotting conditional adjusted predictions is to visualize how a parameter of the (conditional) response distribution varies as a function of (some) interpolated explanatory variables. We can see the result of this code in Figure @fig-bambi_linear_bikes_mean_pss. The left panel shows the posterior mean and the 94% HDI, and the right panel shows the posterior predictive distribution (the predicted distribution of the rented bikes). Notice that the uncertainty for the predictions is much larger than the uncertainty for the mean (`pps=False`). This is because the posterior predictive distribution accounts for the uncertainty in the model parameters and the uncertainty in the data. Whereas, the posterior distribution of the mean only accounts for the uncertainty in the intercept and slope parameters.

![Posterior mean and posterior predictive distribution for the bikes model](/fig/bambi_linear_bikes_mean_pss.png){#fig-bambi_linear_bikes_mean_pss}

The utility of `plot_cap` becomes more evident when we have more than one explanatory variable. For example, let's fit a model that uses both temperature and humidity to predict the number of rented bikes:

```python
model_th = bmb.Model("rented ~ temperature + humidity", bikes, family="negativebinomial")
idata_th = model_th.fit()
plot_cap(model_th, idata_th, {"horizontal": "temperature", "panel": "humidity"},
         fig_kwargs={"sharey":True, "sharex":True});
``` 

On Figure @fig-bambi_linear_bikes_th_mean we can see five panel, each one showing the change of the number of rented bikes with temperature at different values of `humidity`. We can see that the number always increase but the slope is larger when humidity is low. 

![Posterior mean for the bikes model with temperature and humidity](/fig/bambi_linear_bikes_th_mean.png){#fig-bambi_linear_bikes_th_mean}

<!-- We can also use `plot_cap` to visualize the effect of group-level effects. For example, let's fit a model that uses the variable `weekday` to partially pool the estimates of the intercept and the slope of `temperature`:

```python
model_tg = bmb.Model("rented ~ temperature + (temperature | weekday)", bikes, family="negativebinomial")
idata_tg = model_tg.fit()
plot_cap(model_tg, idata_tg, {"horizontal": "temperature", "panel": "weekday"},
         fig_kwargs={"sharey":True, "sharex":True});
``` -->

## Polynomial regression

One way to fit curves using a linear regression model is by building a polynomial, like this:


$$
\mu = \beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3 + \beta_4 x^4 \dots \beta_m x^m
$$

We call $m$ the degree of the polynomial.

There are two important thinks to notice. First, polynomial regression is still linear regression; the linearity refers to the coefficients (the $\beta$s), not the variables (the $x$s). The second one is that we are just creating new variables out of thin air. The only observed variables is `x`, the rest are just powers of `x`. Creating new variables from observed ones is a perfectly valid "trick" when doing regression, sometimes the transformation can be motivated or justified by theory (like taking the square root of the lenght of babies), but sometimes it is just a way to fit a curve. The intuition with polynomials is that for a given value of `x`, the higher the degree of the polynomial the more flexible the curve can be. For a simple linear model with Gaussian likelihood, the posterior mean is a straight line for a polynomial of degree 1 (a line), a curve that can go up or down for degree 2, a curve that can go up and then down (or the otherway) for degree 3, and so for. Notice I said "can" because if we have a polynomial of degree 3 like $\beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3$ but the coefficients $\beta_2$ and $\beta_3$ are 0 (or practically 0), then the curve will be a line. 

Doing polynomial regression with Bambi is super easy, we just need to use the `poly` function and optionally specify the degree of the polynomial. For example, let's fit a polynomial of degree 4 to the bikes data, but this time let's use the `hour` variable:

```python
model_poly = bmb.Model("rented ~ poly(temperature, 4)", bikes, family="negativebinomial")
idata_poly = model_poly.fit()
```

Figure @fig-bambi_poly_bikes_mean_pss shows the posterior mean and the posterior predictive distribution for a linear model, or polynomial of degree 1, if you want (to panels) and the polynomial model with degree 4 (bottom panels). 


![Posterior mean and posterior predictive distribution for the bikes model with temperature and humidity](/fig/bambi_poly_bikes_mean_pss.png){#fig-bambi_poly_bikes_mean_pss}

One problem with polynomials is that they act globally, when we apply a polynomial of degree $m$ we are saying that the relationship between the independent and dependent variables is of degree $m$ for the entire dataset. This can be problematic when different regions of our data need different levels of flexibility. This could lead, for example, to curves that are too flexible. As the degree increases the fit becomes more sensitive to the removal of points, or equivalently to the addition of future data. In other words as the degree increases, the model becomes more prone to overfitting. Bayesian polynomial regression usually suffers less of this "excess" of flexibility because we usually use don't use flat priors, and we do not compute a single set of coefficients, but the entire posterior distribution. Still, we can do better.

## Splines 

A general way to write very flexible models is to aply functions $B_{m}$ to $X_{m}$ and then multiply them by coefficients $\beta_m$:

$$
\mu = \beta_0 + \beta_1 B_{1}(X_{1}) + \beta_2 B_{2}(X_{2}) + \cdots + \beta_m B_{m}(X_{m})
$$

We are free to pick $B_{m}$ as we wish, for instance we can pick polynomials. But we can also pick other functions. A popular choice is to use B-splines, we are not going to discuss their definition, but we can think of them as a way to create smooth curves in such a way that we get flexibility, as with polynomials, but less prone to overfitting. We achieve this by using piecewise polynomials, that is, polynomials that are restricted to affect only a portion of the data. Figure @fig-piecewise show 3 examples of piecewise polynomials of increasing degree. The dotted vertical lines shows the "knots" this are the points used to restrict the regions, the dashed grey line represents the True function that we want to approximate and the black lines the piecewise polynomials.


![Piecewise polynomials of increasing degree](/fig/piecewise.png){#fig-piecewise}


Figure @fig-splines_weighted shows examples of splines of degree 1 and 3, the dots at the bottom represent the knots, and the dashed lines the B-splines. At the top we have all the B-splines with equal weight, we use a gray scale to highlight that we have many B-splines. On the bottom panel each B-splines is weighted differently (we multiply them by $\beta_m$ coefficients), if we sum the weighted B-splines we get the black line as a result. This black line is what we usually call "the spline". We can use Bayesian statistics to find the proper weights for the B-splines.

![B-splines of degree 1 (piecewise linear) or 3 (cubic spline) and the resulting splines](/fig/splines_weighted.png){#fig-splines_weighted}


We can use B-splines with Bambi by using the `bs` function. For example, let's fit a spline of degree 3 to the bikes data:

```python
num_knots = 6
knots = np.linspace(0, 23, num_knots+2)[1:-1]
model_spline = bmb.Model("rented ~ bs(hour, degree=3, knots=knots)", bikes, family="negativebinomial")
idata_spline = model_spline.fit()
```

 From Figure @fig-bambi_spline_bikes we can see that the number of rental bikes is at the lowest number late at night. There is then an increase, probably as people wake up and go to work, school, or other activities. We have a first peak at around hour 8, then a slight decline, followed by the second peak at around hour 18, probably becasue people commute back home, after which there a steady decline. Notice that the curve is not very smooth, this is not because of the spline but because of the data. We have measurments at discrete times (every hour).

![Posterior mean for the spline model](/fig/bambi_spline_bikes.png){#fig-bambi_spline_bikes}


When working with splines, one important decision we must make is determining the number and placement of knots. This can be a somewhat daunting task since the optimal number of knots and their spacing are not immediately apparent. A useful suggestion for determining the knot locations is to consider placing them based on quantiles rather than uniformly. Something line `knots = np.quantile(bikes.hour, np.linspace(0, 1, num_knots))`. By doing so, we would position more knots in areas where we have a greater amount of data, while placing fewer knots in areas with less data. This results in a more adaptable approximation that effectively captures the variability in regions with a higher density of data points. Additionally we may wat to fit splines with varying number of knots and positions and them evaluate the results. In the next Chapter @sec-model_comparison we discuss method to compare models.


## Distributional Models

We earlier saw that we can use linear models for parameters other than the mean (or location parameter). For example, we can use a linear model for the mean and a linear model for the standard deviation of a Gaussian distribution. This models are usually callen distributional models. The syntax for distributional models is very similar, we just need to add a line for the auxiliary parameters we want to model. For instance sigma for a Gaussian, or alpha for a negative binomial.

Let's know reproduce an example from the previous chapter, the babies example.

```python
formula = bmb.Formula(
    "length ~ np.sqrt(month)",
    "sigma ~ month"
)
model_dis = bmb.Model(formula, babies)
idata_dis = model_dis.fit()
```

From Figure @fig-babies_bambi_varying we can see the posterior distribution values of sigma for `model_dis` (varying sigma) and for a model with constant sigma. We can see that when sigma is allow to vary we obtain values below and above the estimate for a constant sigma, meaning that we are both under and over estimating this paramter when we don't alow to change. 

![Constant and varying sigma for the babies data](/fig/babies_bambi_varying.png){#fig-babies_bambi_varying}

Figure @fig-babies_bambi_fit shows the posterior fit for `model_dis`. Notice that the model is able to capture the increase in variability as the babies grow. This figure is very similat to @fig-babies_fit.

![Posterior fit for `model_dis`.](/fig/babies_bambi_fit.png){#fig-babies_bambi_fit}

When working with PyMC we saw that we need to define `Mutable data`, if we then want to sample at not obvserved values, with Bambi this is not necessary. We can use the `predict` method to predict at new values. For example, let's predict the length of a baby at 0.5 months (15 days):

```python
model_dis.predict(idata_dis, kind="pps", data=pd.DataFrame({"month":[0.5]}))
```

## Variable selection with Kulprit
