# Comparing Models {#sec-model_comparison}

> "A map is not the territory it represents, but, if correct, it has a similar structure to the territory." -Alfred Korzybski

Models should be designed as approximations to help us understand a particular problem, or a class of related problems. Models are not designed to be verbatim copies of the _real world_. Thus, all models are wrong in the same sense that maps are not the territory. Even when a priori, we consider every model to be wrong, not every model is equally wrong; some models will be better than others at describing a given problem. In the foregoing chapters, we focused our attention on the inference problem, that is, how to learn values of parameters from the data. In this chapter, we are going to focus on a complementary problem: how to compare two or more models that are used to explain the same data. As we will learn, this is not a simple problem to solve and at the same time is a central problem
in data analysis.

In this chapter, we will explore the following topics:

* Overfitting and underfitting
* Information criteria
* Cross-validation
* Bayes factors


## Posterior predictive tests 

We have previously introduced and discussed posterior predictive tests as a way to assess how well models explain the same data used to fit a model. The purpose of this type of testing is not to determine that a model is incorrect; We already know this! The goal of the exercise is to understand how well we are capturing the data. We often capture different aspects of data in different ways. By performing posterior predictive tests, we aim to better understand the limitations of a model. Once we undestand the limitations we can simply acknoledgment them or try to remove them by improve the model. It is expected that a model will not be able to reproduce all aspects of a problem and this is usually not a problem as models are built with a purpose in mind. Posterior predictive tests are ne way to evaluate that purpose, so if we have more than one model, we can compare them using posteriori predictive tests.


Let's look at a simple example. We have a data set with two variables `x` and `y`. We are going to fit these data with a linear model:

$$
y = \alpha + \beta x
$$

And a quadratic model, that is a model with one more term than the linear model. For this extra term we just take `x` to the power of 2 and add a $\beta$ coefficient, that is:
$$
y = \alpha + \beta_0 x + \beta_1 x^2
$$

We can write in PyMC this models as usual, see the next block of code. The only difference is that we pass the argument `idata_kwargs={"log_likelihood": True}` to `pm.sample`. This will store the log likelihood in the `InferenceData` object, we will use this later.

```python
with pm.Model() as model_l:
    α = pm.Normal('α', mu=0, sigma=1)
    β = pm.Normal('β', mu=0, sigma=10)
    σ = pm.HalfNormal('σ', 5)

    μ = α + β * x_c[0]
    
    y_pred = pm.Normal('y_pred', mu=μ, sigma=σ, observed=y_c)

    idata_l = pm.sample(2000, idata_kwargs={"log_likelihood": True})
    idata_l.extend(pm.sample_posterior_predictive(idata_l))

with pm.Model() as model_q:
    α = pm.Normal('α', mu=0, sigma=1)
    β = pm.Normal('β', mu=0, sigma=10, shape=order)
    σ = pm.HalfNormal('σ', 5)

    μ = α + pm.math.dot(β, x_c)
    
    y_pred = pm.Normal('y_pred', mu=μ, sigma=σ, observed=y_c)

    idata_q = pm.sample(2000, idata_kwargs={"log_likelihood": True})
    idata_q.extend(pm.sample_posterior_predictive(idata_q))
```

Figure @fig-lin-pol-fit shows the mean fit from both models. 
    
![Mean fit for `model_l` (linear) and `model_q`](/fig/lin-pol-fit.png){#fig-lin-pol-fit}
    
Visually, both models seems to provide a resonable fit to the data. To gain further insight we can do a posterior predictive check, Figure @fig-lin-pol-ppc shows KDEs for the observed and predicted data. Here it becomes easier to see that `model_q`, the quadratic model, provides a better fit to the data. We can also see there is a lot of uncertainty, in particular at the tails of the distributions, this is because we have a small number of data points.

![Posterior predictive checks for `model_l` and `model_q` created with `az.plot_ppc` function](/fig/lin-pol-ppc.png){#fig-lin-pol-ppc}

Posterior predictive checks are a very verstail idea. We can compare observed and predicted data in so many ways. Instead of comparing the densities of the distributions we can compare summary statistics. 


For instance on the left panel of Figure @fig-lin-pol-bpv we have the distributions of means for both models. The dot over the x-axis indicates the observed value. We can see that both model captures the mean very well, with the quadratic model having less variance. That both models capture the mean very well is not surprising as we are explicitelly modeling the mean with both models. On the right panel we have the distributions of interquartile range, this comparisons favor the linear model instead.

![Posterior predictive checks for `model_l` and `model_q` created with `az.plot_bpv` function](/fig/lin-pol-bpv.png){#fig-lin-pol-bpv}

In general, a statistic that is _orthogonal_ to what the model is explicitelly modeling will be more informative for evaluating the model. When in doubt, it may be convenient to evaluate more than one statistic. A useful question is to ask yourself what aspects of the data you are interested in capturing.

To generate Figure @fig-lin-pol-bpv we used the ArviZ function `az.plot_bpv`. An extract of the full code to generate that figure is the following:

```python
idatas = [idata_l, idata_q]

def iqr(x, a=-1):
    """interquartile range"""
    return np.subtract(*np.percentile(x, [75, 25], axis=a))

for idata in idatas:
    az.plot_bpv(idata, kind="t_stat", t_stat="mean", ax=axes[0])
    

for idata in idatas:
    az.plot_bpv(idata, kind="t_stat", t_stat=iqr, ax=axes[1])
```

Notice the argument `kind="t_stat"` to indicate we are going to use a summary statistics and the use of `t_stat="mean"`, i.e. a string to indicate that we want to use the mean as the summary statistic. Or we can use a user defined function, as in `t_stat=iqr` to indicate that we want to use the interquartile range as the summary statistic.

You may have noticed that Figure @fig-lin-pol-bpv also includes some values called `bpv`, for Bayesian p-value. The bpv is a numerical way of summarizing a comparison between simulated data and actual data. To obtain them, a summary statistic $T$ is chosen, such as the mean, median, standard deviation, whatever you may think worth of comparison. Then $T$ is calculated for the observed data $T_{\text{obs}}$ and for the simulated data $T_{\text{sim}}$. Finally we ask ourselves the question:

What is the probability that $T_{\text{sim}}$ is less than or equal to $T_{\text{obs}}$? If the observed values agree with the predicted ones, the expected value will be 0.5. In other words, half of the predictions will be below the observations and half above. This quantity is known as the _Bayesian p-value_.

$$\text{Bayesian p-value} \triangleq p(T_{\text{sim}} \le T_{\text{obs}} \mid \tilde Y)$$

For those who are familiar with _p-values_ and their use in frequentist statistics, there are a couple of clarifications. What is **Bayesian** about these _p-values_ is that we are NOT using a sampling distribution but _the posterior predictive distribution_. And an important difference with the frequentist version is that in this case we are not doing a null hypothesis test, nor trying to declare that a difference is "significant". We are simply trying to quantify how well the model explains the data.

There is yet another way to compute a Bayesian p-value. Instead of using a summary statistic, we can use the entire distribution. In this case, we can ask ourselves the question: What is the probability of predicting a lower or equal value for **each observed value**. If the model is well calibrated these probability should be the same for all observed values, the model is capturing all observations equally well. Thus we should expect a uniform distribution. Again ArviZ can help us with this with the function `az.plot_bpv` with the argument `kind="p_value"` (which is the default). Figure @fig-lin-pol-bpv2 shows the results of this calculation. The white line indicates the expected uniform distribution and the gray band shows the expected deviation given the finite size of the sample. It can be seen that both models are very similar.

![Posterior predictive checks for `model_l` and `model_q` created with `az.plot_bpv` function](/fig/lin-pol-bpv2.png){#fig-lin-pol-bpv2}


Predictive _post_testing_ provides a very flexible framework for evaluating and comparing models, either using graphs or numerical summaries such as _Bayesian p-values_, or even a combination of both. The concept is general enough to allow an analyst to use her imagination to find different ways to explore the predictive distribution _posteriori_ and use the best fit for the purposes of interpreting the data and models.

In the following sections we will explore other methods for comparing models. These methods are not mutually exclusive, they can be used in combination with posterior predictive tests. 

## The balance between simplicity and accuracy

When choosing between alternative explanations, there is a principle known as Occam's razor. In very general lines, this principle establishes that given two or more equivalent explanations for the same phenomenon, the simplest is the preferred explanation. A common criterion of simplicity is the number of parameters in a model.

There are many justifications for this heuristic, we are not going to discuss any of them, we are just going to accept it as a reasonable guide.

Another factor that we generally have to take into account when comparing models is their accuracy, that is, how good a model is at fitting the data. According to this criterion, if we have two (or more) models and one of them explains the data better than the other, then that is the preferred model.

Intuitively, it seems that when comparing models, we tend to prefer those that best fit the data and those that are simpler. But what to do if these two principles conflict? Or more generally, is there a quantitative way to consider both contributions? The short answer is yes, in fact there is more than one way to do it. But first let's see an example in order to generate more intuition.

### Many parameters (may) lead to overfitting

Figure @fig-over_under_fit shows 3 models with increazing number of parameters. The first one (order 0) is just a constant value, whatwer the value of `x`, the model always predicts the same values for `y`, the second model (order 1) is a linear model as we saw in the previous chapter and the last one (order 5) is a polynomial model of order 5, that is something of the form $\alpha + \beta_0 x + \beta_0 x^2 + \beta_0 x^3 + \beta_0 x^4 + \beta_0 x^5$. 


![3 simple models and a simple dataset](/fig/over_under_fit.png){#fig-over_under_fit}

From the Figure @fig-over_under_fit we can see that the increase in the complexity of the model (number of parameters) is accompanied by a greater accuracy reflected in the coefficient of determination R², this is a way to measure the fit of a model (for more information please read <https://en.wikipedia.org/wiki/Coefficient_of_determination>). In fact, we can see that the polynomial of order 5 fits the data perfectly, obtaining R²=1.

Why the polynomial of order 5 can capture the data without losing a single one of them? The reason is that we have the same number of parameters as data, that is, 6. Therefore, the model is simply acting as an alternative way of expressing the data. The model is not learning something about the data, it is memorizing the data! This can be problematic, the easier way to notice this is thinking what will happend to a model that memory data, when preseted with new, unobserved data. The performance is expected to be bad, as someone that just memorizes the questions for an exam only to find the questions are different in the actual exam! This is represented in Figure @fig-over_under_fit2, here we have added two new datapoints, maybe we got the money to perform a new experiment or our boss just send us new data. We can see that the model of order 5, that was able to exacltly fit the data, now has actually a worst performance (as measured by R²) than the linear model. From this simple example, we can see that a model with the best fit is not always the ideal one.


![3 simple models and a simple dataset, plus a couple of new points](/fig/over_under_fit2.png){#fig-over_under_fit2}

Looselly speacking, when a model fits the data set used to learn the parameters of that model very well, but new data sets very poorly, we say we have overfitting. This is a very common problem when analyzing data. A very useful way to think about overfitting is to consider a data set as having two components; the signal and the noise. The signal is what we want to capture (or learn) from the data. If we use a data set it is because we believe there is a signal there, otherwise it will be an exercise in futility. Noise, on the other hand, is not useful and is the product of measurement errors, limitations in the way the data was generated or captured, the presence of corrupted data, etc. A model overfits when it is so flexible (for a data set) that it is capable of _learning_ noise. This has the consequence that the signal is hidden.

This is a practical justification for Occam's razor. And he warns us that at least in principle, it is always possible to create a model so complex that it explains all the details, even the most irrelevant ones. As in the Empire described by Borges, where the cartographers reached such a level of sophistication that they created a map of the Empire whose size was the size of the Empire itself, and which coincided point for point with it.

### Too few parameters lead to underfitting

Continuing with the same example but at the other extreme of complexity, we have the model of order 0. This model is simply a Gaussian disguised as a linear model. This model is only capable of capturing the value of the mean of $Y$, and is therefore totally indifferent to the values of $x$. We say that this model has underfitted the data.

## Measures of predictive accuracy

_Everything should be made as simple as possible, but not simpler_ is a quote often attributed to Einstein. As in a healthy diet, when modeling we have to maintain a balance. Ideally, we would like to have a model that neither underfits nor overfits the data. Somehow you have to balance simplicity and goodness of fit.

In the previous example, it is relatively easy to see that the model of order 0 is _too_ simple while the model of order 5 is _too_ complex. But what can we say about the other two models? How could we establish a numerical ranking of these models? In order to do this we need to formalize our intuition about this balance between simplicity and accuracy.

Let's look at a couple of terms that will be useful to us.

* **Accuracy within the sample** (within-sample accuracy). The accuracy measured with the same data used to fit the model.
* **Accuracy out of sample** (out-of-sample accuracy). The accuracy measured with data not used to fit the model.

The within-sample accuracy will, on average, be less than the out-of-sample accuracy. That is why using the within-sample accuracy to evaluate a model in general will lead us to think that we have a better model than it really is. Using out-of-sample accuracy is therefore a better idea to avoid fooling ourselves. However, this approximation requires leaving data out of the fit, which is a luxury we generally cannot afford. Since this is a central problem in data analysis, there are several proposals to address it. Two very popular approaches are:


* Cross validation: This is an empirical strategy based on dividing the available data into separate subsets that are used to fit and alternatively evaluate

* Information criteria: This is a general term used to refer to various expressions that approximate out-of-sample accuracy as in-sample accuracy plus a term that penalizes model complexity.

### Cross validation

Cross validation is a simple and, in most cases, effective solution for comparing models. We take our data and divide it into K slices. We try to keep the slices more or less the same (in size and sometimes also in other characteristics, such as an equal number of classes). We then use K-1 portions to train the model and the rest to test it. This process is systematically repeated leaving, for each iteration, a different slice out of the training set and using that slice as the evaluation set. This is repeated until we have completed K rounds of adjustment-evaluation. The accuracy of the model will be the average over the K rounds. This is known as K-fold cross validation. Finally, once we have done the cross-validation, we use all the data to last fit our model and this is the model that is used to make predictions or for any other purpose.


![K-fold cross-validation](fig/cv.png){#fig-cv width="80%"}

When K equals the number of data points, we get what is known as _leave-one-out cross-validation_ (LOOCV).

Cross validation is a routine practice in _machine learning_. And we have barely described the most essential aspects of this practice. For more information you can read [The Hundred-Page Machine Learning Book](http://themlbook.com/) or [Python Machine Learning](https://www.amazon.com/Python-Machine-Learning-scikit-learn-TensorFlow-ebook/dp/B0742K7HYF/ref=dp_ob_title_def), by Sebastian Rasch ka, or [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/) by Jake Vanderplas.

Cross validation is a very simple and useful idea, but for some models or for large amounts of data, the computational cost of cross validation may be beyond our means. Many people have tried to find simpler quantities to calculate that approximate the results obtained with cross-validation or work in scenarios where cross-validation may not be as easy to perform. And that is the topic of the next section.

### Information criteria

Information criteria are a collection of closely related tools used to compare models in terms of goodness-of-fit and model complexity. In other words, the information criteria formalize the intuition that we developed at the beginning of the chapter.
The exact way in which these quantities are derived has to do with a field known as [Information Theory](http://www.inference.org.uk/mackay/itila/book.html).

An intuitive way to measure how well a model fits the data is to calculate the root mean square error between the data and the predictions made by the model:

$$\frac{1}{n} \sum_{i=1}^{n} (y_i - \operatorname{E} (y_i \mid \theta))^2$$


$\operatorname{E} (y_i \mid \theta)$ is the predicted value given the estimated parameters. It is important to note that this is essentially the average of the difference between the observed and predicted data.
Taking the square of the errors ensures that the differences do not cancel out and emphasizes large errors compared to other alternatives such as calculating absolute value.

The root mean square error may be familiar to us as it is very popular. But if we stop to reflect on this quantity, we will see that in principle there is nothing special about it and we could well devise other similar expressions. When we adopt a probabilistic approach we see that a more general (and *natural*) expression is the following:

$$ \sum_{i=1}^{n} \log p(y_i \mid \theta)$$

That is, the sum (over $n$ data) of the _likelihoods_ (in logarithmic scale). This is _natural_ because when choosing a likelihood in a model we are implicitly choosing a metric to evaluate the fit of the model. When $p(y_i \mid \theta)$ is a Gaussian then the sum of log-likelihood will be proportional to the root mean square error.

#### Akaike Information Criterion

This is a well-known and widely used information criterion outside the Bayesian universe and is defined as:

$$AIC = -2 \sum_{i=1}^{n} \log p(y_i \mid \hat{\theta}_{mle}) + 2 k $$

Where, k is the number of model parameters and $\hat{\theta}_{mle}$ is the maximum likelihood estimate for $\theta$.


Maximum likelihood estimation is common practice for non-Bayesians and is, in general, equivalent to Bayesian maximum a posteriori (MAP) estimation when *flat* priors are used. It is important to note that $\hat{\theta}_{mle}$ is a point estimate and not a distribution.

The factor $-2$ is just a constant, and we could omit it but usually don't. What is important, from a practical point of view, is that the first term takes into account how well the model fits the data, while the second term penalizes the complexity of the model. Therefore if two models fit the data equally well. AIC says that we should choose the model with the least number of parameters.


AIC works fine in non-Bayesian approaches, but is problematic otherwise. One reason is that it does not use the posterior distribution of $\theta$ and therefore discards information about estimation uncertainty. Also, AIC, from a Bayesian perspective, assumes that priors are *flat* and therefore AIC is incompatible with informative and slightly informative priors like those used in this book. Also, the number of parameters in a model is not a good measure of the model's complexity when using informative priors or structures like hierarchical structures, as these are ways of reducing the _effective number of parameters_, also known as *regularization*. We will return to this idea of regularization later.

#### Widely applicable information criteria

WAIC is something like the Bayesian version of AIC, just like the latter WAIC is made up of two terms, one that measures the adjustment and the other that penalizes. The following expression assumes that the posterior distribution is represented as a sample of size S (as obtained from an MCMC method).

$$WAIC = -2 \sum_i^n \log \left(\frac{1}{S} \sum_{s=1}^S p(y_i \mid \theta^s) \right) + 2 \sum_i^n \left( V_{s=1}^S \log p(y_i \mid \theta^s) \right)$$


The first term is similar to the Akaike criterion, only that it is evaluated for all the observations and all the samples of the later one. The second term is a bit more difficult to justify without getting into technicalities. But it is also a way of penalizing the complexity of the model. What is important from a practical point of view is that WAIC uses the entire posterior (and not a point estimate) for the calculation of both terms, so WAIC can be applied to virtually any Bayesian model.


#### Approximating Cross validation

The key problem with leaving one out cross validation is that it is very expensive as we have to refit the model as many times as datapoints we have. Luckily, it is possible to approximate it using the information from a single fit to the data! The method for doing this is called "importance sampling using Pareto smoothing". The name is so long that in practice we call it LOO. Conceptually what we are trying to calculate is:


$$
\text{ELPD}_\text{LOO-CV} = \sum_{i=1}^{n} \log
     \int \ p(y_i \mid \theta) \; p(\theta \mid y_{-i}) d\theta
$$

where $_{-i}$ means that we leave the observation $i$ out. 

This expresion is very similar to the one for the posterior predictive distribution. The diference is that now we want to compute the posterior predictive distribution for observation $y_i$ from a posterior distribution computed without the observation $y_i$. The first approximation we take is to avoid the explicit computation of the integral by taking samples from the posterior distribution. Thus we can write
$$
\sum_{i}^{n} \log
     \left( \frac{1}{s}\sum_j^s p(y_i \mid \theta_{-i}^j) \right)
$$

where the sum is over $s$ posterior samples. We have been using MCMC samples in this book a lot. So this approximation should not sound unfammiliar to you. The tricky part comes next. It is possible to approximate ${p(y_i \mid \theta_{-i}^j)}$ using importance sampling. This is a form of sampling we have not used or explained yet in this book but it can be very helpfull. We are only going to say that importance sampling is a way of approximating a target distribution by re-weighting values obtained from another distribution. This method is useful when we do not know how to sample from the target distribution but we know how to sample from another distribution. Importance sampling works best when the distributions are somehow similar and the know distribution is _wider_ than the target one.

In our case, the known distribution, once a model has been fitted, is the log-likelihood for all the observations. And we want to approximate the log-likelihood if we had dropped one observation. For this we need to estimate the "importance" (or weight) that each observation has in determining the posterior distribution. A given observation will be more "important" the larger the posterior will change if we remove it. Intuitively, a relatively unlikely observation is more important (or carries more weight) than an expected one. Luckily these weights are easy to compute once we have computed the posterior distribution, in fact the weight of the observation $i$ for the $s$ posterior sample is:

$$
w_s = \frac{1}{p(y_i \mid \theta_s)}
$$

This $w_s$ can be may not be reliable. The main issue is that something a few $w_s$ could be so large that they dominate our calculations making them unstable. To tame this crazy weights we can use Pareto smoothing. This solution consist consists of replacing some of these weights with weights obtained from fitting a Pareto distribution. Why a Pareto distribution? Because the theory indicates that the weights should follow this distribution. So for each observation $y_i$ , the largest weights are used to estimate a Pareto distribution and that distribution is used to replace those weights with "smoothed" weights. This procedure gives robustness to the estimation of the ELPD and also provides a way to diagnostic the approximation, i.e. to get a warning that the LOO method may be failing. For this we need to pay attention to the values of $k$, this is a parameter of the Pareto distribution. Values od $k$ greater than 0.7 indicate that we may have "very influential" observations.


#### Other information criteria

Another widely used information criterion is DIC, if we use the *bayesometer™*, DIC is more Bayesian than AIC but less than WAIC. Although still popular, WAIC and mainly LOO have been shown to be more useful both theoretically and empirically than DIC. Therefore we do NOT recommend its use.

Another widely used criterion is BIC (Bayesian Information Criteria), like logistic regression and my mother's *dry soup*, this name can be misleading. BIC was proposed as a way to correct some of the problems with AIC and the author proposed a Bayesian justification for it. But BIC is not really Bayesian in the sense that like AIC it assumes *flat* priors and uses maximum likelihood estimation.

But more importantly, BIC differs from AIC and WAIC in its objective. AIC and WAIC try to reflect which model generalizes better to other data (predictive accuracy) while BIC tries to identify which is the _correct_ model and therefore is more related to Bayes factors than WAIC. Later we will discuss Bayes Factors and see how it differs from criteria like WAIC and LOO.


## Predictive accuracy calculation using ArviZ

Fortunately, calculating information criteria with ArviZ is very simple. We just need to be sure that the Inference Data has the log-likelihood group, when computing a posterior with PyMC this can be achieved by doing `pm.sample(idata_kwargs={"log_likelihood": True})`. Now let's see how to compute LOO

```python
az.loo(idata_l)
```

```bash
Computed from 8000 posterior samples and 33 observations log-likelihood matrix.

         Estimate       SE
elpd_loo   -14.31     2.67
p_loo        2.40        -
------

Pareto k diagnostic values:
                         Count   Pct.
(-Inf, 0.5]   (good)       33  100.0%
 (0.5, 0.7]   (ok)          0    0.0%
   (0.7, 1]   (bad)         0    0.0%
   (1, Inf)   (very bad)    0    0.0%
```

The output of `az.loo` has two sections on the first section we get a table with two rows, the first one is the ELPD (`elpd_loo`) and the second one is the effective number of parameters (`p_loo`). On the second seccion we have the Pareto k diagnostic. This is a measure of the reliability of the approximation of the LOO. Values of k greater than 0.7 indicate that we possibly have "very influential" observations. In this case we have 33 observations and all of them are good, so we can trust the approximation. 

To compute WAIC you can use `az.waic`, the otuput will be similar, except that we will not get the Pareto k diagnostic, or any similar diagnostics, this is a downside of WAIC, we do not get any information about the reliability of the approximation.

If we compute the LOO for the quadratic model we will get a similar output, but the ELPD will be higher (around -4), indicating that the quadratic model is better at predicting the data.

Values of LOO (or WAIC) are not that usefull by themselves, and must be interpreted relatively. That is why ArviZ offers two helper functions to facilitate this comparison let's look at `az.compare` first.

```python
cmp_df = az.compare({'modelo_l':idata_l, 'modelo_p':idata_p})
```

|          |   rank |   elpd_loo | p_loo |   elpd_diff |      weight |   se |  dse | warning   | scale   |
|:---------|-------:|-----------:|------:|------------:|------------:|-----:|-----:|:----------|:--------|
| modelo_p |      0 |   -4.6     | 2.68  |     0       | 1           | 2.36 | 0    | False     | log     |
| modelo_l |      1 |  -14.3     | 2.42  |     9.73625 | 3.01e-14    | 2.67 | 2.65 | False     | log     |


In the rows we have the compared models and in the columns we have

* rank : the order of the models (from best to worst)
* elpd : the point estimate of the elpd using
* p : the effective parameters
* elpd_diff : the difference between the ELPD of the best model and the other models
* weight : the relative weight of each model. If we wanted to make predictions by combining the different models, instead of choosing just one, this would be the weight that we should assign to each model. In this case we see that the polynomial model takes all the weight.
* se : the standard error of the ELPD
* dse : the standard error of the differences
* warning : a warning about high k values
* scale : the scale on which the ELPD is calculated

The other helper function offered by ArviZ is `az.compareplot`, this function provides similar information to `az.compare`, but graphically. Figure @fig-compareplot shows the output of this function.

![Output of `az.compareplot(cmp_df)`](/fig/compareplot.png){#fig-compareplot width="80%"}


* The empty circles represent the ELPD values and the black lines the standard error.
* The highest value of the ELPD is indicated with a vertical dashed gray line to facilitate comparison with other values.
* For all models except *the best*, we also get a triangle indicating the value of the ELPD difference between each model and the *best* model. The gray error bar indicating the standard error of the differences between the point estimates.

The easiest way to use the information criteria is to choose a single model. Just choose the model with the highest ELPD value. If we follow this rule we will have to accept that the quadratic model is the best. Even if we take into account the standard errors we can see that they do not overlap. This gives us some certainty that indeed the models are *different* from each other. If instead the standard errors did overlap, we should provide a more nuanced answer.

## Model averaging

Model selection is attractive for its simplicity, but we might be missing information about uncertainty in our models. This is somewhat similar to calculating the full posterior and then just keeping the average of the posterior; this can lead us to trust _too much_ what we think we know.

An alternative is to select a single model, but to report and analyze the different models together with the values of the calculated information criteria, their standard error values, and perhaps also the predictive post hoc tests. It is important to put all of these numbers and tests in the context of our problem so that we and our audience can get a better idea of the possible limitations and shortcomings of the models. For those working in academia, these elements can be used to add elements to the discussion section of a paper, presentation, thesis, etc. And in industry this can be useful to inform clients about the advantages and limitations of modeling predictions or conclusions.

Another possibility is to average the models. In this way we are introducing the uncertainty that we have about the goodness of each model, and we can generate a meta-model (and meta-predictions) using a weighted average of each model. ArviZ provides a function for this task, `az.weight_predictions`, which takes as arguments a list of InferenceData objects and a list of weights. The weights can be calculated using the `az.compare` function. For example, if we want to average the two models we have been using, we can do the following:

```python
idata_w = az.weight_predictions(idatas, weights=[0.35, 0.65])
```

Figure @fig-lin-pol-weighted shows the results of this calculation. The light gray dashed line is the weighted average of the two models, the black solid line is the linear model and grey solid line is the quadratic one. 

![Weighted average of the linear and quadratic models](/fig/lin-pol-weighted.png){#fig-lin-pol-weighted width="80%"}

There are other ways to average models, such as explicitly building a metamodel that includes all models of interest as particular cases. For example, a degree 2 polynomial contains a linear model as a particular case, or a hierarchical model is the continuous version between two extremes, a grouped model and an ungrouped model.

## Bayes factors

An alternative to LOO, cross-validation, and information criteria are Bayes factors. It is common for Bayesian factors to appear in the literature as a Bayesian alternative to frequentist hypothesis testing.

The "Bayesian way" of comparing $k$ models is to calculate the _marginal likelihood_ of each model $p(y \mid M_k)$, ie the probability of the observed data $Y$ given the model $M_k$. This quantity, the _marginal likelihood_, is simply the normalization constant of Bayes' theorem. We can see this if we write Bayes' theorem and make explicit the fact that all inferences depend on the model.

$$p (\theta \mid Y, M_k ) = \frac{p(Y \mid \theta, M_k) p(\theta \mid M_k)}{p(Y \mid M_k)}$$

where:

* $y$ are the data
* $\theta$ the parameters
* $M_k$ a model of k competing models

If our main objective is to choose only one model, the _best_, from a set of models we can choose the one with the largest value of $p(y \mid M_k)$. This is fine if we assume that **all models** have the same _a priori_ probability. Otherwise we must calculate:

$$p(M_k \mid y) \propto p(y \mid M_k) p(M_k)$$

If instead, our main objective is to compare models to determine which are more likely and to what extent. This can be achieved using the Bayes factors:

$$FB_{01} = \frac{p(y \mid M_0)}{p(y \mid M_1)}$$

that is, the quotient between the marginal likelihood of two models. The higher the FB, the _better_ the model in the numerator ($M_0$ in this example). To facilitate the interpretation of the FB, Harold Jeffreys proposed a scale for the interpretation of the Bayes Factors with levels of *support* or *strength*. This is just one way to put numbers into words.

* 1-3: anecdotal
* 3-10: moderate
* 10-30: strong
* 30-100: very strong
* $>$100: extreme

Keep in mind that if you get numbers below 1, then the support is for the model in the denominator, tables are also available for those cases. Or we can simply take the inverse of the obtained value.

It is very important to remember that these rules are just conventions, simple guides at best. Results should always be put in the context of our problems and should be accompanied by enough detail so that others can assess for themselves whether they agree with our conclusions. The proof necessary to ensure something in particle physics, or in court, or to decide to carry out an evacuation in the face of a looming natural catastrophe is not the same.


## Some observations
 

We will now briefly discuss some key facts about _marginal likelihood_

* The good
     * **Occam's Razor Included**: Models with more parameters have a higher penalty than models with fewer parameters. The intuitive reason is that the greater the number of parameters, the more the _prior_ _extends_ with respect to the likelihood. An example where it is easy to see this is with nested models, eg a polynomial of order 2 "contains" the models models "polynomial of order 1" and polynym of order 0.

* The bad
     * For many problems the marginal likelihood cannot be calculated analytically. And approximating it numerically is usually a difficult task that in the best of cases requires specialized methods and in the worst the estimates are either impractical or unreliable. In fact, the popularity of the MCMC methods is that they allow obtaining the posterior distribution without the need to calculate this quantity.

* The ugly one
     * The marginal probability depends **very sensitively** on the prior distribution of the parameters in each model $p(\theta_k \mid M_k)$.

It is important to note that *good* and *ugly* are related. Using marginal likelihood to compare models is a good idea because it already includes a penalty for complex models (which helps us prevent overfitting), and at the same time, a change in the prior will affect marginal likelihood calculations. At first this sounds a bit silly; We already know that priors affect calculations (otherwise we could just avoid them), but the point here is the word **sensibly**. We are talking about changes in the prior that would hardly have an effect in the posterior that will have a great impact on the value of the marginal likelihood.
 
The use of FBs is often a watershed among Bayesians. The difficulty of its calculation and the sensitivity to the priors are some of the arguments against it. Another reason is that, like p-values and hypothesis testing in general, BFs favor dichotomous thinking over estimation of "effect size." In other words, instead of asking ourselves questions like: How many more years of life can cancer treatment provide, on average? We end by asking if the difference between treating and not treating a patient is "statistically significant." Note that this last question can be useful in some contexts, the point is that in many other contexts, this type of question is not the question that interests us, only the one that we were taught to answer.

## Calculation of BF

As we already mentioned, marginal likelihood (and the Bayes factors derived from it) is generally not available in closed form, except for some models. For this reason, many numerical methods have been devised for its calculation. Some of these methods are so simple and [naive](https://radfordneal.wordpress.com/2008/08/17/the-harmonic-mean-of-the-likelihood-worst-monte-carlo-method-ever/) that they work very poorly in practice.

### Analytically

For some models, such as the beta-binomial model, we can calculate the marginal likelihood analytically. If we write this model as:

$$\theta \sim Beta(\alpha, \beta)$$
$$y \sim Bin(n=1, p=\theta)$$

the _marginal likelihood_ will be:

$$p(y) = \binom {n}{h} \frac{B(\alpha + h,\ \beta + n - h)} {B(\alpha, \beta)}$$

where:

* $B$ is the [beta function](https://en.wikipedia.org/wiki/Beta_function) not to be confused with the $Beta$ distribution
* $n$ is the number of attempts
* $h$ is the success number

Since we only care about the relative value of the _marginal likelihood_ under two different models (for the same data), we can omit the binomial coefficient $\binom {n}{h}$, so we can write:

$$p(y) \propto \frac{B(\alpha + h,\ \beta + n - h)} {B(\alpha, \beta)}$$

This expression has been coded in the next cell, but with a twist. We will use the `betaln` function, which returns the natural logarithm of the `beta` function, it is common in statistics to do calculations on a logarithmic scale, this reduces numerical problems when working with probabilities.


```python
from scipy.special import betaln

def beta_binom(prior, y):
    """
    Calculate the marginal probability, analytically, for a beta-binomial model.

      prior : tuple
          alpha and beta parameter tuple for the prior (beta distribution)
      and : array
          array with "1" and "0" corresponding to success and failure respectively
    """
    alpha, beta = prior
    h = np.sum(y)
    n = len(y)
    p_y = np.exp(betaln(alpha + h, beta + n - h) - betaln(alpha, beta))

    return p_y
```

Our data for this example consists of 100 "coin tosses" and the same number of "heads and tails" observed. We will compare two models one with a uniform prior and one with a _more concentrated_ prior around $\theta = 0.5$

```python
y = np.repeat([1, 0], [50, 50])  # 50 heads, 50 tails
priors = ((1, 1), (30, 30))  # uniform prior, peaked prior
```

Figure @fig-beta-prior shows the two priors, the uniform prior is the black line and the peaked prior is the gray line.

![Uniform and peaked priors](/fig/beta-prior.png){#fig-beta-prior width="80%"}

Now we can calculate the marginal likelihood for each model and the Bayes factor

```python
BF = beta_binom(priors[1], y) / beta_binom(priors[0], y)
print(round(BF))
```
```bash
5
```

We see that the model with the prior $\text{beta}(30, 30)$, more concentrated, has $\approx 5$ times more support than the model with the $\text{beta}(1, 1)$. This is to be expected since the prior for the first case is concentrated around $\theta = 0.5$ and the data $Y$ have the same number of heads and tails, that is, they agree with a value of $\theta$ around 0.5.

### Sequential Monte Carlo

The Sequential Monte Carlo method is a sampling method that basically progresses through a series of successive sequences from prior to posterior. A byproduct of this process is the estimate of marginal likelihood. Actually, for numerical reasons, the return value is the logarithm of the marginal likelihood.

```python
models = []
idatas = []
for alpha, beta in priors:
    with pm.Model() as model:
        a = pm.Beta("a", alpha, beta)
        yl = pm.Bernoulli("yl", a, observed=y)
        idata = pm.sample_smc(random_seed=42)
        models.append(model)
        idatas.append(idata)

BF_smc = np.exp(
    idatas[1].sample_stats["log_marginal_likelihood"].mean()
    - idatas[0].sample_stats["log_marginal_likelihood"].mean()
)
np.round(BF_smc).item()
```
```bash
5.0
```

As we can see in the cell above, SMC gives essentially the same answer as the analytical calculation!

Note: In the cell above we calculate a difference (instead of a division) because we are on the logarithmic scale, for the same reason we take the exponential before returning the result. Finally, the reason we compute the mean is because we get a marginal log-likelihood value per string.

The advantage of using SMC to calculate marginal likelihood is that we can use it for a wider range of models, since we no longer need to know an expression in closed form. The cost we pay for this flexibility is a higher estimate. Also keep in mind that SMC (with a separate Metropolis kernel implemented in PyMC) is not as efficient as NUTS. As the dimensionality of the problem increases, a more precise estimate of the posterior and the _marginal likelihood_ will require a larger number of samples of the posterior.

## Bayes factors and inference

So far we have used Bayes factors to judge which model seems to be better at explaining the data, and we find that one of the models is $\approx 5$ times _better_ than the other.

But what about the posterior we get from these models? How different are they? Let's see with `az.summary`:


|         |   mean |   sd |   hdi_3% |   hdi_97% |
|:--------|-------:|-----:|---------:|----------:|
| uniform |    0.5 | 0.05 |     0.4  |      0.59 |
| peaked  |    0.5 | 0.04 |     0.42 |      0.57 |'

We can argue that the results are quite similar, we have the same mean value for $\theta$ and a slightly wider trailing for `model_0`, as expected since this model has a wider prior. We can also check the posterior predictive distribution to see how similar they are (See Figure @fig-beta-ppc).


![posterior predictive distributions for the models with uniform and peaked priors](/fig/beta-ppc.png){#fig-beta-ppc width="80%"}

In this example, the observed data is more consistent with `model_1`, because the prior is concentrated around the correct value of $\theta$, while `model_0`, assigns the same probability to all possible values of $\theta$. This difference between the models is captured by the Bayes factor. We could say that the Bayes factors measure which model, as a whole, is better to explain the data. And this includes the details of the prior, no matter how similar the model predictions are. In many scenarios what interests us when comparing models is how similar the predictions are. What is estimated by LOO or cross validation.

## Savage–Dickey ratio

For the above examples we have compared two beta-binomial models, we could have compared two completely different models. But there are times when we want to compare a null hypothesis H_0 (or null model) against an alternative H_1. For example, to answer the question _Is this coin biased?_, we could compare the value $\theta = 0.5$ (representing no-bias) with the output of a model in which we allow $\theta$ to vary. For this type of comparison, the null model is nested within the alternative, which means that the null is a particular value of the model we are building. In those cases, calculating the Bayes factor is very easy and does not require any special method. We only need to compare the before and after evaluated to the null value (for example $\theta = 0.5$ ), under the alternative model. We can see that this is true from the following expression:


$$
BF_{01} = \frac{p(y \mid H_0)}{p(y \mid H_1)} \frac{p(\theta=0.5 \mid y, H_1)}{p(\theta=0.5 \mid H_1)}
$$

Which is true [only](https://statproofbook.github.io/P/bf-sddr) when H_0 is a particular case of H_1.

Let's do it with PyMC and ArviZ. We only need to sample the prior and posterior for a model. Let's try the beta-binomial model with uniform prior.


```python
with pm.Model() as model_uni:
    a = pm.Beta("a", 1, 1)
    yl = pm.Bernoulli("yl", a, observed=y)
    idata_uni = pm.sample(2000, random_seed=42)
    idata_uni.extend(pm.sample_prior_predictive(8000))

az.plot_bf(idata_uni, var_name="a", ref_val=0.5)
```

The result is show in Figure @fig-beta-bf, we can see one KDE for the prior (black) and one for the posterior (gray). The two black dots show that we evaluated both distributions at the value 0.5. We can see that the Bayes factor in favor of the null hypothesis, BF_01, is $\approx 7$, which we can interpret as _moderate evidence_ in favor of the null hypothesis (see the Jeffreys scale discussed earlier).

![Bayes factor for the beta-binomial model with uniform prior](/fig/beta-bf.png){#fig-beta-bf width="80%"}

As we already discussed, the Bayes factors measure which model, as a whole, is better at explaining the data. And this includes the prior, even if the prior has a relatively low impact on the computation of the posterior. We can also see this prior effect by comparing a second model to the null model.

If instead our model were a beta-binomial with beta prior (30, 30), the BF_01 would be lower (_anecdotal_ on the Jeffrey scale). This is because, according to this model, the value of $\theta=0.5$ is much more likely priori than for a uniform prior, and therefore the prior and posterior will be much more similar. That is, there is not much _surprise_ to see that the posterior is concentrated around 0.5 after collecting data.

Let's calculate it to see for ourselves.

```python
with pm.Model() as model_conc:
    a = pm.Beta("a", 30, 30)
    yl = pm.Bernoulli("yl", a, observed=y)
    idata_conc = pm.sample(2000, random_seed=42)
    idata_conc.extend(pm.sample_prior_predictive(8000))

az.plot_bf(idata_conc, var_name="a", ref_val=0.5)
```

Figure @fig-beta-bf2 shows the result, we can see that the BF_01 is $\approx 1.5$ which we can interpret as _anecdotal evidence_ in favor of the null hypothesis (see the Jeffreys scale discussed earlier). This is 5 times smaller than with the uniform prior. This is because, according to this model, the value of $\theta=0.5$ is much more likely priori than for a uniform prior, and therefore the prior and posterior will be much more similar. That is, there is not much _surprise_ to see that the posterior is concentrated around 0.5 after collecting data.


![Bayes factor for the beta-binomial model with peaked prior](/fig/beta-bf2.png){#fig-beta-bf2 width="80%"}


<!-- The graph shows one KDE for the prior (black) and one for the posterior (gray). The two black dots show that we evaluated both distributions at the value 0.5. We can see that the Bayes factor in favor of the null hypothesis, BF_01, is $\approx 1.5$, which we can interpret as _anecdotal evidence_ in favor of the null hypothesis (see the Jeffreys scale discussed earlier). -->


## Regularizing priors


Using informative and weakly informative priors is a way of introducing bias in a model and, if done properly, this can be a really good because bias prevents overfitting and thus contributes to models being able to make predictions that generalize well. This idea of adding a bias to reduce a generalization error without affecting the ability of the model to adequately model the data that's used to fit is known as **regularization**. This regularization often takes the form of a term penalizing certain values for the parameters in a model, like too big coefficients in a regression model. Restricting parameter values is a way of reducing the data a model is able to represent and thus reducing the chances that a model captures the noise instead of the signal.

This regularization idea is so powerful and useful that it has been discovered several times, including outside the Bayesian framework. For regression models, and outside Bayesian statistics, two popular regularization method are ridge regression and Lasso regression. From the Bayesian point of view, a ridge regression can be interpreted as using normal distributions for the beta coefficients of a linear model, with a small standard deviation that pushes the coefficients toward zero. In this sense, we have been doing something very close to ridge regression for every single linear model in this book (except the examples in
this chapter that uses SciPy!). On the other hand, Lasso regression can be interpreted from a Bayesian point of view as the MAP of the posterior computed from a model with Laplace priors for the beta coefficients. The Laplace distribution looks similar to the Gaussian distribution, but with a sharp peak at zero, you can also interpret it as two _back-to-back_ exponential distributions (try `pz.Laplace(0, 1).plot_pdf()`). The Laplace distribution concentrates its probability mass much closer to zero compared to the Gaussian distribution. The idea of using such a prior is to provide both regularization and variable selection. The idea is that since we have this peak at zero, we expect the prior to induce sparsity, that is, we create a model with a lot of parameters and the prior will automatically makes most of them zero, keeping only the relevant variables contributing to the output of the model. Unfortunately, and contrary to ridge regression this idea does not directly translate from the frequestist realm to the Bayesian one. Nevertheless, there are Bayesian priors that can be used for inducing sparsity and performing variable selection, like the horseshoe prior. If you want to learn more about the horseshoe and other shrinkage priors you may find this article interesting [@Piironen_2017]. In the next chapter we will discuss more about variable selection. Just one final note, it is important to notice that the classical versions of ridge and Lasso regressions correspond to single point estimates, while the Bayesian versions gave full posterior distributions.


## Summary

In this chapter we have seen how to compare models using posterior predictive checks, information criteria, approximated cross-validation and Bayes factors. 

Posterior predictive checks is a general concept and practice that can help us understand how well models are capturing different aspects of the data. We can perform posterior predictive checks with just one model or with many models, and thus we can use it as a method for model comparison.
Posterior predictive checks are generally done via visualizations, but numerical summaries like Bayesian -values can also be helpful.

Good models have a good balance between complexity and predictive accuracy. We exemplified this feature by using the classical example of polynomial regression. We discussed two methods to estimate the out-of-sample accuracy without leaving data aside: cross-validation and information criteria. From a practical point of view, information criteria is a family of theoretical methods looking to balance two contributions: a measurement of how well a model fits the data and a penalization term for complex models. We briefly discussed AIC, for his historical importance and then WAIC which is a better method for Bayesian models, as it takes into accout the entire posterior distribution and uses a more sophisticated method to compute the effective number of parameters.

We also discussed cross-validation. And we saw we can approcimate leave-one-out-cross-validation using LOO. Both WAIC and LOO tend to produce very similar results, but LOO can be more reliable. So we recomend its use. Both WAIC and LOO can be used for model selection and for model averaging. Instead of selecting a single best model, model averaging is about combining all available models by taking a weighted average of them.

A different approach to model selection, comparison, and model averaging is Bayes factors, which are the ratio of the marginal likelihoods of two models. Bayes factor computations can be really challenging. In this chapter, we showed two routes to compute them with PyMC and ArviZ: Using the sampling method known as Sequential Monte Carlo and using the Savage–Dickey ratio. The first method can be used for any model as long as Sequential Monte Carlo provides a good posterior, with the current implmenetation of SMC in PyMC, this can be challenging for high-dimensional models or hierarchical models. The second method can only be used when the null model is a particular case of the alternative model. Besides being computationally challenging, Bayes factors are problematic to use given that they are very (overly)sensitive to prior specification. 

We show that Bayes factors and LOO/WAIC are the answers to two related but different questions. The former one is focused on identifying the right model and the
other on identifying the model with lower generalization loss, i.e. the model making the best predictions. None of these methods are free of problems, but WAIC and in particular LOO are much more robust in practice.


## Exercises


