# Programming Probabilistically {#sec-programming_probabilistically}

> "Our golems rarely have a physical form, but they too are often made of clay living in silicon as computer code." - Richard McElreath


Now that we have a very basic understanding of probability theory and Bayesian statistics, we are going to learn how to build probabilistic models using computational tools. Specifically, we are going to learn about probabilistic programming with PyMC. The basic idea is that we use code to specify statistical models and then PyMC will solve those models for us. We will not need to write Bayes' theorem in explicit form. This is a good strategy for two reasons. First, many models do not lead to an analytic closed form, and thus we can only solve those models using numerical techniques.  Second, modern Bayesian statistics is mainly done by writing code. We will be able to see that probabilistic programming offers an effective way to build and solve complex models and allows us to focus more on model design, evaluation, and interpretation, and less on mathematical or computational details. In this chapter, we will cover the following topics:

* Probabilistic programming
* A PyMC primer
* The coin-flipping problem revisited
* Summarizing the posterior
* The Gaussian and Student's t models
* Comparing groups and the effect size

## Probabilistic programming

Bayesian statistics is conceptually very simple; we have the *knows* and the *unknowns*; we use Bayes' theorem to condition the latter on the former. If we are lucky, this process will reduce the uncertainty about the *unknowns*. Generally, we refer to the *knowns* as **data** and treat it like a constant, and the *unknowns* as **parameters** and treat them as *random variables*. Although conceptually simple, fully probabilistic models often lead to analytically intractable expressions. For many years, this was a real problem and one of the main issues that hindered the adoption of Bayesian methods beyond some niche applications. The arrival of the computational era and the development of numerical methods that, at least in principle, can be used to solve any inference problem, have dramatically transformed the Bayesian data analysis practice. We can think of these numerical methods as *universal inference engines*. The possibility of automating the inference process has led to the development of probabilistic programming languages (PPLs), which allows for a clear separation between model creation and inference. In the PPL framework, users specify a full probabilistic model by writing a few lines of code, and then inference follows automatically. It is expected that probabilistic programming will have a major impact on data science and other disciplines by enabling practitioners to build complex probabilistic models in a less time-consuming and less error-prone way. I think one good analogy for the impact that programming languages can have on scientific computing is the introduction of the Fortran programming language more than six decades ago. While nowadays Fortran has lost its shine [^fortran], at one time, it was considered to be very revolutionary. For the first time, scientists moved away from computational details and began focusing on building numerical methods, models, and simulations more naturally.

[^fortran]: Some folks are working on making Fortran cool again <https://fortran-lang.org/en/>

## Flipping coins the PyMC way

Let's revisit the coin-flipping problem from @sec-thinking_probabilistically, Thinking Probabilistically, but this time using PyMC. We will use the same synthetic data we used in that chapter. Since we are generating the data, we know the true value of $\theta$, called `theta_real`, in the following block of code. Of course, for a real dataset, we will not have this knowledge:

```python
np.random.seed(123)
trials = 4
theta_real = 0.35 # unknown value in a real experiment
data = pz.Binomial(n=1, p=theta_real).rvs(trials)
```


Now that we have the data, we need to specify the model. Remember that this is done by specifying the likelihood and the prior. For the likelihood, we will use the binomial distribution with parameters $n=1$, $p=\theta$ and for the prior, a Beta distribution with the parameters $\alpha=\beta=1$. A Beta distribution with such parameters is equivalent to a Uniform distribution on the interval [0, 1]. Using mathematical notation we can write the model as:

$$
\begin{aligned}
\theta &\sim \text{Beta}(\alpha=1, \beta=1) \\
Y &\sim \text{Binomial}(n=1, p=\theta)
\end{aligned}
$$

This statistical model has an almost one-to-one translation to PyMC:

```python
with pm.Model() as our_first_model:
    θ = pm.Beta('θ', alpha=1., beta=1.)
    y = pm.Bernoulli('y', p=θ, observed=data)
    idata = pm.sample(1000)
```

The first line of the code creates a container for our model. Everything inside the with-block will be automatically added to our_first_model. You can think of this as syntactic sugar to ease model specification as we do not need to manually assign variables to the model. The second line specifies the prior. As you can see, the syntax follows the mathematical notation closely. The third line specifies the likelihood, the syntax is almost the same as for the prior, except that we pass the data using the `observed` argument. The observed values can be passed as a Python list, a tuple, a NumPy array, or a pandas DataFrame. Now, we are finished with the model specification! Pretty neat, right?

We still have one more line of code to explain. The last line is where the magic happens. Behind this innocent line, PyMC has hundreds of *oompa loompas* singing and baking a delicious Bayesian inference just for you! Well, not exactly, but PyMC is automating a lot of tasks. For the time being, we are going to treat that line as a black box that will give us the correct result. What is important to understand is that under the hood we will be using numerical methods to compute the posterior distribution. In principle, these numerical methods are capable of solving any model we can write. The cost we pay for this generality is that the solution is going to take the form of samples from the posterior. Later we will be able to corroborate that these samples come from a Beta distribution, as we learn from the previous chapter. Because the numerical methods are stochastic, the samples will vary every time we run them. However, if the inference process works as expected, the samples will be representative of the posterior distribution and thus we will obtain the same conclusion from any of those samples. The details of what happens under the hood and how to check if the samples are indeed trustworthy will be explained in @sec-inference_engines.

One more thing, the `idata` variable is an `InferenceData` object, which is a container for all the data generated by PyMC. We will learn more about this later in this same chapter. 

OK, so on the last line, we are asking for 1,000 samples from the posterior. If you run the code, you will get a message like this:

```text
Auto-assigning NUTS sampler...
Initializing NUTS using jitter+adapt_diag...
Multiprocess sampling (4 chains in 4 jobs)
NUTS: [θ]
Sampling 4 chains for 1_000 tune and 1_000 draw iterations
(4_000 + 4_000 draws total) took 1 second.
```

The first and second lines tell us that PyMC has automatically assigned the NUTS sampler (one inference engine that works very well for continuous variables), and has used a method to initialize that sampler (these methods need some initial guess of where to start sampling). The third line says that PyMC will run four chains in parallel, thus we will get four independent samples from the posterior. As PyMC attempt to parallelize these chains across the available processors in your machine, we will get the four for the price of one. The exact number of chains is computed taking into account the number of processors in your machine, you can change it using the `chains` argument for the `sample` function. The next line is telling us which variables are being sampled by which sampler. For this particular case, this line is not adding new information, because NUTS is used to sample the only variable we have $\theta$. However, this is not always the case as PyMC can assign different samplers to different variables. PyMC has rules to ensure that each variable is associated with the best possible sampler. Users can manually assign samplers using the `step` argument of the `sample` function, you will hardly need to do that.

Finally, the last line is a progress bar, with several related metrics indicating how fast the sampler is working, including the number of iterations per second. If you run the code, you will see the progress bar get updated really fast. Here, we are seeing the last stage when the sampler has finished its work. You will notice that we have asked for 1000 samples, but PyMC is computing 8000 samples. We have 1000 draws per chain to tune the sampling algorithm (NUTS, in this example). These draws will be discarded by default, PyMC uses them to increase the efficiency and reliability of the sampling method, which are both important to obtain a useful approximation to the posterior. We also have 1000 productive draws per chain for a total of 4000, these are the ones we are going to use as our posterior. We can change the number of tuning steps with the `tune` argument of the sample function and the number of draws with the `draw` argument.

## Summarizing the posterior

Generally, the first task we will perform after sampling from the posterior is to check what the results look like. The `plot_trace` function from ArviZ is ideally suited to this task:


```python
az.plot_trace(idata)
```

![A trace plot for the posterior of `our_first_model`](/fig/idata_trace.png){#fig-idata_trace}


@fig-idata_trace shows the default result when calling `az.plot_trace`, we get two subplots for each unobserved variable. The only unobserved variable in our model is $\theta$. Notice that $y$ is an observed variable representing the data; we do not need to sample that because we already know those values. Thus we only get two subplots. On the left, we have a Kernel Density Estimation (KDE) plot; this is like the smooth version of the histogram. Ideally, we want all chains to have a very similar KDE, like in @fig-idata_trace. On the right, we get the individual values at each sampling step, we got as many lines as chains. Ideally, we want to be something that looks noisy, with no clear pattern, and we should have a hard time identifying one chain from the others. In @sec-inference_engines we give more details on how to interpret these plots. The gist is that if we ran many samples, like four, there are practically indistinguishable from each other. The sampler did a good job and we can trust the samples.

As with other ArviZ functions `az.plot_trace` has many options. For instance, we can run the function with the `combined` argument set to `True` to get a single KDE plot for all chains and with `kind=rank_bars` to get a **rank plot**


```python
az.plot_trace(idata, kind="rank_bars", combined=True, rank_kwargs={"colors": "k"});
```

![A trace plot for the posterior of `our_first_model`, using the options `kind="rank_bars"`, `combined=True`](/fig/idata_rank.png){#fig-idata_rank}


A rank plot is another way to check if we can trust the samples, for this plot, we get one histogram per chain and we want all of them to be as uniform as possible, like in  @fig-idata_rank. Some small deviations for uniformity are expected due to random sampling, but large deviations from uniformity are a signal that chains are exploring different regions of the posteriors. Ideally, we want all chains to explore the entire posterior. In @sec-inference_engines, we provide further details on how to interpret rank plots and how they are constructed.

ArviZ provides several other plots to help interpret the posterior, and we will see them in the following pages. We may also want to have a numerical summary of the posterior. We can get that using `az.summary`, which will return a pandas DataFrame:

```python
az.summary(idata, kind="stats").round(2)
```

|          |   mean |   sd |  hdi_3% |  hdi_97% |
|:--------:|-------:|-----:|-------- |----------|
| $\theta$ |  0.34  | 0.18 |  0.03   |  0.66    |


The first column is the name of the variable, the second column is the mean of the posterior, the third column is the standard deviation of the posterior, and the last two columns are the lower and upper boundaries of the 94% highest density interval. Thus, according to our model and data, we think the value of $\theta$ is likely to be 0.34 with a 94% probability that it is actually between 0.03 and 0.66. We can report a similar summary using the standard deviation. The advantage of the standard deviation over the HDI is that it is a more popular statistic. As a disadvantage, we have to be more careful interpreting it, otherwise, it can lead to meaningless results. For example, if we compute the mean $\pm$ 2 standard deviations we will get the intervals (-0.02, 0.7) the upper value is not that far from 0.66, which we got from the HDI, but the lower bound is actually outside the possible values of $\theta$, which is between 0 and 1.

Another way to visually summarize the posterior is to use the `az.plot_posterior` function that comes with ArviZ. We have already used this distribution in the previous chapter for a fake posterior. We are going to use it now for a real posterior. By default, plot_posterior shows a histogram for discrete variables and KDEs for continuous variables. We also get the mean of the distribution (we can ask for the median or mode using the `point_estimate` argument) and the 94% HDI as a black line at the bottom of the plot. Different interval values can be set for the HDI with the credible_interval argument. This type of plot was introduced by John K. Kruschke in his great book Doing Bayesian Data Analysis [@kruschke_2014]:

```python
az.plot_posterior(idata)
```

![The plot shows the posterior distribution of $\theta$ and the 94% HDI.](/fig/idata_posterior.png){#fig-idata_posterior}


## Posterior-based decisions

Sometimes, describing the posterior is not enough. Sometimes, we need to make decisions based on our inferences. We have to reduce a continuous estimation to a dichotomous one: yes-no, health-sick, contaminated-safe, and so on. We may need to decide if the coin is fair or not. A fair coin is one with a $\theta$ value of exactly 0.5. We can compare the value of 0.5 against the HDI interval. From @fig-idata_posterior, we can see that the HDI goes from 0.03 to 0.7 and hence 0.5 is included in the HDI. We can interpret this as an indication that the coin may be tail-biased, but we cannot completely rule out the possibility that the coin is actually fair. If we want a sharper decision, we will need to collect more data to reduce the spread of the posterior or maybe we need to find out how to define a more informative prior.


### Savage-Dickey density ratio

One way to evaluate how much support the posterior provides for a given value is to compare the ratio of the posterior and prior densities at that value. This is called the Savage-Dickey density ratio and we can compute it with ArviZ using the `az.plot_bf` function

```python
az.plot_bf(idata, var_name="θ",
           prior=np.random.uniform(0, 1, 10000), ref_val=0.5);
```

![The plot shows the prior and posterior for `our_first_model` the black dots represent their values evaluated at the reference value 0.5](/fig/idata_bf.png){#fig-idata_bf}


From @fig-idata_bf we can see that the value of BF_01 is 1.3, which means that the value of $\theta=0.5$ is 1.3 times more likely under the posterior distribution than under the prior distribution. To compute this value we just divided the height of the posterior at $\theta=0.5$ by the height of the prior at $\theta=0.5$. The value of BF_10 is just the inverse $\frac{1}{1.3} \approx 0.8$, we can think of this as the value of $\theta \neq 0.5$ being 0.77 times more likely under the posterior than under the prior. How do we interpret these numbers? With a pinch of salt, the following table shows one possible interpretation originally proposed by @kass_1995:

| BF_01        | Interpretation |
|--------------|----------------|
| 1 to 3.2     | Not worth more than a bare mention|
| 3.2 to 10    | Substantial    |
| 10 to 100    | Strong         |
| > 100 	   | Decisive       |

The Savage-Dickey density ratio is a particular way to compute what is called Bayes Factor. We will learn more about Bayes Factors, and their caveats, in @sec-model_comparison.

### Region of Practical Equivalence

Strictly speaking, the chance of observing exactly 0.5 (that is, with infinite trailing zeros) is zero. Also, in practice, we generally do not care about exact results but results within a certain margin. Accordingly, in practice, we can relax the definition of fairness and we can say that a fair coin is one with a value of *around* 0.5. For example, we could say that any value in the interval [0.45, 0.55] will be, for our purposes, practically equivalent to 0.5. We call this interval a Region Of Practical Equivalence (ROPE). Once the ROPE is defined, we compare it against the HDI. We can get at least three scenarios:

* The ROPE does not overlap with the HDI; we can say the coin is not fair
* The ROPE contains the entire HDI; we can say the coin is fair
* The ROPE partially overlaps with HDI; we cannot say the coin is fair or unfair

If we choose the ROPE to match the support of a parameter, like [0, 1] for the coin-flipping example, we will always say we have a fair coin. Notice that we do not need to collect data to perform any type of inference.

The choice of ROPE is completely arbitrary we can choose any value we want. Some choices are not very useful. If for the coin-flipping example, we choose the ROPE to be [0, 1], then we will always say the coin is fair. Even more, we don't need to collect data or perform any analysis to reach this conclusion, this is a trivial example. More worrisome is to pick the ROPE after performing the analysis. This is problematic because we can accommodate the results to say whatever we want them to say, again why do we even bother to do an analysis, if we are going to accommodate the result to our expectations? The ROPE should be informed from domain knowledge.

We can use the plot_posterior function to plot the posterior with the HDI interval and the ROPE. The ROPE appears as a semi-transparent thick (gray) line:

```python
az.plot_posterior(idata, rope=[0.45, .55])
```

![The plot shows the posterior distribution of $\theta$ and the 94% HDI. The ROPE is shown as a thick light-gray line. ](/fig/idata_posterior_rope.png){#fig-idata_posterior_rope}

Another tool we can use to help us make a decision is to compare the posterior against a reference value. We can do this using plot_posterior. As you can see, we get a vertical (gray) line and the proportion of the posterior above and below our reference value:

```python
az.plot_posterior(idata, ref_val=0.5)
```

![The plot shows the posterior distribution of $\theta$ and the 94% HDI. The reference value is shown as a gray vertical line.](/fig/idata_posterior_ref.png){#fig-idata_posterior_ref}

For a more detailed discussion on the use of the ROPE you could read Chapter 12 Doing Bayesian Data Analysis by @kruschke_2014. That chapter also discusses how to perform hypothesis testing in a Bayesian framework and the caveats of hypothesis testing, whether in a Bayesian or non-Bayesian setting.

### Loss functions

If you think these ROPE rules sound a little bit clunky and you want something more formal, loss functions are what you are looking for! To make a good decision, it is important to have the highest possible level of precision for the estimated value of the relevant parameters, but it is also important to take into account the cost of making a mistake. The cost/benefit trade-off can be mathematically formalized using loss functions. The names for loss functions or their inverses vary across different fields, and we could find names such as cost functions, objective functions, fitness functions, utility functions, and so on. No matter the name, the key idea is to use a function that captures how different the true value and the estimated value of a parameter are. The larger the value of the loss function, the worse the estimation is (according to the loss function). Some common examples of loss functions are:


* The quadratic loss function, $(\theta - \hat \theta)^2$
* The absolute loss function, $|\theta - \hat \theta|$
* The 0-1 loss function, $\mathbb{1}(\theta \neq \hat \theta)$, where $\mathbb{1}$ is the indicator function

In practice, we don't know the value of the true parameter. Instead, we have an estimation in the form of a posterior distribution. Thus, what we can do is find out the value of $\theta$ that minimizes the expected loss function. By expected loss function, we mean the loss function averaged over the whole posterior distribution. In the following block of code, we have two loss functions: the absolute loss (`lossf_a`) and the quadratic loss (`lossf_b`). We will explore the value of over a grid of 200 points. We will then plot those curves and we will also include the value of $\theta$ that minimizes each loss function. The following block shows the Python code without the plotting part:

```python
grid = np.linspace(0, 1, 200)
θ_pos = idata.posterior['θ']
lossf_a = [np.mean(abs(i - θ_pos)) for i in grid]
lossf_b = [np.mean((i - θ_pos)**2) for i in grid]
for lossf, c in zip([lossf_a, lossf_b], ['C0', 'C1']):
    ...
```

![The absolute (black) and quadratic (gray) loss functions applied to the posterior from `our_first_model`](/fig/idata_loss.png){#fig-idata_loss}


As we can see, the result looks somewhat similar for the absolute (black) and quadratic (gray) loss functions. What is interesting from this result is that the first value is equal to the median of the posterior and the last value is equal to the mean of the posterior. You can check this for yourself by computing `np.mean(θ_pos)`, `np.median(θ_pos)`. This is no coincidence, different loss functions are related to different point estimates. The mean is the point estimate that minimizes the quadratic loss, the median, the absolute loss, and the mode, the 1-0 loss.

If we want to be formal and we want to compute a single-point estimate, we must decide which loss function we want. Conversely, if we choose one point estimate, we are implicitly (and maybe unconsciously) assuming a loss function.

The advantage of explicitly choosing a loss function is that we can tailor the function to our problem, instead of using a predefined rule. It is very common to observe that the cost of making a decision is asymmetric; for example, vaccines can produce an overreaction of the immune system, but the benefit to the vaccinated persons and even non-vaccinated persons overcomes the risk, usually, by many orders of magnitude. Thus, if our problem demands it, we can construct an asymmetric loss function.

It is also important to notice that, as the posterior is in the form of numerical samples, we can compute complex loss functions that don't need to be restricted by mathematical convenience or mere simplicity. The following code, and @fig-idata_loss2 generated from it, is just a silly example of this:

```python
lossf = []
for i in grid:
    if i < 0.5:
        f = 1/np.median(θ_pos / np.abs(i**2 - θ_pos))
    else:
        f = np.mean((i - θ_pos)**2 + np.exp(-i)) - 0.25
    lossf.append(f)
```

![A weird loss function applied to the posterior from `our_first_model`](/fig/idata_loss2.png){#fig-idata_loss2}


## Gaussians all the way down

 We introduced the main Bayesian notions using the beta-binomial model mainly because of its simplicity. Another very simple model is the Gaussian or Normal model. Gaussians are very appealing from a mathematical point of view, working with them is relatively easy and many operations applied to Guassians return another Gaussian. Additionally, many natural phenomena can be nicely approximated using Gaussians; essentially, almost every time that we measure the average of something, using a *big enough* sample size, that average will be distributed as a Gaussian. The details of when this is true, when this is not true, and when this is more or less true, are elaborated in the **central limit theorem** (CLT); you may want to stop reading now and search about this really *central* statistical concept (very bad pun intended).

 Well, we were saying that many phenomena are indeed averages. Just to follow a cliché, the height (and almost any other trait of a person, for that matter) is the result of many environmental factors and many genetic factors, and hence we get a nice Gaussian distribution for the height of adult people. Well, indeed we get a mixture of two Gaussians, which is the result of overlapping the distribution of heights of women and men, but you get the idea. In summary, Gaussians are easy to work with and they are abundant in natural phenomena, and hence many of the statistical methods you may already know, assume normality. Thus, it is important to learn how to build these models, and then it is also equally important to learn how to relax the normality assumptions, something surprisingly easy in a Bayesian framework and with modern computational tools such as PyMC.

### Gaussian inferences

 Nuclear magnetic resonance (NMR) is a powerful technique used to study molecules and also living things such as humans, sunflowers and yeast (because, after all, *we are just a bunch of molecules*). NMR allows you to measure different kinds of observable quantities that are related to unobservable interesting molecular properties [@arroyuelo_2021]. One of these observables is known as **chemical shift**; we can only get chemical shifts for the nuclei of certain types of atoms. The details belong to quantum chemistry and are irrelevant to this discussion. 
 
 For all we care at the moment, we could have been measuring the height of a group of people, the average time to travel back home, or the weight of bags of oranges. For all these examples the variables are continuous and it makes sense to think of them as an average value plus a dispersion. Sometimes we can use a Gaussian model for discrete variables if the number of possible values is large enough, for example, Bonobos are very promiscuous, so maybe we can model the number of sexual partners of our cousins with a Gaussian.
 
 Going back to our example, we have 48 chemical shift values represented in a boxplot in @fig-boxplot, we can see that the median (the line inside the box) is around 53 and the interquartile range (the box) is around 52 and 55. We can see that there are two values far away from the rest of the data (empty circles).

 ![Boxplot of the 48 chemical shift values. We observed two values above 60, far away from the rest of the data](/fig/boxplot.png){#fig-boxplot}


Let's forget about those two points for a moment and assume that a Gaussian distribution is a good description of the data. Since we do not know the mean or the standard deviation, we must set priors for both of them. Therefore, a reasonable model could be:

$$
\begin{aligned}
\mu &\sim \mathcal{U}(l, h) \\
\sigma &\sim \mathcal{HN}(\sigma_{\sigma}) \\
Y &\sim \mathcal{N}(\mu, \sigma)
\end{aligned}
$$


where $\mathcal{U}(l, h)$ is the Uniform distribution between $l$ and $h$, $\mathcal{HN}(\sigma_{\sigma})$ is the HalfNormal distribution with standard deviation $\sigma_{\sigma}$, and $\mathcal{N}(\mu, \sigma)$ is the Gaussian distribution with mean $\mu$ and standard deviation $\sigma$. A HalfNormal distribution is similar to a Normal distribution but restricted to positive values (including zero). @fig-model_g_dag shows the graphical representation of this model.

![Graphical representation of `model_g`](/fig/model_g_dag.pdf){#fig-model_g_dag width="50%"}

If we do not know the possible values of $\mu$ and $\sigma$, we can set priors reflecting our ignorance. One option is to set the boundaries of the Uniform distribution to be $l=40$, $h=75$, which is a range larger than the range of the data. Alternatively, we can choose a range based on our previous knowledge. For instance, we may know that this is not physically possible to have values below 0 or above 100 for this type of measurement. And thus use those values as the boundaries of the Uniform distribution. For the HalfNormal, and in the absence of more information, we can choose a large value compared to the scale of the data. Using PyMC, we can write the model as follows:

The code for this model is:

```python
with pm.Model() as model_g:
    μ = pm.Uniform('μ', lower=40, upper=70)
    σ = pm.HalfNormal('σ', sigma=5)
    Y = pm.Normal('Y', mu=μ, sigma=σ, observed=data)
    idata_g = pm.sample()
```

![Posterior from `model_g` ploted using `az.plot_trace(idata_g)`](/fig/idata_g.png){#fig-idata_g_trace}

Let's see what the posterior looks like, @fig-idata_g_trace was generated with the ArviZ function `plot_trace`, it has one row for each parameter. For this model, the posterior is bidimensional, so each row is showing one marginal distribution. We can use the `plot_pair` function from ArviZ to see what the bi-dimensional posterior looks like, together with the marginal distributions for $\mu$ and $\sigma$, see @fig-idata_g_pair.


![Posterior from `model_g` ploted using `az.plot_pair(idata_g, kind='kde', marginals=True)`](/fig/idata_g_pair.png){#fig-idata_g_pair}


We are going to print the summary for later use, we use the code:

```python
az.summary(idata_g, kind="stats").round(2)
```

And we get the following table as output:

|          |   mean |   sd |   hdi_3% |   hdi_97% |
|:--------:|-------:|-----:|---------:|----------:|
| $\mu$	   |  53.50 | 0.52 |  52.51   | 54.44     |
| $\sigma$ |   3.52 | 0.38 |   2.86   |  4.25     |


## Posterior predictive checks

One of the nice elements of the Bayesian toolkit is that once we have a posterior $p(\theta \mid Y)$, it is
possible to use it to generate predictions $p(\tilde Y)$. Mathematically this can be done by computing:

$$
p(\tilde Y \mid Y) = \int p(\tilde Y \mid \theta) \; p(\theta \mid Y) d\theta
$$

This distribution is known as the **posterior predictive distribution**. *Predictive* because it is used to make predictions, and *posterior* because it is computed using the posterior distribution. So we can think of this as the distribution of future data given the model, and observed data.

Using PyMC is easy to get posterior predictive samples, we don't need to compute any integral. We just need to call the `sample_posterior_predictive` function and pass the `InferenceData` object as the first argument. We also need to pass the model object, and we can use the `extend_inferencedata` argument to add the posterior predictive samples to the `InferenceData` object. The code is:

```python
pm.sample_posterior_predictive(idata_g,
                              model=model_g,
                              extend_inferencedata=True)
```

One common use of the posterior predictive distribution is to perform posterior predictive checks. These are a set of tests that can be used to check if the model is a good fit for the data. We can use the `plot_ppc` function from ArviZ to visualize the posterior predictive distribution and the observed data. The code is:

```python
az.plot_ppc(idata_g, num_pp_samples=100)
```

![Posterior predictive check for `model_g` ploted using `az.plot_ppc`](/fig/idata_g_ppc.png){#fig-idata_g_ppc}


In @fig-idata_g_ppc, the black line is a KDE of the data and the gray lines are KDEs computed from each one of the 100 posterior predictive samples. The gray lines reflect the uncertainty we have about the distribution of the predicted data. The plots look *hairy* or *wonky*, this will happen when you have very few data points. By default, the KDEs in ArviZ are estimated within the actual range of the data and assumed to be zero outside. While some could reckon this as a bug, I think it's a feature, since it's reflecting a property of the data instead of over-smoothing it.

From @fig-idata_g_ppc we can see that the mean of the simulated data is slightly displaced to the right and that the variance seems to be larger for the simulated data than for the actual data. The source of this discrepancy can be attributed to the combination of our choice of likelihood and the two observations away from the bulk of the data (the empty dots in @fig-boxplot). How can we interpret this plot? Is the model wrong or right? Can we use it or do we need a different model? Well, it depends. The interpretation of a model and its evaluation and criticism is always context-dependent. Based on my experience with this kind of measurement I would say this model is a reasonable enough representation of the data and a useful one for most of my analysis. Nevertheless, it is important to keep in mind that we could find other models that accommodate better the whole dataset, including the two observations that are far from the bulk of the data. Let's see how we can do that.

## Robust inferences

One objection we may have with `model_g` is that we are assuming a Normal distribution, but we have two data points away from the bulk of the data. By using a Normal distribution for the likelihood we are indirectly assuming that we are not expecting to see a lot of data points far away from the bulk. @fig-idata_g_ppc shows the result of combining these assumptions with the data. Since the tails of the Normal distribution fall quickly as we move away from the mean, the Normal distribution (at least an anthropomorphized one) is *surprised by seeing* those two points and *reacts* in two ways, moving its mean towards those points and increasing its standard deviation. Another intuitive way of interpreting this is by saying that those points are having an excessive weight in determining the parameters of the Normal distribution.

So, what can we do? One option is to check for errors in the data, if we retrace our steps we may find an error in the code while cleaning or preprocessing the data, or we can relate the putative anomalous values to the malfunction of the measuring equipment. Unfortunately, this is not always an option, many times the data was collected by others and we don't have a good register of how it was collected, measured or processed. Anyway, inspecting the data before modeling is always a good idea, that's a good practice in general.

Another option is to declare those points outliers and remove them from the data. Two common rules of thumb for identifying outliers in a dataset are:

1. Using the interquartile range (IQR): Any data point that falls below 1.5 times the IQR from the lower quartile, or above 1.5 times the IQR from the upper quartile, is considered an outlier.

2. Using the standard deviation: Any data point that falls below or above $N$ times the standard deviation of the data is considered an outlier. With $N$ usually being 2 or 3. 

However, it's important to note that these rules of thumb are not perfect and may result in discarding valid data points. Like any automatic method, these rules of thumb are not perfect and they may discard valid data points.

From a modeling perspective, instead of blaming the data we can blame the model and change it, as explained in the next section.
   

### Degrees of normality 

As a general rule, Bayesians prefer to encode assumptions directly into the model by using different priors and likelihoods rather than through ad hoc heuristics such as outlier removal rules.

There is one distribution that looks very similar to a Normal distribution, it has three parameters: a location parameter $\mu$, a scale parameter $\sigma$ and a normality parameter $\nu$ [^np]. This distribution's name is Student's T distribution, @fig-student_t shows members of this family. When $\nu= \infty$ the distribution is the Normal distribution, $\mu$ is the mean and $\sigma$ is the standard deviation. When $\nu=1$ we get the Cauchy or Lorentz distribution. $\nu$ can go from 0 to $\infty$, the lower this number the heavier their tails, we can also say the higher the kurtosis, with the kurtosis being the fourth moment as you may remember from the previous chapter. By heavy tails, we mean that it is more probable to find values away from the mean compared to a Normal, or in other words values are not as concentrated around the mean as in a lighter tail distribution like the Normal. For example, 95% of the values from a Student's T($\mu=0, \sigma=1, \nu=1$) are found between -12.7 and 12.7. Instead, for a Normal($\mu=0, \sigma=1, \nu=\infty$), this occurs between -1.96 and 1.96.

![The Student's t-distribution.](/fig/student_t.png){#fig-student_t width=80%}

A very curious feature of the Student's t-distribution is that it has no defined mean value when $\nu \le 1$. While any finite sample from a Student's t-distribution is just a bunch of numbers from which it is always possible to compute an empirical mean, the theoretical distribution itself is the one without a defined value for the mean. Intuitively, this can be understood as follows: the tails of the distribution are so heavy that at any moment we might get a sampled value from almost anywhere from the real line, so if we keep getting numbers, we will never approach a fixed value. Instead, the estimate will keep wandering around. 


[^np]: The normality parameter is most commonly known as degrees of freedom. But I preferred Kruschke's suggestion of the normality parameter because it is more descriptive of the parameter's role in the distribution.


Similarly, the variance of this distribution is only defined for values of $\nu > 2$. So, be careful that the scale of the Student's t-distribution is not the same as the standard deviation. The scale and the standard deviation become closer and closer as $\nu$ approaches infinity:


### A robust version of the Normal model

We are going to rewrite the previous model (`model_g`) by replacing the Gaussian distribution with the Student's t-distribution:


$$
\begin{aligned}
\mu &\sim \mathcal{U}(l, h) \\
\sigma &\sim \mathcal{HN}(\sigma_{\sigma}) \\
\nu &\sim \text{Exp}(\lambda) \\
Y &\sim \mathcal{T}(\nu, \mu, \sigma)
\end{aligned}
$$

Because the Student's t-distribution has one more parameter ($\nu$) than the Gaussian, we need to specify one more prior. We are going to set $\nu$ as an Exponential distribution with a mean of 30. From Figure 2.12, we can see that a Student's t-distribution with $\nu = 30$ looks pretty similar to a Gaussian (even when it is not). In fact, from the same diagram, we can see that *most of the action* happens for relatively small values of $\nu$. Hence, we can say that the Exponential prior with a mean of 30 is a weakly informative prior telling the model we more or less think should be around 30 but can move to smaller and larger values with ease. In many problems, estimating $\nu$ is of no direct interest.

@fig-model_t_dag shows the graphical representation of this model.

![Graphical representation of `model_t`](/fig/model_t_dag.pdf){#fig-model_t_dag width="50%"}


As usual, PyMC allows us to (re)write models just by specifying a few lines. The only cautionary word here is that the Exponential  in PyMC is parameterized with the inverse of the mean:

```python
with pm.Model() as model_t:
    μ = pm.Uniform('μ', 40, 75)
    σ = pm.HalfNormal('σ', sigma=10)
    ν = pm.Exponential('ν', 1/30)
    y = pm.StudentT('y', nu=ν, mu=μ, sigma=σ, observed=data)
    idata_t = pm.sample()
```

Compare the trace from `model_g` (@fig-idata_g_trace) with the trace of `model_t` (@fig-idata_t_trace). Now, print the summary of `model_t` and compare it with the one from `model_g`. Before you keep reading, take a moment to spot the difference between both results. Did you notice something interesting?

![Posterior from `model_t` ploted using `az.plot_trace(idata_t)`](/fig/idata_t_trace.png){#fig-idata_t_trace}

And we get the following table as output:

|          |   mean |   sd |   hdi_3% |   hdi_97% |
|:--------:|-------:|-----:|---------:|----------:|
| $\mu$	   |  53.02 | 0.39 |  52.27   | 53.71     |
| $\sigma$ |   2.21 | 0.42 |   1.46   |  3.01     |
| $\nu$    |   4.94 | 5.45 |   1.07   | 10.10     |



The estimation of $\mu$ between both models is similar, with a difference of $\approx0.5$. The estimation of $\sigma$ is $\approx 3.5$ for `model_g` and $\approx 2.2$ for `model_t`. This is a consequence of the Student's t-distribution allocating less weight to values away from the mean. Loosely speaking, the Student's t-distribution is *less surprised* by values away from the mean. We can also see that the mean of $\nu$ is $\approx 5$, meaning that we have heavy-tailed distribution and not Gaussian-like distribution.

@fig-idata_t_ppc shows a posterior predictive check for `model_t`. Let's compare it with the one form `model_g` (@fig-idata_g_ppc). Using the Student's t-distribution in our model leads to predictive samples that seem to better fit the data in terms of the location of the peak of the distribution and also its spread. Notice how the samples extend far away from the bulk of the data, and how a few of the predictive samples look very flat. This is a direct consequence of the Student's t-distribution expecting to see data points far away from the mean or bulk of the data. If you check the code used to generate @fig-idata_t_ppc you will see that we have used `ax.set_xlim(40, 70)`.


![Posterior predictive check for `model_t`](/fig/idata_t_ppc.png){#fig-idata_t_ppc}

The Student's t-distribution allows us to have a more **robust estimation** of the mean and standard deviation, because the outliers have the effect of decreasing $\nu$, instead of pulling the mean or increasing the standard deviation. Thus, the mean and the scale are estimated by weighting the data points close to the bulk more than those apart from it.  As a rule of thumb, for values of $\nu > 2$ and *not too small*, we can consider the scale of a Student's t-distribution as a reasonable practical proxy for the standard deviation of the data after removing outliers. This is a rule of thumb because we know that the scale is not the standard deviation


## InferenceData

InferenData is a rich container for the results of Bayesian inference. A modern Bayesian analysis potentially generates many sets of data including posterior samples and posterior predictive samples. But we also have observed data, samples from the prior and even statistics generated by the sampler. All this data, and more, can be stored in an InferenceData object. To help keep all this information organized each one of these sets of data has its group. For instance, the posterior samples are stored in the `posterior` group. The observed data is stored in the `observed_data` group. @fig-idata shows an HMTL representation of the InferenceData for `model_g`. We can see 4 groups `posterior`, `posterior_predictive`, `sample_stats` and `observed_data`. All of them are collapsed except for the `posterior` group. We can see we have two coordinates `chain` and `draw` of dimensions 4 and 1000 respectively. We also have 2 variables $\mu$ and $\sigma$  

![InferenceData object for `model_g`](/fig/idata.png){#fig-idata}


So far PyMC has generated an InferenceData object and ArviZ has used that to generate plots or numerical summaries. But we can also manipulate an InferenceData object. Some common operations are to access specific groups, for instance, to access the posterior group we can write:

```python
posterior = idata_g.posterior
```

This will return a xarray Dataset. If you are not familiar with xarray [@xarray_2017] [^xarray], imagine NumPy multidimensional arrays but with labels! This makes many operations easier as you don't have to remember the order of the dimensions. For example

```python
posterior.sel(draw=0, chain=[0, 2])
```

Will return the first draw from chain 0 and chain 2. We can also use the `sel` method to select a range of values. For example

```python
posterior.sel(draw=slice(0, 100))
```

returns the first 100 draws from all chains

```python
posterior.mean()
```

returns the mean for $\mu$ and $\sigma$ computed over all draws and chains, while

```python
posterior.mean("draw")
```

returns the mean over the draws, i.e. this returns four values for $\mu$ and four values for $\sigma$, one per chain.

More often than not we don't care about chains and draws, we just want to get the posterior samples. In those cases we can use the `az.extract` function:

```python
stacked = az.extract(idata_g)
```

This combines the `chain` and `draw` into a `sample` coordinate which can make further operations easier. By default `az.extract` works on the posterior, but you can specify other groups with the `group` argument. You can also use `az.extract` to get a random sample of the posterior:

```python
az.extract(idata_g, num_samples=100)
```

We are going to use InferenceData object all the time in this book, so you will have the time to get familiar with it and learn more about it in the coming pages.


[^xarray]: <https://docs.xarray.dev/en/stable/>

## Groups comparison

One pretty common statistical analysis is group comparison. We may be interested in how well patients respond to a certain drug, the reduction of car accidents by the introduction of new traffic regulations, student performance under different teaching approaches, and so on. Sometimes, this type of question is framed under the hypothesis testing scenario and the goal is to declare a result *statistically significant*. Relying only on statistical significance can be problematic for many reasons: on the one hand, statistical significance is not equivalent to practical significance; on the other hand, a really small effect can be declared significant just by collecting enough data. The idea of hypothesis testing is connected to the concept of p-values. This is not a fundamental connection but a cultural one; people are used to thinking that way mostly because that's what they learn in most introductory statistical courses. There is a long record of studies and essays showing that, more often than not, p-values are used and interpreted the wrong way, even by people who are using them daily. Instead of doing hypothesis testing, we are going to take a different route and we are going to focus on estimating the effect size, that is, quantifying the difference between two groups. One advantage of thinking in terms of effect size is that we move away from the yes-no questions like; Does it work? or Is there any effect? into the more nuanced type of question like; How well does it work? How large is the effect?


Sometimes, when comparing groups, people talk about a control group and a treatment group. For example, when we want to test a new drug, we want to compare the new drug (the treatment) against a placebo (the control group). The placebo effect is a psychological phenomenon where a patient experiences perceived improvements in their symptoms or condition after receiving an inactive substance or treatment. By comparing the effects of the drug with a placebo group in clinical trials, researchers can discern whether the drug is genuinely effective. The placebo effect is an example of the broader challenge in experimental design and statistical analysis of the difficulty of accounting for all factors in an experiment. 

One interesting alternative to this design is to compare the new drug with the commercially available most popular or efficient drug to treat that illness. In such a case, the control group cannot be a placebo; it should be the other drug. Bogus control groups are a splendid way to lie using statistics.

For example, imagine you work for a dairy product company that wants to sell overly sugared yogurts to kids by telling their dads and moms that this particular yogurt boosts the immune system or helps their kids grow stronger. One way to cheat with data is by using milk or even water as a control group, instead of another cheaper, less sugary, less marketed yogurt. It may sound silly when I put it this way, but I am describing actual experiments published in actual scientific journals. When someone says something is harder, better, faster, or stronger, remember to ask what the baseline used for the comparison was.


### The tips dataset

To explore the subject matter of this section, we are going to use the tips dataset [@bryant_1995].  We want to study the effect of the day of the week on the tips earned at a restaurant. For this example, the different groups are the days. Notice there is no control group or treatment group. If we wish, we can arbitrarily establish one day, for example, Thursday, as the reference or control. For now, let's start the analysis by loading the dataset as a pandas DataFrame using just one line of code. If you are not familiar with pandas, the tail  command is used to show the last rows of a DataFrame (you can also try using head):

```python
tips = pd.read_csv("data/tips.csv")
tips.tail()
```

|     | total_bill | tip | sex | smoker | day | time | size |
|-----|------------|-----|-----|--------|-----|------|------|
| 239 | 29.03 | 5.92 | Male | No | Sat | Dinner | 3 |
| 240 | 27.18 | 2.00 | Female | Yes | Sat | Dinner | 2 |
| 241 | 22.67 | 2.00 | Male | Yes | Sat | Dinner | 2 |
| 242 | 17.82 | 1.75 | Male | No | Sat | Dinner | 2 |
| 243 | 18.78 | 3.00 | Female | No | Thurs | Dinner | 2 |


From this DataFrame, we are only going to use the day and tip columns.@fig-tips_ridgeplot shows the distributions of this data using ridge plots. This figure was done with ArviZ. Even though ArviZ is designed for Bayesian model analysis, some of its functions can be useful for data analysis.


![Distribution of tips by day)`](/fig/tips_ridgeplot.png){#fig-tips_ridgeplot.png}

We are going to do some small preprocessing of the data. First, we are going to create the variable `tip` representing the tips in dollars. Then we create the `idx` variable, a categorical dummy variable encoding the days with numbers,  that is, `[0, 1, 2, 3]` instead of `['Thur', 'Fri', 'Sat', 'Sun']`.

```python
categories = np.array(["Thur", "Fri", "Sat", "Sun"])

tip = tips["tip"].values
idx = pd.Categorical(tips["day"], categories=categories).codes
```

The model for this problem is almost the same as `model_g`; the only difference is that now $\mu$ and $\sigma$ are going to be vectors instead of scalars. PyMC syntax is extremely helpful for this situation: instead of writing for loops, we can write our model in a vectorized way.

```python
    with pm.Model() as comparing_groups:
        μ = pm.Normal("μ", mu=0, sigma=10, shape=4)
        σ = pm.HalfNormal("σ", sigma=10, shape=4)

        y = pm.Normal("y", mu=μ[idx], sigma=σ[idx], observed=tip)
```

Notice how we passed a `shape` argument for the prior distribution. For $\mu$ this means that we are specifying four independent $\mathcal{N}(0, 10)$ and for $\sigma$ four independent $\mathbfcal{HN}(10)$. Also, notice how we use the `idx` variable to properly index the values of $\mu$ and $\sigma$ we pass to the likelihood.

PyMC provides an alternative syntax, which consists of specifying coordinates and dimensions. The advantage of this alternative is that it allows better integration with ArviZ.

Let's see, in this example, we have 4 values for the means and 4 for the standard deviations, and that's why we use `shape=4`. The InferenceData will have 4 indices `0, 1, 2, 3` mapping to each of the 4 days. But it is the user's job to associate those numerical indices with the days. By using coordinates and dimensions we, and ArviZ, can use the labels `"Thur", "Fri", "Sat", "Sun"` to easily maps parameters to their associated days.

We are going to specify two coordinates, `"days"` with the dimensions `"Thur", "Fri", "Sat", "Sun"` and `"days_flat"` that will contain the same labels but repeated according to the order and length that corresponds to each observation. `"days_flat"` will be useful later, for posterior predictive tests.

```python
coords = {"days": categories, "days_flat":categories[idx]}

with pm.Model(coords=coords) as comparing_groups:
    μ = pm.HalfNormal("μ", sigma=5, dims="days")
    σ = pm.HalfNormal("σ", sigma=1, dims="days")

    y = pm.Gamma("y", mu=μ[idx], sigma=σ[idx], observed=tip, dims="days_flat")

    idata_cg = pm.sample()
    idata_cg.extend(pm.sample_posterior_predictive(idata_cg))
```

Once the posterior distribution is computed, we can do all the analyzes that we believe are pertinent. Let's do a posterior predictive test, by calling `az.plot_ppc`, we use the `coords` and `flatten` parameters to get one subplot per day.

```python
_, axes = plt.subplots(2, 2)
az.plot_ppc(idata_cg, num_pp_samples=100,
            coords={"days_flat":[categories]}, flatten=[], ax=axes)
```

![Posterior predictive checks for the tips dataset](/fig/ppc_tips.png){#fig-ppc_tips}

From @fig-ppc_tips we can see that the model can capture the general shape of the distributions, still, some details are elusive. This may be due to the relatively small sample size, factors other than day influencing the tips, or a combination of both. 

For now, we are going to consider that the model is good enough for us and move to explore the posterior. @fig-tips_posterior is quite informative, for instance, we see that the average values of tips are a few cents and that for Sundays the value is slightly higher than for the rest of the days analyzed. But perhaps we consider that it may be better to display the data in another way. For example, we may want to express the results in terms of differences in posterior means. In addition, we might want to use some measure of effect size that is popular with our audiences, such as the probability of superiority or Cohen's d.

### Cohen's d

A common way to measure the effect size is Cohen's d, which is defined as follows:
$$
\frac{\mu_2 - \mu_1}{\sqrt{\frac{\sigma_1^2 + \sigma_2^2}{2}}}
$$

Because we have a posterior distribution we can compute a distribution of Cohen's d, and if we want a single value we can compute the mean or median of that distribution. 

This expression tells us that the effect size is the difference between the means scaled by the pooled standard deviation of both groups. By taking the pooled standard deviation we are standardizing the differences of means by considering the pooled standard deviation, this is important as a difference of 1 when you have a standard deviation of 0.1 is larger that the same difference when the standard deviation is 10. A Cohen's d can be interpreted as a Z-score (a standard score). A Z-score is the signed number of standard deviations by which a value differs from the mean value of what is being observed or measured. Thus, a value of 0.5 Cohen's d could be interpreted as a difference of 0.5 standard deviations from one group to the other.

Even when the differences of means are standardized, we may still need to calibrate ourselves based on the context of a given problem to be able to say if a given value is big, small, medium, and so on. For instance, if we are used to performing several analyses for the same or similar problems we can get used to a Cohen's d of say 1, so when we get a Cohen's d of say 2, we know we have something important (or someone made a mistake somewhere!). If you do not have this practice yet, you can ask a domain expert for their valuable input. A very nice web page to explore what different values of Cohen's d look like is <http://rpsychologist.com/d3/cohend>. On that page, you will also find other ways to express an effect size; some of them could be more intuitive, such as the probability of superiority, which we will discuss next.

### Probability of superiority

This is another way to report the effect size, and this is defined as the probability that a data point taken at random from one group has a larger value than one also taken at random from the other group. If we assume that the data we are using is normally distributed, we can compute the probability of superiority from Cohen's d using the following expression:

$$
\text{ps} = \Phi \left( \frac{\delta}{\sqrt{2}} \right)
$$

where $\Phi$ is the cumulative Normal distribution and $\delta$ is the Cohen's d. 

If we are OK with the normality assumption, we can use this formula to get the probability of superiority from the value of Cohen's d. Otherwise, we can compute the probability of superiority directly from the posterior samples just by taking random samples from two groups and counting how many times one value is larger than the other. To do that we don't need Cohen's d or assume normality (see the Exercises section). This is an example of an advantage of using Markov chain Monte Carlo (MCMC) methods; once we get samples from the posterior, we can compute many quantities from it often in ways that are easier than with other methods.

### Posterior analysis of mean differences

Let's wrap up all the previous discussion by computing the posterior distributions of the differences of means, the Cohends d and the probability of superiority and put all of that into a single plot. @fig-tips_posterior has a lot of information. Depending on the audience, the plot may be overloaded, or too crowded. Perhaps it is useful for a discussion within your team, but for the general public, it may be convenient to remove elements or distribute the information between a figure and a table or two figures. Anyway, here we show it precisely so you can compare different ways of presenting the same information, so take some time to ponder this figure. 

![Posterior distributions of the differences of means, Cohen's d and the probability of superiority for the tips dataset](/fig/tips_posterior.png){#fig-tips_posterior}


One way to read @fig-tips_posterior is by comparing the reference value, of zero difference, with the HDI interval. We have only one case when the 94% HDI excludes the reference value, that is, the difference in tips between Thursday and Sunday. For all the other comparisons, we cannot rule out a difference of zero, at least according to the HDI-reference-value-overlap criteria. But even for that case, the average difference is $\approx 0.5$ dollars. Is that difference large enough? Is that difference enough to accept working on Sunday and missing the opportunity to spend time with family or friends? Is that difference enough to justify averaging the tips over the four days and giving every waitress and waiter the same amount of tip money? The short answer is that those kinds of questions cannot be answered by statistics; they can only be informed by statistics. I hope you don't feel cheated by that answer, but we can not get automatic answers unless we include in the analysis all the values that are important to the stakeholders. Formally, that requires the definition of a loss function or at least the definition of some threshold value for the effect size, which should be informed by those values.


## Exercises


1. Using PyMC, change the parameters of the prior Beta distribution in `our_first_model` to match those of the previous chapter. Compare the results to the previous chapter.

2. Compare the model `our_first_model` with prior $\theta \sim \text{Beta}(1, 1)$ with a model with prior $\theta \sim \mathcal{U}(0, 1)$. Are the posterior similar or different? Is the sampling slower, faster, or the same? What about using a Uniform over a different interval such as [-1, 2]? Does the model run? What errors do you get?

3. PyMC has a function `pm.model_to_graphviz` that can be used to visualize the model. Use it to visualize the model `our_first_model` Compare the result with the Kruschke diagram. Use `pm.model_to_graphviz` to visualize model `comparing_groups`.

4. Read about the coal mining disaster model that is part of the PyMC documentation (<https://shorturl.at/hyCX2>). Try to implement and run this model by yourself.

5. Modify `model_g`, change the prior for the mean to a Gaussian distribution centered at the empirical mean, and play with a couple of reasonable values for the standard deviation of this prior. How robust/sensitive are the inferences to
these changes? What do you think of using a Gaussian, which is an unbounded distribution (goes from $-\inf$ to $\inf$), to model bounded data such as this? Remember that we said it is not possible to observe values below 0 or above 100.

6. Using the data from `chemical_shifts.csv` file, compute the empirical mean and the standard deviation with and without outliers. Compare those results to the Bayesian estimation using the Gaussian and Student's t-distribution. What do you observe?

7. Repeat the previous exercise by adding more outliers to `chemical_shifts.csv`, and compute new posteriors for  `model_g` and `model_t` using this new data. What do you observe?

8. Explore the InferecenData object `idata_cg`.

a. How many groups it contains
b. Inspect the posterior distribution of the parameter $\mu$ for a specific day using the `sel` method.
c. Compute the distributions of mean differences between Thursday and Sunday. What are the coordinates and dimensions of the resulting DataArray?

9. For the tips example compute the probability of superiority directly from the posterior (without computing Cohen's d first). You can use the `pm.sample_posterior_predictive()` function to take a sample from each group. Is it different from the calculation assuming normality? Can you explain the result?







<!-- ::: callout-note
Two important things to note. See that we use the name $\theta$ twice, first as a Python variable and then as a string for the first argument of the Beta function (and the same goes for $y$); using the same name is a good practice to avoid confusion. The $\theta$ variable is a random variable; it is not a number, but an object representing a probability distribution from which we can compute random numbers and probability densities. -->





<!-- 
## Posterior predictive checks

One of the nice elements of the Bayesian toolkit is that once we have a posterior $p(\theta \mid Y)$, it is
possible to use it to generate predictions, $p(\tilde Y)$. Mathematically this can be done by computing:

$$
p(\tilde Y \mid Y) = \int p(\tilde Y \mid \theta) p(\theta \mid Y) d\theta
$$

This is the **posterior predictive distribution**. *Posterior* because we are taking the values of $\theta$ according to the posterior distribution, *Predictitve* because we are computing the distribution of the data, $\tilde Y$, given the observed data, $Y$. Or in other words predictions. And *distribution* because we are not computing a single value, but a distribution of values.

In practice, we are not going to directly compute this integral, but we will approximate it by sampling in two steps:

1. We sample a value of $\theta$ from the posterior $p(\theta \mid Y)$,
2. We feed that value of to the likelihood $p(Y \mid \theta)$

This will generate one sample from $p(\tilde Y)$, we keep doing this many times. The resulting samples will be distributed as the posterior predictive distribution.


:::{.callout-note}
The posterior predictive distribution combines two sources of uncertainty: the uncertainty of the parameters; as captured by the posterior; and the sampling uncertainty; as captured by the likelihood
:::

 show how to sample using PreliZ and then show how to do it using PyMC -->
