<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Bayesian Analysis with Python - 1&nbsp; Thinking Probabilistically</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./references.html" rel="next">
<link href="./index.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Thinking Probabilistically</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Bayesian Analysis with Python</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Preface</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chp_01.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Thinking Probabilistically</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#statistics-models-and-this-books-approach" id="toc-statistics-models-and-this-books-approach" class="nav-link active" data-scroll-target="#statistics-models-and-this-books-approach"><span class="toc-section-number">1.1</span>  Statistics, models, and this book’s approach</a></li>
  <li><a href="#working-with-data" id="toc-working-with-data" class="nav-link" data-scroll-target="#working-with-data"><span class="toc-section-number">1.2</span>  Working with data</a></li>
  <li><a href="#bayesian-modeling" id="toc-bayesian-modeling" class="nav-link" data-scroll-target="#bayesian-modeling"><span class="toc-section-number">1.3</span>  Bayesian modeling</a></li>
  <li><a href="#probability-theory" id="toc-probability-theory" class="nav-link" data-scroll-target="#probability-theory"><span class="toc-section-number">1.4</span>  Probability theory</a>
  <ul class="collapse">
  <li><a href="#sample-spaces-and-events" id="toc-sample-spaces-and-events" class="nav-link" data-scroll-target="#sample-spaces-and-events"><span class="toc-section-number">1.4.1</span>  Sample Spaces and Events</a></li>
  </ul></li>
  <li><a href="#interpreting-probabilities" id="toc-interpreting-probabilities" class="nav-link" data-scroll-target="#interpreting-probabilities"><span class="toc-section-number">1.5</span>  Interpreting probabilities</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Thinking Probabilistically</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<blockquote class="blockquote">
<p>“Probability theory is nothing but common sense reduced to calculation.” - Pierre Simon Laplace</p>
</blockquote>
<p>In this chapter, we will learn about the core concepts of Bayesian statistics and some of the instruments in the Bayesian toolbox. We will use some Python code, but this chapter will be mostly theoretical; most of the concepts we will see here will be revisited many times throughout this book. This chapter, being heavy on the theoretical side, is perhaps a little anxiogenic for the coder in you, but I think it will ease the path in effectively applying Bayesian statistics to your problems.</p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>Statistical modeling</li>
<li>Probabilities and uncertainty</li>
<li>Bayes’ theorem and statistical inference</li>
<li>Single-parameter inference and the classic coin-flip problem</li>
<li>Choosing priors and why people often don’t like them, but should</li>
<li>Communicating a Bayesian analysis</li>
</ul>
<section id="statistics-models-and-this-books-approach" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="statistics-models-and-this-books-approach"><span class="header-section-number">1.1</span> Statistics, models, and this book’s approach</h2>
<p>Statistics is about collecting, organizing, analyzing, and interpreting data, and hence statistical knowledge is essential for data analysis. Two main statistical methods are used in data analysis:</p>
<ul>
<li>Exploratory Data Analysis (EDA): This is about numerical summaries, such as the mean, mode, standard deviation, and interquartile ranges (this part of EDA is also known as descriptive statistics). EDA is also about visually inspecting the data, using tools you may be already familiar with, such as histograms and scatter plots. *Inferential statistics: This is about making statements beyond the current data. We may want to understand some particular phenomenon, or maybe we want to make predictions for future (as yet unobserved) data points, or we need to choose among several competing explanations for the same observations. Inferential statistics is the set of methods and tools that will help us to answer these types of questions.</li>
</ul>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>The focus of this book is on how to perform Bayesian inferential statistics, and then how to use EDA to summarize, interpret, check, and communicate the results of Bayesian inference.</p>
</div>
</div>
<p>Most introductory statistical courses, at least for non-statisticians, are taught as a collection of recipes that go like this: go to the statistical pantry, pick one tin can and open it, add data to taste, and stir until you obtain a consistent p-value, preferably under 0.05. The main goal of these courses is to teach you how to pick the proper can. I never liked this approach, mainly because the most common result is a bunch of confused people unable to grasp, even at the conceptual level, the unity of the different learned methods. We will take a different approach: we will also learn some recipes, but these will be homemade rather than canned food; we will learn how to mix fresh ingredients that will suit different gastronomic occasions and, more importantly, that will let you apply concepts far beyond the examples in this book.</p>
<p>Taking this approach is possible for two reasons:</p>
<ul>
<li><p><strong>Ontological</strong>: Statistics is a form of modeling unified under the mathematical framework of probability theory. Using a probabilistic approach provides a unified view of what may seem like very disparate methods; statistical methods and machine learning methods look much more similar under the probabilistic lens.</p></li>
<li><p><strong>Technical</strong>: Modern software, such as PyMC, allows practitioners, just like you and me, to define and solve models in a relatively easy way. Many of these models were unsolvable just a few years ago or required a high level of mathematical and technical sophistication.</p></li>
</ul>
</section>
<section id="working-with-data" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="working-with-data"><span class="header-section-number">1.2</span> Working with data</h2>
<p>Data is an essential ingredient in statistics and data science. Data comes from several sources, such as experiments, computer simulations, surveys, and field observations. If we are the ones in charge of generating or gathering the data, it is always a good idea to first think carefully about the questions we want to answer and which methods we will use, and only then proceed to get the data. There is a whole branch of statistics dealing with data collection, known as experimental design. In the era of the data deluge, we can sometimes forget that gathering data is not always cheap. For example, while it is true that the Large Hadron Collider (LHC) produces hundreds of terabytes a day, its construction took years of manual and intellectual labor.</p>
<p>As a general rule, we can think of the process generating the data as stochastic, because there is ontological, technical, and/or epistemic uncertainty, that is, the system is intrinsically stochastic, there are technical issues adding noise or restricting us from measuring with arbitrary precision, and/or there are conceptual limitations veiling details from us. For all these reasons, we always need to interpret data in the context of models, including mental and formal ones. Data does not speak but through models.</p>
<p>In this book, we will assume that we already have collected the data. Our data will also be clean and tidy, something rarely true in the real world. We will make these assumptions to focus on the subject of this book. I just want to emphasize, especially for newcomers to data analysis, that even when not covered in this book, these are important skills that you should learn and practice to successfully work with data.</p>
<p>A very useful skill when analyzing data is knowing how to write code in a programming language, such as Python. Manipulating data is usually necessary given that we live in a messy world with even messier data, and coding helps to get things done. Even if you are lucky and your data is very clean and tidy, coding will still be very useful since modern Bayesian statistics is done mostly through programming languages such as Python or R. If you want to learn how to use Python for cleaning and manipulating data, you can find a good introduction in Python for Data Analysis by <span class="citation" data-cites="mckinney_2022">(<a href="references.html#ref-mckinney_2022" role="doc-biblioref">2022</a>)</span></p>
</section>
<section id="bayesian-modeling" class="level2" data-number="1.3">
<h2 data-number="1.3" class="anchored" data-anchor-id="bayesian-modeling"><span class="header-section-number">1.3</span> Bayesian modeling</h2>
<p>Models are simplified descriptions of a given system or process that, for some reason, we are interested in. Those descriptions are deliberately designed to capture only the most relevant aspects of the system and not to explain every minor detail. This is one reason a more complex model is not always a better one. There are many different kinds of models; in this book, we will restrict ourselves to Bayesian models. We can summarize the Bayesian modeling process using three steps:</p>
<ol type="1">
<li>Given some data and some assumptions on how this data could have been generated, we design a model by combining building blocks known as <strong>probability distributions</strong>. Most of the time these models are crude approximations, but most of the time that’s all we need.</li>
<li>We use Bayes’ theorem to add data to our models and derive the logical consequences of combining the data and our assumptions. We say we are <strong>conditioning</strong> the model on our data.</li>
<li>We evaluate the model, and its predictions, under different criteria, including the data, our expertise on the subject, and sometimes by comparing it to other models.</li>
</ol>
<p>In general, we will find ourselves performing these three steps in an iterative non-linear fashion. We will retrace our steps at any given point: maybe we made a silly coding mistake, or we found a way to change the model and improve it, or we realized that we need to add more data or collect a different kind of data.</p>
<p>Bayesian models are also known as <strong>probabilistic models</strong> because they are built using probabilities. Why probabilities? Because probabilities are a very useful tool to model uncertainty, we even have very good arguments to state they are the correct mathematical concept. So let’s take a walk through the garden of forking paths <span class="citation" data-cites="borges_2018">(<a href="references.html#ref-borges_2018" role="doc-biblioref"><strong>borges_2018?</strong></a>)</span>.</p>
</section>
<section id="probability-theory" class="level2" data-number="1.4">
<h2 data-number="1.4" class="anchored" data-anchor-id="probability-theory"><span class="header-section-number">1.4</span> Probability theory</h2>
<p>The title of this section may be a little bit pretentious, it is not possible to learn probability theory in just a few pages, but that is not my intention. I just want to introduce a few general and important concepts that are necessary to better understand Bayesian methods, and that should be enough for understanding the rest of the book. If necessary, we will expand or introduce new probability-related concepts as we need them. For a detailed study of probability theory, I highly recommend the book, Introduction to Probability by Joseph K Blitzstein and Jessica Hwang <span class="citation" data-cites="blitzstein_2019">(<a href="references.html#ref-blitzstein_2019" role="doc-biblioref"><strong>blitzstein_2019?</strong></a>)</span>.</p>
<section id="sample-spaces-and-events" class="level3" data-number="1.4.1">
<h3 data-number="1.4.1" class="anchored" data-anchor-id="sample-spaces-and-events"><span class="header-section-number">1.4.1</span> Sample Spaces and Events</h3>
<p>Let’s say we are conducting a survey to see how people feel about the weather in their area. We ask three individuals whether they enjoy sunny weather, with possible responses being “yes” or “no”. The sample space of all possible outcomes can be denoted by <span class="math inline">\(S\)</span> and consists of eight possible combinations:</p>
<p><span class="math inline">\(S\)</span> = {(yes, yes, yes), (yes, yes, no), (yes, no, yes), (no, yes, yes), (yes, no, no), (no, yes, no), (no, no, yes), (no, no, no)}</p>
<p>Here, each element of the sample space represents the responses of the three individuals in the order they were asked. For example, (yes, no, yes) means the first and third people answered “yes” while the second person answered “no”.</p>
<p>We can define events as subsets of the sample space, such as the event <span class="math inline">\(A\)</span> being that all three individuals answered “yes”:</p>
<p><span class="math inline">\(A\)</span> = {(yes, yes, yes)}</p>
<p>Similarly, we can define event <span class="math inline">\(B\)</span> that at least one person answered “no”, and then we will have:</p>
<p><span class="math inline">\(B\)</span> = {(yes, yes, no), (yes, no, yes), (no, yes, yes), (yes, no, no), (no, yes, no), (no, no, yes), (no, no, no)}</p>
<p>We can use probabilities as a measure of how likelly is a particular event. If the event is certain, for instance we go outside and we see that is actually raining, then the probability of that event will be 1 and if there is not rain 0. Most interesting things happend when probabilities take numbers in between. Let us see.</p>
<p>Given the sample space <span class="math inline">\(S\)</span>, the probability of event A, which is the event that all three individuals answered “yes”, is:</p>
<p>$$P(A) = </p>
<p>In this case, there is only one outcome in A, and there are eight outcomes in <span class="math inline">\(S\)</span>. Therefore, the probability of <span class="math inline">\(A\)</span> is:</p>
<p><span class="math display">\[P(A) = \frac{1}{8} = 0.125\]</span></p>
<p>Similarly, we can calculate the probability of event <span class="math inline">\(B\)</span>, which is the event that at least one person answered “no”. Since there are seven outcomes in <span class="math inline">\(B\)</span> and eight outcomes in <span class="math inline">\(S\)</span>, the probability of <span class="math inline">\(B\)</span> is:</p>
<p><span class="math display">\[P(B) = 7/8 = 0.875\]</span></p>
<p>Notice that for these computations we have assumed that all event in <span class="math inline">\(S\)</span> are equally likely, but that’s not need to be like that. Considering all event equally likely is just a particular case.</p>
<p>Now let us suppose we ask one person whether it will rain tomorrow, with possible responses “yes” and “no”. The sample space of possible responses is <span class="math inline">\(S = {\text{yes}, \text{no}}\)</span>. An the even that it will rain tomorrow is <span class="math inline">\(A = {\text{yes}}\)</span></p>
<p>The axioms of probability apply as follows:</p>
<pre><code>Non-negativity: The probability of any event is a non-negative number. For example, the probability of event A, that it will rain tomorrow, is P(A) = 1/2, which is a non-negative number.

Normalization: The probability of the entire sample space is equal to 1. In this case, the probability of the sample space S = {yes, no} is P(S) = 1, since it will either rain or not rain tomorrow.

Additivity: The probability of the union of any two disjoint events is equal to the sum of their individual probabilities. For example, let's consider two events: A, which is the event that it will rain tomorrow, and B, which is the event that it will not rain tomorrow. These events are disjoint, meaning they cannot occur simultaneously. Therefore, the probability of their union (i.e., the event that either A or B occurs) is:</code></pre>
<p>P(A U B) = P(A) + P(B)</p>
<p>Since B is the complement of A, meaning that it will either rain or not rain tomorrow, we have P(B) = 1 - P(A). Therefore:</p>
<p>P(A U B) = P(A) + P(B) = P(A) + (1 - P(A)) = 1</p>
<p>Since P(A U B) is equal to 1, the axiom of additivity is also satisfied in this example.</p>
<p>Nevertheless there is more that one</p>
<p>This means that if we were to repeat the survey many times, we would expect all three individuals to answer “yes” about 12.5% of the time.</p>
<p>Similarly, we can calculate the probability of event B, which is the event that at least one person answered “no”. Since there are seven outcomes in B and eight outcomes in S, the probability of B is:</p>
<p>P(B) = 7/8 = 0.875</p>
<p>This means that if we were to repeat the survey many times, we would expect at least one person to answer “no” about 87.5% of the time.</p>
<p>We can use probabilities to discribe how likelly an event is. If the event is certain, we go outside and is raining, then p(R) = 1 if there is no rain p(R) = 0. Things becomes more interesting for values between 0 and 1. For instance if we see dark clouds and hear thunder we may say I think is veru likely that it will rain in the next few minutes and to formalize this idea we can assign a number between 0 and 1 to p(R), with the number being closer to 1 the more likely we consider rain to happend. This is what whether forescast do, but usually they use %, i.e.&nbsp;the scale goes from 0 to 100, but the principle is the same. We can think of pobabilities as a conserved quantity. In the sense that if all that cvan happend is desctribed by <span class="math inline">\(S\)</span> the p(S) = 1, i.e.&nbsp;something must happend and evne more if all that can happend is R and not-R then if p(R) increases p(not-R) should decrease by the same amount so that the total probability of both evnet remainS</p>
<p>Probabilities are numbers in the <span class="math inline">\([0, 1]\)</span> interval, that is, numbers between 0 and 1, including both extremes, and they folow certain rules.</p>
<p>Probabilities follow a few rules; on such rule is the product rule:</p>
<p><span class="math display">\[p(A, B) = p(A \mid B) p(B)\]</span></p>
<p>We read this as follows: the probability of <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> is equal to the probability of <span class="math inline">\(A\)</span> given <span class="math inline">\(B\)</span>, times the probability of <span class="math inline">\(B\)</span>.</p>
</section>
</section>
<section id="interpreting-probabilities" class="level2" data-number="1.5">
<h2 data-number="1.5" class="anchored" data-anchor-id="interpreting-probabilities"><span class="header-section-number">1.5</span> Interpreting probabilities</h2>
<p>While probability theory is a mature and well-established branch of mathematics, there is more than one interpretation of probability. From a Bayesian perspective, a probability is a measure that quantifies the uncertainty level of a statement. Under this definition of probability, it is totally valid and natural to ask about the probability of life on Mars, the probability of the mass of the electron being 9.1 x 10-31 kg, or the probability of the 9th of July of 1816 being a sunny day in Buenos Aires. Notice, for example, that life on Mars exists or does not exist; the outcome is binary, a yes-no question. But given that we are not sure about that fact, a sensible course of action is trying to find out how likely life on Mars is. Since this definition of probability is related to our epistemic state of mind, people often call it the subjective definition of probability. But notice that any scientific-minded person will not use their personal beliefs, or the information provided by an angel to answer such a question, instead they will use all the relevant geophysical data about Mars, and all the relevant biochemical knowledge about necessary conditions for life, and so on. Therefore, Bayesian probabilities, and by extension Bayesian statistics, is as subjective (or objective) as any other well-established scientific method we have. If we do not have information about a problem, it is reasonable to state that every possible event is equally likely, formally this is equivalent to assigning the same probability to every possible event. In the absence of information, our uncertainty is maximum. If we know instead that some events are more likely, then this can be formally represented by assigning higher probability to those events and less to the others. Notice than when we talk about events in stats-speak, we are not restricting ourselves to things that can happen, such as an asteroid crashing into Earth or my auntie’s 60th birthday party; an event is just any of the possible values (or subset of values) a variable can take, such as the event that you are older than 30, or the price of a Sachertorte, or the number of bikes sold last year around the world.</p>
<p>The concept of probability it is also related to the subject of logic. Under Aristotelian or classical logic, we can only have statements that take the values of true or false. Under the Bayesian definition of probability, certainty is just a special case: a true statement has a probability of 1, a false statement has probability of 0. We would assign a probability of 1 to the statement, There is Martian life, only after having conclusive data indicating something is growing, and reproducing, and doing other activities we associate with living organisms. Notice, however, that assigning a probability of 0 is harder because we can always think that there is some Martian spot that is unexplored, or that we have made mistakes with some experiment, or several other reasons that could lead us to falsely believe life is absent on Mars even when it is not. Related to this point is Cromwell’s rule, stating that we should reserve the use of the prior probabilities of 0 or 1 to logically true or false statements. Interestingly enough, Richard Cox mathematically proved that if we want to extend logic to include uncertainty, we must use probabilities and probability theory. Bayes’ theorem is just a logical consequence of the rules of probability, as we will see soon. Hence, another way of thinking about Bayesian statistics is as an extension of logic when dealing with uncertainty, something that clearly has nothing to do with subjective reasoning in the pejorative sense—people often used the term subjective. To summarize, using probability to model uncertainty is not necessarily related to the debate about whether nature is deterministic or random at is most fundamental level, nor is related to subjective personal beliefs. Instead, it is a purely methodological approach to model uncertainty. We recognize most phenomena are difficult to grasp because we generally have to deal with incomplete and/or noisy data, we are intrinsically limited by our evolution-sculpted primate brain, or any other sound reason you could add. As a consequence, we use a modeling approach that explicitly takes uncertainty into account.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>From a practical point of view, the most relevant piece of information from this section is that Bayesian’s use probabilities as a tool to quantify uncertainty.</p>
</div>
</div>


<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography" style="display: none">
<div id="ref-mckinney_2022" class="csl-entry" role="doc-biblioentry">
McKinney, Wes. 2022. <em>Python for <span>Data</span> <span>Analysis</span>: <span>Data</span> <span>Wrangling</span> with Pandas, <span>NumPy</span>, and <span>Jupyter</span></em>. Beijing Boston Farnham Sebastopol Tokyo.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./index.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Preface</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./references.html" class="pagination-link">
        <span class="nav-page-text">References</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>