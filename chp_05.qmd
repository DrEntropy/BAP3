# Comparing Models {#sec-model_comparison}

> "A map is not the territory it represents, but, if correct, it has a similar structure to the territory." -Alfred Korzybski

Models should be designed as approximations to help us understand a particular problem,
or a class of related problems. Models are not designed to be verbatim copies of the real
world. Thus, all models are wrong in the same sense that maps are not the territory. Even
when a priori, we consider every model to be wrong, not every model is equally wrong;
some models will be better than others at describing a given problem. In the foregoing
chapters, we focused our attention on the inference problem, that is, how to learn values of
parameters from the data. In this chapter, we are going to focus on a complementary
problem: how to compare two or more models that are used to explain the same data. As
we will learn, this is not a simple problem to solve and at the same time is a central problem
in data analysis.

In this chapter, we will explore the following topics:

* Overfitting and underfitting
* Cross-validation
* Bayes factors
* Regularizing priors


## Posterior predictive tests 

We have previously introduced and discussed posterior predictive tests as a way to assess how well models explain the same data used to fit a model. The purpose of this type of testing is not to determine that a model is incorrect; We already know this! The goal of the exercise is to understand how well we are capturing the data. We often capture different aspects of data in different ways. By performing posterior predictive tests, we aim to better understand the limitations of a model. Once we undestand the limitations we can simply acknoledgment them or try to remove them by improve the model. It is expected that a model will not be able to reproduce all aspects of a problem and this is usually not a problem as models are built with a purpose in mind. Posterior predictive tests are ne way to evaluate that purpose, so if we have more than one model, we can compare them using posteriori predictive tests.


Let's look at a simple example. We have a data set with two variables `x` and `y`. We are going to fit these data with a linear model:

$$
y = \alpha + \beta x
$$

And a quadratic model, that is a model with one more term than the linear model. For this extra term we just take `x` to the power of 2 and add a $\beta$ coefficient, that is:
$$
y = \alpha + \beta_0 x + \beta_1 x^2
$$

We can write in PyMC this models as usual, see the next block of code. The only difference is that we pass the argument `idata_kwargs={"log_likelihood": True}` to `pm.sample`. This will store the log likelihood in the `InferenceData` object, we will use this later.

```python
with pm.Model() as model_l:
    α = pm.Normal('α', mu=0, sigma=1)
    β = pm.Normal('β', mu=0, sigma=10)
    σ = pm.HalfNormal('σ', 5)

    μ = α + β * x_c[0]
    
    y_pred = pm.Normal('y_pred', mu=μ, sigma=σ, observed=y_c)

    idata_l = pm.sample(2000, idata_kwargs={"log_likelihood": True})
    idata_l.extend(pm.sample_posterior_predictive(idata_l))

with pm.Model() as model_q:
    α = pm.Normal('α', mu=0, sigma=1)
    β = pm.Normal('β', mu=0, sigma=10, shape=order)
    σ = pm.HalfNormal('σ', 5)

    μ = α + pm.math.dot(β, x_c)
    
    y_pred = pm.Normal('y_pred', mu=μ, sigma=σ, observed=y_c)

    idata_q = pm.sample(2000, idata_kwargs={"log_likelihood": True})
    idata_q.extend(pm.sample_posterior_predictive(idata_q))
```

Figure @fig-lin-pol-fit shows the mean fit from both models. 
    
![Mean fit for `model_l` (linear) and `model_q`](/fig/lin-pol-fit.png){#fig-lin-pol-fit}
    
Visually, both models seems to provide a resonable fit to the data. To gain further insight we can do a posterior predictive check, Figure @fig-lin-pol-ppc shows KDEs for the observed and predicted data. Here it becomes easier to see that `model_q`, the quadratic model, provides a better fit to the data. We can also see there is a lot of uncertainty, in particular at the tails of the distributions, this is because we have a small number of data points.

![Posterior predictive checks for `model_l` and `model_q` created with `az.plot_ppc` function](/fig/lin-pol-ppc.png){#fig-lin-pol-ppc}

Posterior predictive checks are a very verstail idea. We can compare observed and predicted data in so many ways. Instead of comparing the densities of the distributions we can compare summary statistics. 


For instance on the left panel of Figure @fig-lin-pol-bpv we have the distributions of means for both models. The dot over the x-axis indicates the observed value. We can see that both model captures the mean very well, with the quadratic model having less variance. That both models capture the mean very well is not surprising as we are explicitelly modeling the mean with both models. On the right panel we have the distributions of interquartile range, this comparisons favor the linear model instead.

![Posterior predictive checks for `model_l` and `model_q` created with `az.plot_bpv` function](/fig/lin-pol-bpv.png){#fig-lin-pol-bpv}

In general, a statistic that is _orthogonal_ to what the model is explicitelly modeling will be more informative for evaluating the model. When in doubt, it may be convenient to evaluate more than one statistic. A useful question is to ask yourself what aspects of the data you are interested in capturing.

To generate Figure @fig-lin-pol-bpv we used the ArviZ function `az.plot_bpv`. An extract of the full code to generate that figure is the following:

```python
idatas = [idata_l, idata_q]

def iqr(x, a=-1):
    """interquartile range"""
    return np.subtract(*np.percentile(x, [75, 25], axis=a))

for idata in idatas:
    az.plot_bpv(idata, kind="t_stat", t_stat="mean", ax=axes[0])
    

for idata in idatas:
    az.plot_bpv(idata, kind="t_stat", t_stat=iqr, ax=axes[1])
```

Notice the argument `kind="t_stat"` to indicate we are going to use a summary statistics and the use of `t_stat="mean"`, i.e. a string to indicate that we want to use the mean as the summary statistic. Or we can use a user defined function, as in `t_stat=iqr` to indicate that we want to use the interquartile range as the summary statistic.

You may have noticed that Figure @fig-lin-pol-bpv also includes some values called `bpv`, for Bayesian p-value. The bpv is a numerical way of summarizing a comparison between simulated data and actual data. To obtain them, a summary statistic $T$ is chosen, such as the mean, median, standard deviation, whatever you may think worth of comparison. Then $T$ is calculated for the observed data $T_{\text{obs}}$ and for the simulated data $T_{\text{sim}}$. Finally we ask ourselves the question:

What is the probability that $T_{\text{sim}}$ is less than or equal to $T_{\text{obs}}$? If the observed values agree with the predicted ones, the expected value will be 0.5. In other words, half of the predictions will be below the observations and half above. This quantity is known as the _Bayesian p-value_.

$$\text{Bayesian p-value} \triangleq p(T_{\text{sim}} \le T_{\text{obs}} \mid \tilde Y)$$

For those who are familiar with _p-values_ and their use in frequentist statistics, there are a couple of clarifications. What is **Bayesian** about these _p-values_ is that we are NOT using a sampling distribution but _the posterior predictive distribution_. And an important difference with the frequentist version is that in this case we are not doing a null hypothesis test, nor trying to declare that a difference is "significant". We are simply trying to quantify how well the model explains the data.

There is yet another way to compute a Bayesian p-value. Instead of using a summary statistic, we can use the entire distribution. In this case, we can ask ourselves the question: What is the probability of predicting a lower or equal value for **each observed value**. If the model is well calibrated these probability should be the same for all observed values, the model is capturing all observations equally well. Thus we should expect a uniform distribution. Again ArviZ can help us with this with the function `az.plot_bpv` with the argument `kind="p_value"` (which is the default). Figure @fig-lin-pol-bpv2 shows the results of this calculation. The white line indicates the expected uniform distribution and the gray band shows the expected deviation given the finite size of the sample. It can be seen that both models are very similar.

![Posterior predictive checks for `model_l` and `model_q` created with `az.plot_bpv` function](/fig/lin-pol-bpv2.png){#fig-lin-pol-bpv2}


Predictive _post_testing_ provides a very flexible framework for evaluating and comparing models, either using graphs or numerical summaries such as _Bayesian p-values_, or even a combination of both. The concept is general enough to allow an analyst to use her imagination to find different ways to explore the predictive distribution _posteriori_ and use the best fit for the purposes of interpreting the data and models.

In the following sections we will explore other methods for comparing models. These methods are not mutually exclusive, they can be used in combination with posterior predictive tests. 

## The balance between simplicity and accuracy

When choosing between alternative explanations, there is a principle known as Occam's razor. In very general lines, this principle establishes that given two or more equivalent explanations for the same phenomenon, the simplest is the preferred explanation. A common criterion of simplicity is the number of parameters in a model.

There are many justifications for this heuristic, we are not going to discuss any of them, we are just going to accept it as a reasonable guide.

Another factor that we generally have to take into account when comparing models is their accuracy, that is, how good a model is at fitting the data. According to this criterion, if we have two (or more) models and one of them explains the data better than the other, then that is the preferred model.

Intuitively, it seems that when comparing models, we tend to prefer those that best fit the data and those that are simpler. But what to do if these two principles conflict? Or more generally, is there a quantitative way to consider both contributions? The short answer is yes, in fact there is more than one way to do it. But first let's see an example in order to generate more intuition.

### Many parameters (can) lead to overfitting

Figure @fig-over_under_fit shows 3 models with increazing number of parameters. The first one (order 0) is just a constant value, whatwer the value of `x`, the model always predicts the same values for `y`, the second model (order 1) is a linear model as we saw in the previous chapter and the last one (order 5) is a polynomial model of order 5, that is something of the form $\alpha + \beta_0 x + \beta_0 x^2 + \beta_0 x^3 + \beta_0 x^4 + \beta_0 x^5$. 


![3 simple models and a simple dataset](/fig/over_under_fit.png){#fig-over_under_fit}

From the Figure @fig-over_under_fit we can see that the increase in the complexity of the model (number of parameters) is accompanied by a greater accuracy reflected in the coefficient of determination R², this is a way to measure the fit of a model (for more information please read <https://en.wikipedia.org/wiki/Coefficient_of_determination>). In fact, we can see that the polynomial of order 5 fits the data perfectly, obtaining R²=1.

Why the polynomial of order 5 can capture the data without losing a single one of them? The reason is that we have the same number of parameters as data, that is, 6. Therefore, the model is simply acting as an alternative way of expressing the data. The model is not learning something about the data, it is memorizing the data! This can be problematic, the easier way to notice this is thinking what will happend to a model that memory data, when preseted with new, unobserved data. The performance is expected to be bad, as someone that just memorizes the questions for an exam only to find the questions are different in the actual exam! This is represented in Figure @fig-over_under_fit2, here we have added two new datapoints, maybe we got the money to perform a new experiment or our boss just send us new data. We can see that the model of order 5, that was able to exacltly fit the data, now has actually a worst performance (as measured by R²) than the linear model. From this simple example, we can see that a model with the best fit is not always the ideal one.


![3 simple models and a simple dataset, plus a couple of new points](/fig/over_under_fit2.png){#fig-over_under_fit2}

Looselly speacking, when a model fits the data set used to learn the parameters of that model very well, but new data sets very poorly, we say we have overfitting. This is a very common problem when analyzing data. A very useful way to think about overfitting is to consider a data set as having two components; the signal and the noise. The signal is what we want to capture (or learn) from the data. If we use a data set it is because we believe there is a signal there, otherwise it will be an exercise in futility. Noise, on the other hand, is not useful and is the product of measurement errors, limitations in the way the data was generated or captured, the presence of corrupted data, etc. A model overfits when it is so flexible (for a data set) that it is capable of _learning_ noise. This has the consequence that the signal is hidden.

This is a practical justification for Occam's razor. And he warns us that at least in principle, it is always possible to create a model so complex that it explains all the details, even the most irrelevant ones. As in the Empire described by Borges, where the cartographers reached such a level of sophistication that they created a map of the Empire whose size was the size of the Empire itself, and which coincided point for point with it.

### Too few parameters lead to underfitting

Continuing with the same example but at the other extreme of complexity, we have the model of order 0. This model is simply a Gaussian disguised as a linear model. This model is only capable of capturing the value of the mean of $Y$, and is therefore totally indifferent to the values of $x$. We say that this model has underfitted the data.

## Measures of predictive accuracy