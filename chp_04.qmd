# Modeling with Lines {#sec-linear}

> "In more than three centuries of science everything has changed except perhaps one thing: the love for the simple." - Jorge Wagensberg

Music---from classical compositions to *Sheena is a Punk Rocker* by The Ramones, passing through the unrecognized *hit* from a garage band and
Piazzolla\'s Libertango---is made from recurring patterns. The same scales, combinations of chords, riffs, motifs, and so on appear over and over again, giving rise to a wonderful sonic landscape capable of eliciting and modulating the entire range of emotions humans can experience. Similarly, the universe of statistics is built upon recurring patterns, small motifs that appear now and again. In this chapter, we are going to look at one of the most popular and useful of them, the **linear model** (or motif, if you want). This is a very useful model on its own and also the building block of many other models. If you ever took a statistics course, you may have heard of simple and multiple linear regression, logistic regression, ANOVA, ANCOVA, and so on. All these methods are variations of the same underlying motif, the linear regression model. In this chapter, we will cover the following topics:

* Simple linear regression
* NegativeBinomial regression
* Robust regression
* Logistic regression
* Variable variance
* Hierarchical linear regression
* Multiple linear regression


## Simple linear regression

Many problems we find in science, engineering, and business are of the following form. We have a variable $X$ and we want to model or predict a variable $Y$. Importantly, these variables are paired like $\{(x_1, y_1), (x_2, y_2), \cdots, (x_n, y_n)\}$. In the most simple scenario, known as simple linear regression, both $X$ and $Y$ are uni-dimensional continuous random variables. By continuous, we mean a variable represented using real numbers. Using NumPy, you will represent these variables as one-dimensional arrays of floats. Usually, people call $Y$ the dependent, predicted, or outcome variable, and $X$ the independent, predictor, or input variable.

Some typical situations where linear regression models can be used are: 

* Model the relationship between soil salinity and crop productivity. Then, answer questions such as: is the relationship linear? How strong is this relationship?

* Find a relationship between average chocolate consumption by country and the number of Nobel laureates in that country, and then understand why this relationship could be spurious.

* Predict the gas bill (used for heating and cooking) of your house by using the solar radiation from the local weather report. How accurate is this prediction?

## The core of the linear regression models

In @sec-programming_probabilistically we saw the normal model, which we define as:

$$
\begin{aligned}
\mu &\sim \text{some prior} \\
\sigma &\sim \text{some other prior} \\
Y &\sim \mathcal{N}(\mu, \sigma)
\end{aligned}
$$

The main idea from linear regression is to extend this model, by adding a predictor variable $X$ to the estimation of the mean $\mu$:

$$
\begin{aligned}
\alpha &\sim \text{a prior} \\
\beta &\sim \text{another prior} \\
\sigma &\sim \text{some other prior} \\
\mu &= \alpha + \beta X \\
Y &\sim \mathcal{N}(\mu, \sigma)
\end{aligned}
$$

This model says that there is a linear relation between the variable $X$ and the variable $Y$. But that relationship is not deterministic, because of the noise term $\sigma$. Additionally, the model says that the mean of $Y$ is a linear function of $X$, with **intercept** $\alpha$ and **slope** $\beta$. The intercept tells us the value of $Y$ when $X=0$, the slope tells us the change in $Y$ per unit change in $X$. Because we don't know the values of $\alpha$, $\beta$, or $\sigma$ we set priors distribution over them.

A common assumption when setting priors for linear models is to assume that they are independent. This assumption greatly simplifies setting priors because we then need to set three priors instead of one joint prior. At least in principle $\alpha$ and $\beta$ can take any value on the real line, thus it is common to use normal priors for them. And because $\sigma$ is a positive number, it is common to use a half-normal or exponential prior for it.

The values the intercept can take can vary a lot from one problem to another and for different domain knowledge. For many problems I have worked on, $\alpha$ is usually centered around zero and with a standard deviation no larger than 1, but this is just my experience (almost anecdotal) with a small subset of problems and not something easy to transfer to other problems. Usually, it may be easier to have an informed guess for the slope ($\beta$). For instance, we may know the sign of the slope a priori; for example, we expect the variable weight to increase, on average, with the variable height. For $\sigma$, we can set it to a large value on the scale of the variable $Y$, for example, 2 times the value for its standard deviation. We should be careful of using the observed data to guesstimate priors, usually, it is fine if the data is used to avoid using very restrictive priors. If we don't have too much knowledge of the parameter, makes sense to ensure our prior is vague. If we instead want more informative priors, then we should not get that information from the observed data, instead, we should get it from our domain knowledge.


::: callout-note
A linear regression model is an extension of the Normal model where the mean is computed as a linear function of a predictor variable 
:::


## Linear bikes

We now have a general idea of what Bayesian linear models looks like. Let's try to cement that idea with an example. We are going to start very simply, we have a record of temperatures and the number of bikes rented in a city. We want to model the relationship between the temperature and the number of bikes rented. We will use the data from the [bike-sharing dataset](https://archive.ics.uci.edu/ml/datasets/bike+sharing+dataset) from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php). The original dataset contains 17379 records, and each record has 17 variables, but we will have a smaller version with only 359 records and we are going to only use two variables `temperature` and `rented`. The variable `temperature` is the temperature in Celsius and `rented` is the number of bikes rented. @fig-bike_temperature shows a scatter plot of these two variables. 

![Bike sharing dataset. Scatter plot of temperature in Celcius vs number of rented bikes](fig/bike_temperature.png){#fig-bike_temperature width=80%}

Let's build a Bayesian linear model for this data. The temperature is going to be our independent variable (our "X") and the number of bikes rented is going to be our dependent variable (our "Y"). We are going to use the following model:

```python
with pm.Model() as model_lb:
    α = pm.Normal("α", mu=0, sigma=100)
    β = pm.Normal("β", mu=0, sigma=10)
    σ = pm.HalfCauchy("σ", 10)
    μ = pm.Deterministic("μ", α + β * bikes.temperature)
    y_pred = pm.Normal("y_pred", mu=μ, sigma=σ, observed=bikes.rented)
    idata_lb = pm.sample()
```

Take a moment to read the code line by line and be sure to understand what is going on, also check @fig-linear_bikes_dag for a visual representation of this model. As we have previously said this is like a normal model, but now the mean is modeled as a linear function of the temperature. The intercept is $\alpha$ and the slope is $\beta$. The noise term is $\epsilon$ and the mean is $\mu$. The only new thing here is the `Deterministic` variable $μ$, this variable is not a random variable, it is a deterministic variable, and it is computed from the intercept, the slope and the temperature. We need to specify this variable because we want to save it in the InferenceData for later use. We could have just written `μ = α + β * bikes.temperature` or even `_ = pm.Normal('y_pred', mu=α + β * bikes.temperature, sigma=ϵ, observed=bikes.rented)` and the model will be the same, but we would not have been able to save $μ$ in the InferenceData.  Notice that $μ$ is a vector with the same length as` bikes.temperature`, which is the same as the number of records in the dataset.


![Bayesian linear model for the bike sharing dataset](fig/linear_bikes_dag.png){#fig-linear_bikes_dag width=50%}

### Interpreting the posterior mean

To explore the results of our inference, we are going to generate a posterior plot, but omit the deterministic variable `μ`. We commit it because otherwise, we would get a lot of plots, one for each value of `temperature`. We can do this by passing the names of the variables we want to include in the plot as a list to the `var_names` argument or we can negate the variable that we want to exclude as in the following block of code.

```python
az.plot_posterior(idata_lb, var_names=['~μ'])
```

![Posterior plot for the bike linear model](fig/linear_bikes_posterior.png){#fig-linear_bikes_posterior}

From @fig-linear_bikes_posterior we can see the marginal posterior distribution for $\alpha$, $\beta$ and $\sigma$, if we only read the means of each distribution say that $\mu = 69 + 7.9 X$, with this information we can say that the expected value of rented bikes when the temperature is 0 is 69 and for each degree of temperature the number of rented bikes increases by 7.9. So for a temperature of 28 degrees, we expect to rent $69 + 7.9 * 28 \approx 278$ bikes. This is our expectation, but the posterior also informs us about the uncertainty around this estimate. For instance, the 94% HDI for $\beta$ is (6.1, 9.7), so for each degree of temperature the number of rented bikes could increase from 6 to about 10. Also even if we omit the posterior uncertainty and we only pay attention to the means, we still have uncertainty about the number of rented bikes, because we have a value of $\sigma$ of 170. So if we say that for a temperature of 28 degrees, we expect to rent 278 bikes, we should not be surprised if the actual number turns out to be somewhere between 100 and 500 bikes.

Now let's create a few plots that will help us to visualize the combined uncertainty of these parameters. Let's start with two plots for the mean (see  @fig-linear_bikes_posterior_lines). Both are plots of the mean number of rented bikes as a function of the temperature. The difference is how we represent the uncertainty. We show two popular ways of doing it. For the left subpanel, we take 50 samples from the posterior and plot them as individual lines, for the right subpanel we, instead, take all the available posterior samples for $μ$ and use it to compute the 94% HDI.

![Posterior plot for the bike linear model](fig/linear_bikes_posterior_lines.png){#fig-linear_bikes_posterior_lines width=80%}

The plots in @fig-linear_bikes_posterior_lines convey essentially the same information, but one represents uncertainty as a set of lines and the other as a shaded area. Notice that if you repeat the code to generate the plot, you will get different lines, because we are sampling from the posterior. The shaded area, however, will be the same, because we are using all the available posterior samples. If we go further and refit the model, we will not only get different lines but the shaded area could also change, but probably the difference between runs is going to be very small, if not probably need to increase the number of draws, or there is something funny about your model and sampling (see @sec-inference_engines for guidance).

Anyway, why are we showing two slightly different plots if they convey the same information? Well, to highlight that there are different ways to represent uncertainty. Which one is better? As usual, that is context-dependent, the shaded area is a good option, it is very common, and it is simple to compute and interpret. Unless we have reasons to show individual posterior samples that may be your choice. But we may want to show individual posterior samples, for instance, most of the lines might span a certain region but we get a few with very high slopes. A shaded area could hide that information. When showing individual samples from the posterior it may be a good idea to animate them if you are showing them in a presentation or a video (see @kale_2018 for more about this).

Another reason to show it is that you can see different ways to extract information from the posterior. Omitting the actual code for plotting the computation we performed for @fig-linear_bikes_posterior_lines as described in the next block of code.

```python
posterior = az.extract(idata_lb, num_samples=50)
x_plot = xr.DataArray(
    np.linspace(bikes.temperature.min(), bikes.temperature.max(), 50), dims="plot_id"
)
mean_line = posterior["α"].mean() + posterior["β"].mean() * x_plot
lines = posterior["α"] + posterior["β"] * x_plot
hdi_lines = az.hdi(idata_lb.posterior["μ"])
...
```

You can see that in the first line, we used `az.extract` takes the `chain` and `draw` dimension and stack them in a single `sample` dimension, which can be useful for later processing. Additionally, we use the `num_samples` argument to ask for a subsample from the posterior. By default `az.extract` will operate on the posterior group, if you want to extract information from another group, you can use the `group` argument. On the second line, we define a DataArray `x_plot`, with equally spaced values ranging from the minimum to the maximum observed temperatures. The reason to create a DataArray is to be able to use Xarray automatic alignment capabilities in the next two lines, if we use a NumPY array we will need to add extra dimensions, which is usually confusing, the best way to fully understand what I mean is to define `x_plot = np.linspace(bikes.temperature.min(), bikes.temperature.max())` and try to redo the plot. In the third line of code, we compute the mean of the posterior for $\mu$ for each value of `x_plot` and in the fourth line, we compute individual values for $\mu$. For these two lines, we could have used `posterior['μ']`, but instead we explicitly rewrite the linear model, we do this with the hope that it will help you to gain more intuition about linear models.

### Interpreting the posterior predictions

What if we are not just interested in the expected (mean) value, but we want to think in terms of predictions, that is in terms of rented bikes? Well for that we can do posterior predictive sampling. After executing the next line of code `idata_lb` will be populated with a new group `posterior_predictive` with a variable `y_pred` representing the posterior predictive distribution for the number of rented bikes.

```python
pm.sample_posterior_predictive(idata_lb, model=model_lb, extend_inferencedata=True)
```

The black line in @fig-linear_bikes_posterior_predictive is the mean of the number of rented bikes, this is the same as in @fig-linear_bikes_posterior_lines. The new elements are the dark gray band, representing the central 50% (quantiles 0.25 and 0.5) for the rented bikes and the light gray band, representing the central 94% (quantiles 0.03 and 0.97). You may notice that our model is predicting a negative number of bikes, which does not make sense. But upon reflection, this should be expected as we use a Normal distribution for the likelihood in `model_lb`. A very dirty *fix* could be to clip the predictions at values lower than zero, but that's ugly. In the next section, we will see that we can easily improve this model to avoid non-sensical predictions.

![Posterior predictive plot for the bike linear model](fig/linear_bikes_posterior_predictive.png){#fig-linear_bikes_posterior_predictive width=80%}

## Generalizing the linear model

The linear model we have been using is a special case of a more general model, the generalized linear model (GLM). The GLM is a generalization of the linear model that allows us to use different distributions for the likelihood. At a high level, we can write a Bayesian GLM like:

$$
\begin{aligned}
\alpha &\sim \text{a prior} \\
\beta &\sim \text{another prior} \\
\theta &\sim \text{some prior} \\
\mu &= \alpha + \beta X \\
Y &\sim \phi(f(\mu), \theta)
\end{aligned}
$$

where $\phi$ is an arbitrary distribution some common cases are Normal, Student's T, Gamma, NegativeBinomial, etc. $\theta$ represent any *auxiliary* or *noise* parameter the distribution may have, like $\sigma$ for the Normal. We also have $f$, usually called the inverse link function. When $\phi$ is Normal, then $f$ is the identity function. For distributions like Gamma and NegativeBinomial $f$ is usually the exponential function. Why do we need $f$? Because $\mu$ will generally be on the real line, the $\mu$ parameter (or its equivalent) may be defined on a different domain. For instance, the parameter $\mu$ of the NegativeBinomial is defined for positive values, so we need to transform $\mu$ to the positive integers. The exponential function is a good candidate for this transformation. We are going to explore a few GLMs in this book, a good exercise for you is to create a table and every time we see a new one, you add one line indicating what is $phi$, $theta$, $f$ and maybe some notes about when this GLM is used. OK, let's start with our first concrete example of a GLM.


## Counting bikes

How we can change `model_lb` to better accommodate the bike data? There are two things to note, the number of rented bikes is discrete and it is bounded at zero. This is usually known as count data. That is data that is the result of counting something. Count data is sometimes modeled using a continuous distribution like a Normal, especially when the number of counts is large. But it is often a good idea to use a discrete distribution. Two common choices are the Poisson distribution and the NegativeBinomial. The main difference is that for the Poisson the mean and the variance are the same, but this is not true or even approximately true, then the NegativeBinomial may be a better choice as it allows the mean and variance to be different. When in doubt you can fit a Poisson and a NegativeBinomial and see which one provides a better model, we are going to do that on @sec-model_comparison. But for now, we are going to use the NegativeBinomial.

The PyMC model is very similar as before but with two main differences. First, we use `pm.NegativeBinomial` instead of `pm.Normal` for the likelihood. The NegativeBinomial distribution has two parameters, the mean $\mu$ and a dispersion parameter $\alpha$. The variance of the NegativeBinomial is $\mu + \frac{\mu^2}{\alpha}$. So the larger the value of $\alpha$ the larger the variance. The second difference is that $\mu$ is `pm.math.exp(α + β * bikes.temperature)` instead of just `α + β * bikes.temperature`, as we already explained this is needed to transform the real line into the positive interval.

```python 
with pm.Model() as model_neg:
    α = pm.Normal("α", mu=0, sigma=1)
    β = pm.Normal("β", mu=0, sigma=10)
    σ = pm.HalfNormal("σ", 10)
    μ = pm.Deterministic("μ", pm.math.exp(α + β * bikes.temperature))
    y_pred = pm.NegativeBinomial("y_pred", mu=μ, alpha=σ, observed=bikes.rented)
    idata_neg = pm.sample()
    idata_neg.extend(pm.sample_posterior_predictive(idata_neg))
```

The posterior predictive distribution for ` model_neg` is shown in @fig-linear_bikes_posterior_predictive_nb. The posterior predictive distribution is also very similar to the one we obtained with the linear model (@fig-linear_bikes_posterior_predictive). The main difference is that now we are not predicting a negative number of rented bikes! We can also see that the variance of the predictions increases with the mean. This is expected as the variance of the NegativeBinomial is $\mu + \frac{\mu^2}{\alpha}$.

![Posterior predictive plot for the bike NegativeBinomial linear model](fig/linear_bikes_posterior_predictive_nb.png){#fig-linear_bikes_posterior_predictive_nb width=80%}

@fig-linear_bikes_posterior_predictive_check shows the posterior predictive check for `model_lb` on the left and `model_neg` on the right. We can see that when using a Normal, the largest mismatch is that the model predicts a negative number of rented bikes, but even on the positive side we see that the fit is not that good. On the other hand, the NegativeBinomial model seems to be a better fit, although not perfect, see the right tail is heavier for the predictions than observations, but also notice that the probability of this very high demand is low. So overall we restate that the NegativeBinomial model is better than the Normal one. 

![Posterior predictive check for the bike linear model](fig/linear_bikes_posterior_predictive_check.png){#fig-linear_bikes_posterior_predictive_check}

## Robust regression

I was once running a complex simulation of a molecular system. At each step of the simulation, I need it to fit a linear regression as an intermediate step. We have theoretical and empirical reasons to think that our "Y" was conditionally Normal given our "Xs", so simple linear regression should do the trick. But from time to time the simulation generated a few values of "Y" way above or below the bulk of the data, this completely ruins our simulation and we had to restart it. Usually, these values very different from the bulk of the data are called outliers. The reason for the failure of our simulations was that the outliers were *pulling* the regression line away from the bulk of the data and when we pass this estimate to the next step in the simulation, the thing just halt. We solve this with the help of our good friend the Student's T distribution, which as we saw in @sec-programming_probabilistically, has heavier tails than the Normal distribution. This means that the outliers have less influence on the regression line. This is an example of a robust regression.

To exemplify the robustness that a Student's t-distribution brings to linear regression, we are going to use a very simple and nice dataset: the third data group from the Anscombe quartet. If you do not know what the Anscombe Quartet is, remember to check it out later at
Wikipedia (<https://en.wikipedia.org/wiki/Anscombe%27s_quartet>). 


In the following model ` model_t`, we are using a shifted exponential to avoid values close to zero. The non-shifted exponential puts too much weight on values close to zero. In my experience, this is fine for data with none to moderate outliers, but for data with extreme outliers (or data with a few bulk points), like in Anscombe's third dataset, it is better to avoid such low values. Take this, as well as other priors recommendations, with a pinch of salt. The defaults are good starting points, but there's no need to stick to them. Other common priors for are gamma(2, 0.1) or gamma(mu=20, sd=15):


```python
with pm.Model() as model_t:
    α = pm.Normal("α", mu=ans.y.mean(), sigma=1)
    β = pm.Normal("β", mu=0, sigma=1)
    σ = pm.HalfNormal("σ", 5)
    ν_ = pm.Exponential("ν_", 1 / 29)
    ν = pm.Deterministic("ν", ν_ + 1)
    μ = pm.Deterministic("μ", α + β * ans.x)
    _ = pm.StudentT("y_pred", mu=μ, sigma=σ, nu=ν, observed=ans.y)

    idata_t = pm.sample(2000)
```

In @fig-linear_robust_regression we can see the robust fit, according to `model_t`, and the non-robust fit, according to SciPy's `linregress` (this function is doing least-squares regression).

![Robust regression](fig/linear_robust_regression.png){#fig-linear_robust_regression width="80%"}

While the non-robust fit tries to *compromise* and include all points, the robust Bayesian
model, `model_t`, automatically *discards* one point and fits a line that passes closer through all the remaining points. I know this is a very peculiar dataset, but the message remains the same for other datasets; a Student's t-distribution, due to its heavier tails, gives less importance to points that are far away from the bulk of the data.


From @fig-linear_robust_regression_ppc we can see that for the bulk of the data, we get a very good match. Also, notice that our model predicts
values away from the bulk to both sides and not just above the bulk (as in the observed data). For our current purposes, this model is performing just fine and it does not need further changes. Nevertheless, notice that for some problems, we may want to avoid this. In such a case, we should probably go back and change the model to restrict the possible values of y_pred to positive values, using a truncated Student's t-distribution. This is left as an exercise for the reader.

![Posterior predictive check for `model_t`](fig/linear_robust_regression_ppc.png){#fig-linear_robust_regression_ppc width="80%"}



## Logistic regression

The logistic regression model is a generalization of the linear regression model, that we can use when the response variable is binary. This model uses the logistic function as an inverse link function. Let's get familiar with this function before we move on to the model.

$$
\text{logistic}(z) = \frac{1}{1 + e^{-z}}
$$

For our purpose, the key property of the logistic function is that irrespective of the values of its argument $z$ the result will always be a number in the [0-1] interval. Thus, we can see this function as a convenient way to compress the values computed from a linear model into values that we can feed into a Bernoulli distribution. This logistic function is also known as the sigmoid function, because of its characteristic S-shaped aspect, as we can see from @fig-logistic


![Logistic function](fig/logistic.png){#fig-logistic width="80%"}

### The logistic model

We have almost all the elements to turn a simple linear regression into a simple logistic regression. Let's begin with the case of only two classes, for example, ham/spam, safe/unsafe, cloudy/sunny, healthy/ill, or hotdog/not hotdog. First, we codify these classes by saying that the predicted variable $y$ can only take two values, 0 or 1, that
is $y \in \{0, 1\}$.

Stated this way, the problem sounds very similar to the coin-flipping we used in previous chapters. We may remember we used the Bernoulli distribution as the likelihood. The difference with the coin-flipping problem is that now $\theta$ is not going to be generated from a beta distribution; instead, $\theta$ is going to be defined by a linear model with the logistic as the inverse link function. Omitting the priors we have:

$$
\begin{aligned}
\theta &= \text{logistic}(\alpha + \beta x) \\
y &\sim \text{Bernoulli}(\theta)
\end{aligned}
$$

We are going to apply logistic regression to the iris dataset. This is a classic dataset containing information about flowers from three closely related species: setosa, virginica, and versicolor. We have taken measurements from 50 individuals of each species. These measurements are the petal length, petal width, sepal length, and sepal width. In case you are wondering, sepals are modified leaves whose function is generally related to protecting the flowers in a bud.

We are going, to begin with a simple case. Let's assume we only have two classes setosa and versicolor, and just one independent variable or feature, the sepal_length. We want to predict the probability of a flower being setosa given its sepal length. 


As it is usually done, we are going to encode the setosa and versicolor categorical variables with the numbers 0 and 1. Using pandas, we can do the following:

```python
df = iris.query("species == ('setosa', 'versicolor')")
y_0 = pd.Categorical(df["species"]).codes
x_n = "sepal_length"
x_0 = df[x_n].values
x_c = x_0 - x_0.mean()
```

As with other linear models, centering the data can help with the sampling. Now that we have the data in the proper format, we can finally build the model with PyMC.

```python
with pm.Model() as model_lrs:
    α = pm.Normal("α", mu=0, sigma=1)
    β = pm.Normal("β", mu=0, sigma=5)
    μ = α + x_c * β
    θ = pm.Deterministic("θ", pm.math.sigmoid(μ))
    bd = pm.Deterministic("bd", -α / β)
    yl = pm.Bernoulli("yl", p=θ, observed=y_0)

    idata_lrs = pm.sample()
```

`model_lrs` has two deterministic variables: `θ` and `bd`. `θ` is the output of the logistic function applied to the `μ` variable, and `bd` is the boundary decision, which is the value used to separate classes; we will discuss this later in detail. Another point worth mentioning is that instead of writing the logistic function ourselves, we are using the one provided by PyMC, `pm.math.sigmoid`.

@fig-logistic_regression shows the result of ` model_lrs`, we have the sepal length versus the probability of being versicolor $\theta$ (and if you want also the probability of being setosa $1-\theta$). We have added some jitter (noise) to the binary response so the point does not overlap. An S-shaped (black) line is the mean value of $\theta$. This line can be interpreted as the probability of a flower being versicolor, given that we know the value of the sepal length. The semitransparent S-shaped band is the 94% HDI. What about the vertical line, that's the topic of the next section.

![Logistic regression, result of `model_lrs`](fig/logistic_regression.png){#fig-logistic_regression width="80%"}

### Classification with logistic regression

My mother prepares a delicious dish called sopa seca, which is basically a spaghetti-based recipe and literally means dry soup. While it may sound like a misnomer or even an
oxymoron, the name of the dish makes total sense when we learn how it is cooked. Something similar happens with logistic regression, a model that, despite its name, is generally framed as a method to solve classification problems. Let's see the source of this duality.

Regression problems are about predicting a continuous value for an output variable given the values of one or more input variables. We have seen many examples of regression that include logistic regression. But logistic regression is sometimes discussed in terms of classification. Understanding by classification the assignment of discrete value (representing a class, like versicolor) to an output variable given some input variables. For instance, stating that a flower is versicolor or setosa given its sepal length. So, is logistic regression a regression or a classification method? The answer is that it is a regression method, we are regression the probability of beloging to some class, but it can be used for classification too. The only thing we need is a decision rule like we assign the class `versicolor` if $\theta \ge 0.5$ and `setosa` otherwise. The vertical line in @fig-logistic_regression is the boundary decision, and it is defined as the value of the independent variable that makes the probability of being versicolor equal to 0.5. In other words, the boundary decision is the value of the independent variable that makes the probability of being versicolor equal to 0.5. For this model we can compute this value analytically and is $-\frac{\alpha}{\beta}$. This follows from the definition of the model:


$$
\theta = \text{logistic}(\alpha + \beta x) 
$$

And from the definition of the logistic function, we have that $\theta = 0.5$ when $\alpha + \beta x = 0$.


$$
0.5 = \text{logistic}(\alpha + \beta x) \Longleftrightarrow 0 = \alpha + \beta x 
$$

Reordering we find that the value of $x$ that makes $\theta = 0.5$ is $-\frac{\alpha}{\beta}$.

Because we have uncertainty in the value of $\alpha$ and $\beta$ we also have uncertainty about the value of the boundary decision. This uncertainty is represented as the vertical (gray) band in @fig-logistic_regression, which goes from $ \approx 5.3$ to $\approx 5.6$. If we were doing automatic classification of flowers based on their sepal length (or any similar problem that could be framed within this model), we could assign setosa to flowers with sepal length below 5.3 and versicolor to flowers with sepal length above 5.6. For flowers with sepal lengths between 5.3 and 5.6, we would be uncertain about their class, so we could either assign them randomly or use some other information to make a decision, including asking a human to check the flower.

To summarise this section 

* The value of $\theta$ is, generally speaking $P(Y=1 \mid X)$. In this sense, the logistic regression is a true regression; the key detail is that we are regressing the
the probability that a data point belongs to class 1, given a linear combination of features.

* We are modeling the mean of a dichotomous variable, that is a number in the [0-1] interval. Thus if we want to use logistic regression for classification, we need to introduce a rule to turn this probability into a two-class assignment. For example if $P(Y=1) > 0.5$ we assign that observation to class 1, otherwise to class 0.

* There is nothing special about the value of 0.5, other than that it is the number in the middle of 0 and 1. This boundary can be justified when we are OK with misclassifying a data point in either direction. But this is not always the case, because the cost associated with the misclassification does not need to be symmetrical. For example, if we are trying to predict whether a patient has a disease or not, we may want to use a boundary that minimizes the number of false negatives (patients that have the disease but we predict they don't) or false positives (patients that don't have the disease but we predict they do). We will discuss this in more detail in the next section.


### Interpreting the coefficients of logistic regression

We must be careful when interpreting the coefficients of logistic regression. Interpretation is not as straightforward as with simple linear models. Using the logistic inverse link function introduces a non-linearity that we have to take into account. If $\beta$ is positive, increasing $x$ will increase $p(y=1)$ by some amount, but the amount is not a linear function of $x$. Instead, the dependency is non-linear on the value of $x$, meaning that the effect of $x$ on $p(y=1)$ depends on the value of $x$. We can visualize this fact from @fig-logistic_regression, instead of a line with a constant slope, we have an S-shaped line with a slope that changes as a function of $x$. 

A little bit of algebra can give us some further insight into how much $p(y=1)$ changes with $x$. The basic logistic model is:

$$
\theta = \text{logistic}(\alpha + \beta x)
$$

The inverse of the logistic is the logit function, which is:

$$
\text{logit}(z) = \log \frac{z}{1-z} 
$$

Thus, if we take the first equation in this section and apply the logit function to both terms, we get:

$$
\text{logit}(\theta) = \alpha + \beta x
$$

Or equivalently:

$$
\log \frac{\theta}{1-\theta} = \alpha + \beta x
$$

Remember that $\theta$ in our model is $p(y=1)$, so we can rewrite the previous expresion as:

$$
\log(\frac{p(y=1)}{1-p(y=1)}) = \alpha + \beta x
$$


The $\frac{p(y=1)}{1-p(y=1)}$ quantity is known as the **odds** of $y=1$. If we call $y=1$ a *success*, then the odds of success is the ratio of the probability of success over the probability of failure. For example, while the probability of getting a 2 by rolling a fair die is $\frac{1}{6}$, the odds of getting a 2 are $\frac{1/6}{5/6} = \frac{1}{5} = 0.2$. In words, one favorable event for every five unfavorable events. Odds are often used by gamblers mainly because odds provide a more intuitive tool than raw probabilities when thinking about the proper way to bet.


::: callout-note
In logistic regression, the $\beta$ coefficient (*slope*) encodes the increase in log-odds units by unit increase of the $x$ variable.
:::

The transformation from probability to odds is a monotonic transformation, meaning the odds increase as the probability increases, and the other way around. While probabilities are restricted to the $[0, 1]$ interval, odds live in the $[0, \infty )$ interval. The logarithm is another monotonic transformation and log-odds are in the $(-\infty, \infty)$ interval. @fig-logistic_odds shows how probabilities are related to odds and log-odds:


![Relationship between probability, odds and log-odds](fig/logistic_odds.png){#fig-logistic_odds width="80%"}


## Variable variance

We have been using the linear motif to model the mean of a distribution and, in the previous section, we used it to model interactions. In statistics, it is said that a linear regression model presents heteroscedasticity when the variance of the errors is not constant in all the observations made. For those cases, we may want to consider the variance (or standard deviation) as a (linear) function of the dependent variable.

The World Health Organization (WHO) and other health institutions around the world collect data for newborns and toddlers and design growth chart standards. These charts
are an essential component of the pediatric toolkit and also a measure of the general well-being of populations to formulate health-related policies, plan interventions, and monitor their effectiveness. An example of such data is the lengths (heights) of newborn/toddlers girls as a function of their age (in months):


```python
data = pd.read_csv("data/babies.csv")
data.plot.scatter("month", "length")
```

To model this data, we are going to introduce three elements we have not seen before:

* $\sigma$ is now a linear function of the predictor variable. To do this, we add two new parameters, $\gamma$ and $\delta$, these are direct analogs of $\alpha$ and $\beta$ in the linear model for the mean. 
* The linear model for the mean is a function of $\sqrt(X)$. This is just a simple trick to fit a linear model to a curve.
* We define a `MutableData` variable, `x_shared`. Why we want to do this will become clear soon:

Our full model is:

```python
with pm.Model() as model_vv:
    x_shared = pm.MutableData("x_shared", data.month.values.astype(float))
    α = pm.Normal("α", sigma=10)
    β = pm.Normal("β", sigma=10)
    γ = pm.HalfNormal("γ", sigma=10)
    δ = pm.HalfNormal("δ", sigma=10)

    μ = pm.Deterministic("μ", α + β * x_shared**0.5)
    σ = pm.Deterministic("σ", γ + δ * x_shared)

    y_pred = pm.Normal("y_pred", mu=μ, sigma=σ, observed=data.length)
    
    idata_vv = pm.sample()
```

On the left panel of @fig-babies_fit we can see the mean of $\mu$ represented by a black curve, and the two semi-transparent grey bands represent 1 and 2 standard deviations. On the right panel, we have the estimated variance as a function of the length. As you can see, the variance increases with the length, which is what we expected.

![Posterior fit for `model_vv` on the left panel. On the right the mean estimated variance as a function of the length.](fig/babies_fit.png){#fig-babies_fit}


Now that we have the model fitted we might want to use the model to find out how the length of a particular girl compares to the distribution. One way to answer this question is to ask the model for the distribution of the variable _length_ for babies of say 0.5 months. We can answer this question by sampling from the posterior predictive distribution conditional on a length of 0.5. Using PyMC we can get the answer by sampling `pm.sample_posterior_predictive`, the only problem is that by default this function will return values of $\tilde y$ for the already observed values of $x$, i.e. the values used to fit the model. The easiest way to get predictions for unobserved values is to define a `MutableData` variable (`x_shared` in the example) and then update the value of this variable right before sampling the predictive distribution _posteriori_. This is what the following code does:

```python
with model_vv:
    pm.set_data({"x_shared": [0.5]})
    ppc = pm.sample_posterior_predictive(idata_vv)
    y_ppc = ppc.posterior_predictive["y_pred"].stack(sample=("chain", "draw"))
```

Now we can plot the expected distribution of lengths for 2-week-old girls and calculate additional amounts, for example, the percentile for a girl of that length:

![Expected distribution of length at 0.5 months, the shaded area represents 32% of the accumulated mass](fig/babies_ppc.png){#fig-babies_ppc width="80%"}


## Hierarchical linear regression

On @sec-hierarchical we learned about the rudiments of hierarchical models. We can apply this concept to linear regression as well. This allows models to deal with inferences at the group level and estimations above the group level. As we already saw, this is done by including hyperpriors. We also show that groups can share information by using a common hyperprior. This is a very powerful concept that allows us to model complex data structures. We are going to see two examples. The first one use a synthetic dataset, and the second one uses the `pigs` dataset.

For the first example, I have created eight related groups, including one group with just one datapoint. On GitHub you will find the full code, for the moment just check how the data looks like:

![Synthetic data for the hierarchical linear regression example](fig/hierarchical_data.png){#fig-hierarchical_data width="80%"}

First, we are going to fit a non-hierarchical model.

```python
coords = {"group": ["A", "B", "C", "D", "E", "F", "G", "H"]}

with pm.Model(coords=coords) as unpooled_model:
    α = pm.Normal("α", mu=0, sigma=10, dims="group")
    β = pm.Normal("β", mu=0, sigma=10, dims="group")
    σ = pm.HalfNormal("σ", 5)
    _ = pm.Normal("y_pred", mu=α[idx] + β[idx] * x_m, sigma=σ, observed=y_m)

    idata_up = pm.sample()
```

@fig-hierarchical_up_forest shows the estimated values for $\alpha$ and $\beta$, as you can see, the estimates for group H, the one consisting of a single datapoint, are very different from the rest. This is expected as we don't have enough information to fit a line through a single point. We need at least two points, otherwise the parameters are unbounded. That is true unless we provide some more information; we can do this by using priors. Another one is adding more structure to the model. Let's do that and build a hierarchical model.

![Posterior distribution for $\alpha$ and $\beta$ for `unpooled_model`](fig/hierarchical_up_forest.png){#fig-hierarchical_up_forest width="80%"}

This is the PyMC model for the hierarchical model:

```python
with pm.Model(coords=coords) as hierarchical_centered:
    # hyper-priors
    α_μ = pm.Normal("α_μ", mu=y_m.mean(), sigma=1)
    α_σ = pm.HalfNormal("α_σ", 5)
    β_μ = pm.Normal("β_μ", mu=0, sigma=1)
    β_σ = pm.HalfNormal("β_σ", sigma=5)

    # priors
    α = pm.Normal("α", mu=α_μ, sigma=α_σ, dims="group")
    β = pm.Normal("β", mu=β_μ, sigma=β_σ, dims="group")
    σ = pm.HalfNormal("σ", 5)
    _ = pm.Normal("y_pred", mu=α[idx] + β[idx] * x_m, sigma=σ, observed=y_m)


    idata_cen = pm.sample()
```

If you run `hierarchical_centered` you will see a message from PyMC saying something like `There were 149 divergences after tuning. Increase target_accept or reparameterize.` This message means that samples generated from PyMC may not be trustworthy. So far we have assumed that PyMC always returns samples that we can use without issues, but that's not always the case. In @sec-inference_engines we further discuss why is this along with diagnostic methods to help you identify those situations and recommendations to fix the potential issues. In that section, we also explain what divergences are. For now, we will only say that when working with hierarchical linear models we may usually get a lot of divergences. The easy way to solve them is to increase `target_accept`, as PyMC kindly suggests. This is an argument of `pm.sample()` that defaults to 0.8 and can take a maximum value of 1. If you see divergences setting this argument to values like 0.85, 0.9 or even higher can help, but if you reach values like 0.99 and still have divergences probably you are out of luck with this simple trick and you need to do something else. And that's reparametrization. What is this? Reparametrization is writing a model in a different way, but that is mathematically equivalent to your original model, i.e. you are not changing the model, just writing in another way. Many models if not all can be written in alternative ways. Sometimes reparametrization can have a positive effect on the efficiency of the sampler or on the model's interpretability. For instance, you can remove divergences by doing a reparametrization. Let's see how to do that in the next section.

### Center vs noncentered hierarchical models

There are two common parametrizations for hierarchical linear models, centered and non-centered. The model `hierarchical_centered` uses the centered one, the hallmark of this parametrization is that we are directly estimating parameters for individual groups, for instance, we are explicitly estimating the slope of each group. On the contrary for the non-centered parametrization, we estimate the common slope for all groups and then a deflection for each group. Is important to notice that we are still modeling the slope of each group, but relative to the common slope, the information we are getting is the same, just represented differently. As a model is worth a thousand words let's check `hierarchical_non_centered`.

```python
with pm.Model(coords=coords) as hierarchical_non_centered:
    # hyper-priors
    α_μ = pm.Normal("α_μ", mu=y_m.mean(), sigma=1)
    α_σ = pm.HalfNormal("α_σ", 5)
    β_μ = pm.Normal("β_μ", mu=0, sigma=1)
    β_σ = pm.HalfNormal("β_σ", sigma=5)

    # priors
    α = pm.Normal("α", mu=α_μ, sigma=α_σ, dims="group")

    β_offset = pm.Normal("β_offset", mu=0, sigma=1, dims="group")
    β = pm.Deterministic("β", β_μ + β_offset * β_σ, dims="group")

    σ = pm.HalfNormal("σ", 5)
    _ = pm.Normal("y_pred", mu=α[idx] + β[idx] * x_m, sigma=σ, observed=y_m)

    idata_ncen = pm.sample(target_accept=0.85)
```

The difference is that for `hierarchical_centered`  we defined $\beta \mathcal{N}(\beta_\mu, \beta_\sigma)$ and for `hierarchical_non_centered` we are using $\beta = \beta_\mu + \beta_\text{offset} * \beta_\sigma$. The non-centered parametrization is more efficient, when I run the model I only get 2 divergences, instead of 148 as before. To remove these remaining diverges we may still need to increase `target_accept`. For this particular case changing it from 0.8 to 0.85 worked as magic. To fully undersand why this reparametrization works you need to understand the geometry of the posterior distribution, but that's beyond the scope of this section but don't worry, we will discuss this in @sec-inference_engines.

Now that our samples are divergence-free we can go back to analyze the posterior. @fig-hierarchical_non_centered_forest shows the estimated values for $\alpha$ and $\beta$ for `hierarchical_model`. The estimates for group H are still the ones with higher uncertainty. But the results look less crazy than those from @fig-hierarchical_up_forest, the reason is that groups are sharing information. Hence even when we don't have enough information to fit a line to a single point, group H _is being informed_ by the other groups. Actually, all groups have informed all groups. This is the power of hierarchical models.

![Posterior distribution for $\alpha$ and $\beta$ for `hierarchical_non_centered`](fig/hierarchical_non_centered_forest.png){#fig-hierarchical_non_centered_forest width="80%"}

@fig-hierarchical_non_centered_fit shows the fitted lines for each of the eight groups. We can see that we managed to fit a line to a single point. At first, this may sound weird or even fishy, but this is just a consequence of the structure of the hierarchical model. Each line is informed by the lines of the other groups, thus we are not truly adjusting a line to a single point. Instead, we are adjusting a line that's been informed by the points in the other groups to a single point.

![Fitted lines for `hierarchical_non_centered`](fig/hierarchical_non_centered_fit.png){#fig-hierarchical_non_centered_fit}


## Multiple linear regression

So far, we have been working with one dependent variable and one independent variable. Nevertheless, it is not unusual to have several independent variables that we want to
include in our model. Some examples could be:

* Perceived quality of the wine (dependent) and acidity, density, alcohol level, residual sugar, and sulfates content (independent variables)
* A student's average grades (dependent) and family income, distance from home to school, and mother's education level (categorical variable)

We can easily extend the simple linear regression model to deal with more than one independent variable. We call this model multiple linear regression or less often
multivariable linear regression (not to be confused with multivariate linear regression, the case where we have multiple dependent variables).

In a multiple linear regression model, we model the mean of the dependent variable as follows:

$$
\mu = \alpha + \beta_1 X_{1} + \beta_2 X_{2} + \dots + \beta_k X_{k}
$$

Using linear algebra notation, we can write a shorter version:

$$
\mu = \alpha + \mathbf{X} \beta
$$

where $\mathbf{X}$ is a matrix of size $n \times k$ with the values of the independent variables and $\beta$ is a vector of size $k$ with the coefficients of the independent variables, and $n$ is the number of observations. 

If you are a little rusty with your linear algebra, you may want to check the Wikipedia article about the dot product between two vectors and its generalization to matrix multiplication. Basically what you need to know is that we are just using a shorter and more convenient way to write our model:

$$
 \mathbf{X} \beta = \sum_i^n  \beta_i X_{i} = \beta_1 X_{1} + \beta_2 X_{2} + \dots + \beta_k X_{k}
$$

Using the simple linear regression model, we find a straight line that (hopefully) explains our data. Under the multiple linear regression model we find, instead, a hyperplane of dimension $k$. Thus, the multiple linear regression model is essentially the same as the simple linear regression model, the only difference being that now $\beta$ is a vector and $\mathbf{X}$ is a matrix.

To see an example of a multiple linear regression model, let's go back to the bikes dataset. We will use the temperature and the humidity of the day to predict the number of rented bikes. 

```python
with pm.Model() as model_mlb:
    α = pm.Normal("α", mu=0, sigma=1)
    β0 = pm.Normal("β0", mu=0, sigma=10)
    β1 = pm.Normal("β1", mu=0, sigma=10)
    σ = pm.HalfNormal("σ", 10)
    μ = pm.Deterministic("μ", pm.math.exp(α + β0 * bikes.temperature + β1 * bikes.hour))
    _ = pm.NegativeBinomial("y_pred", mu=μ, alpha=σ, observed=bikes.rented)

    idata_mlb = pm.sample()
```

Please take a moment to compare `model_mlb` which has two independent variables `temperature` and `hour`  with `model_neg` which only has one independent variable `temperature`. The only difference is that now we have two $\beta$ coefficients, one for each independent variable. The rest of the model is the same. Notice that we could have written `β = pm.Normal("β1", mu=0, sigma=10, shape=2)` and then use `β1[0]` and `β1[1]` in the definition of $\mu$. I usually do that.

As you can see writing a multiple regression model is not that different from writing a simple regression model. Interpreting the results can be more challenging, though. For instance, the coefficient of `temperature` is now $\beta_0$ and the coefficient of `hour` is $\beta_1$. We can still interpret the coefficients as the change in the dependent variable for a unit change in the independent variable. But now we have to be careful to specify which independent variable we are talking about. For instance, we can say that for a unit increase in the temperature, the number of rented bikes increases by $\beta_0$ units, _holding the hour constant_. Or we can say that for a unit increase in the hour, the number of rented bikes increases by $\beta_1$ units, _holding the temperature constant_. Also, the value of a coefficient for a given variable is dependent on what other variables we are including in the model. For instance, the coefficient of `temperature` will be different if we include `hour` in the model or not @fig-neg_vs_mlb shows the $\beta$ coefficients for models `model_neg` (only `temperature`) and for model `model_mld` (`temperature` and `hour`). We can see that the coefficient of `temperature` is different in both models. This is because the effect of `temperature` on the number of rented bikes depends on the hour of the day. Even more the values of the $\beta$ coefficients have been scaled by the standard deviation of their corresponding independent variable, so we can make them comparable. We can see that once we include `hour` in the model the effect of `temperature` on the number of rented bikes get smaller. This is because the effect of `hour` is already explaining some of the variations in the number of rented bikes that were previously explained by `temperature`. In extreme cases, the addition of a new variable can make the coefficient go to zero or even change the sign. We will discuss more of this in the next chapter.


![Scaled $\beta$ coefficients for `model_neg` and  `model_mlb`](fig/neg_vs_mlb.png){#fig-neg_vs_mlb width="80%"}


## Summary


In this chapter, we have learned about linear regression, which aims to model the relationship between a dependent variable and an independent variable. We have seen how to use PyMC to fit a linear regression model and how to interpret the results and make plots that we can share with different audiences. Our first example was a model with a Gaussian response. But then we saw that this is just one assumption and we can easily change it to deal with none Gaussian responses, such as count data, using a negative binomial regression model or a logistic regression model for Binary data. We saw that when doing so we also need to set an inverse link function to map the linear predictor to the response variable. Using a  Student's t distribution as likelihood can be useful to deal with outliers. We spent most of the chapter modeling the mean as a linear function of the independent variable but we learned that we can also model other parameters like the variance. This is useful when we have heteroscedastic data. We learned how to apply the concept of partial pooling to create hierarchical linear regression models. Finally, we briefly discuss multiple linear regression models.

PyMC makes it very easy to implement all these different flavors of Bayesian linear regression, by changing one or a few lines of code. In the next chapter, we will learn more about linear regression and we will learn about Bambi a tool built on top of PyMC that makes it even easier to build and analyze linear regression models.

## Exercises


<!-- Redo @fig/linear_bikes_posterior_lines but using posterior['mu'] -->
<!-- posterior = az.extract(idata_lb, num_samples=50)
x_plot = xr.DataArray(np.linspace(bikes.temperature.min(), bikes.temperature.max(), 50), dims="plot_id")
mean_line = posterior['α'].mean() + posterior['β'].mean() * x_plot
lines = posterior['α'] + posterior['β'] * x_plot
hdi_lines = az.hdi(idata_lb.posterior['μ']) -->

<!-- 2. Add the best line obtained using model_g to @fig-linear_robust_regression -->